
<!DOCTYPE HTML>
<html lang="zh" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>特征工程 · 数据挖掘开源书</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        <meta name="author" content="wizardforcel">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-atoc/atoc.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-comment/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../w5-tuning-parameter/5w.html" />
    
    
    <link rel="prev" href="../w3-decision-tree-and-ensemble-learning/3w.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="輸入並搜尋" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://www.gitbook.com/book/wizardforcel/data-mining-book" target="_blank" class="custom-link">数据挖掘开源书</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../w0-introduction/0w.html">
            
                <a href="../w0-introduction/0w.html">
            
                    
                    数据挖掘导论和信贷模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../w1-regression/1w.html">
            
                <a href="../w1-regression/1w.html">
            
                    
                    回归模型和房价预测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../w2-preceptron-and-logistic-regression/2w.html">
            
                <a href="../w2-preceptron-and-logistic-regression/2w.html">
            
                    
                    感知机和逻辑回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                    
                    决策树和集成学习
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.6" data-path="4w.html">
            
                <a href="4w.html">
            
                    
                    特征工程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../w5-tuning-parameter/5w.html">
            
                <a href="../w5-tuning-parameter/5w.html">
            
                    
                    参数调优
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../w6-unsupervised-learning/6w.html">
            
                <a href="../w6-unsupervised-learning/6w.html">
            
                    
                    无监督学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../w7-text-mining/7w.html">
            
                <a href="../w7-text-mining/7w.html">
            
                    
                    文本挖掘
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../w8-neural-network/8w.html">
            
                <a href="../w8-neural-network/8w.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../w9-deep-learning/9w.html">
            
                <a href="../w9-deep-learning/9w.html">
            
                    
                    深度学习
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本書使用 GitBook 釋出
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >特征工程</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="sections"><a name="sections" class="plugin-anchor" href="#sections"><i class="fa fa-link" aria-hidden="true"></i></a>Sections</h2>
<ul>
<li><a href="#what-is-feature-engineering">What is Feature Engineering?</a></li>
<li><p><a href="#data-preprocessing">Data preprocessing</a></p>
<ul>
<li><a href="#dealing-with-missing-data">Dealing with missing data</a><ul>
<li><a href="#eliminating-samples-or-features-with-missing-values">Eliminating samples or features with missing values</a></li>
<li><a href="#imputing-missing-values">Imputing missing values</a></li>
</ul>
</li>
<li><a href="#handling-categorical-data">Handling categorical data</a><ul>
<li><a href="#mapping-ordinal-features">Mapping ordinal features</a></li>
<li><a href="#encoding-class-labels">Encoding class labels</a></li>
<li><a href="#performing-one-hot-encoding-on-nominal-features">Performing one-hot encoding on nominal features</a></li>
</ul>
</li>
<li><a href="#partitioning-a-dataset-in-training-and-test-sets">Partitioning a dataset in training and test sets</a></li>
<li><a href="#bringing-features-onto-the-same-scale">Bringing features onto the same scale</a></li>
</ul>
</li>
<li><p><a href="#feature-selection">Feature selection</a></p>
<ul>
<li><a href="#univariate-statistics">Univariate statistics</a></li>
<li><a href="#recursive-feature-elimination">Recursive feature elimination</a></li>
<li><a href="#feature-selection-using-selectfrommodel">Feature selection using SelectFromModel</a><ul>
<li><a href="#l1-based-feature-selection">L1-based feature selection</a></li>
<li><a href="#tree-based-feature-selection">Tree-based feature selection</a></li>
</ul>
</li>
</ul>
</li>
<li><p><a href="#Feature-extraction">Feature extraction</a></p>
<ul>
<li><a href="#unsupervised-dimensionality-reduction-via-principal-component-analysis">Unsupervised dimensionality reduction via principal component analysis</a><ul>
<li><a href="#total-and-explained-variance">Total and explained variance</a></li>
<li><a href="#feature-transformation">Feature transformation</a></li>
<li><a href="#principal-component-analysis-in-scikit-learn">Principal component analysis in scikit-learn</a></li>
</ul>
</li>
<li><a href="#supervised-data-compression-via-linear-discriminant-analysis">Supervised data compression via linear discriminant analysis</a><ul>
<li><a href="#computing-the-scatter-matrices">Computing the scatter matrices</a></li>
<li><a href="#selecting-linear-discriminants-for-the-new-feature-subspace">Selecting linear discriminants for the new feature subspace</a></li>
<li><a href="#projecting-samples-onto-the-new-feature-space">Projecting samples onto the new feature space</a></li>
<li><a href="#lda-via-scikit-learn">LDA via scikit-learn</a></li>
</ul>
</li>
<li><a href="#using-kernel-principal-component-analysis-for-nonlinear-mappings">Using kernel principal component analysis for nonlinear mappings</a><ul>
<li><a href="#implementing-a-kernel-principal-component-analysis-in-Python">Implementing a kernel principal component analysis in Python</a></li>
<li><a href="#example-1:-Separating-half-moon-shapes">Example 1: Separating half-moon shapes</a></li>
<li><a href="#example-2:-separating-concentric-circles">Example 2: Separating concentric circles</a></li>
<li><a href="#projecting-new-data-points">Projecting new data points</a></li>
<li><a href="#kernel-principal-component-analysis-in-scikit-learn">Kernel principal component analysis in scikit-learn</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#using-regularization">Using regularization</a><ul>
<li><a href="#ridge-regression">Ridge Regression</a></li>
<li><a href="#lasso-regression">LASSO Regression</a></li>
<li><a href="#logistic-regression-with-regularization">Logistic regression with regularization</a></li>
</ul>
</li>
</ul>
<p><br>
<br></p>
<h2 id="what-is-feature-engineering"><a name="what-is-feature-engineering" class="plugin-anchor" href="#what-is-feature-engineering"><i class="fa fa-link" aria-hidden="true"></i></a>What is Feature Engineering?</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.</p>
<h3 id="sub-problems-of-feature-engineering"><a name="sub-problems-of-feature-engineering" class="plugin-anchor" href="#sub-problems-of-feature-engineering"><i class="fa fa-link" aria-hidden="true"></i></a>Sub-Problems of Feature Engineering</h3>
<ul>
<li>Feature Importance: An estimate of the usefulness of a feature</li>
<li>Feature Selection: From many features to a few that are useful</li>
<li>Feature Extraction: The automatic construction of new features from raw data</li>
<li>Feature Construction: The manual construction of new features from raw data</li>
</ul>
<h3 id="iterative-process-of-feature-engineering"><a name="iterative-process-of-feature-engineering" class="plugin-anchor" href="#iterative-process-of-feature-engineering"><i class="fa fa-link" aria-hidden="true"></i></a>Iterative Process of Feature Engineering</h3>
<ul>
<li>Brainstorm features: Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal.</li>
<li>Devise features: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two.</li>
<li>Select features: Use different feature importance scorings and feature selection methods to prepare one or more &#x201C;views&#x201D; for your models to operate upon.</li>
<li>Evaluate models: Estimate model accuracy on unseen data using the chosen features.</li>
</ul>
<h3 id="general-examples-of-feature-engineering"><a name="general-examples-of-feature-engineering" class="plugin-anchor" href="#general-examples-of-feature-engineering"><i class="fa fa-link" aria-hidden="true"></i></a>General Examples of Feature Engineering</h3>
<ul>
<li>Decompose Categorical Attributes<ul>
<li>Imagine you have a categorical attribute, like &#x201C;Item_Color&#x201D; that can be Red, Blue or Unknown.</li>
</ul>
</li>
<li>Decompose a Date-Time<ul>
<li>A date-time contains a lot of information that can be difficult for a model to take advantage of in it&#x2019;s native form, such as ISO 8601 (i.e. 2014-09-20T20:45:40Z).</li>
</ul>
</li>
<li>Reframe Numerical Quantities<ul>
<li>Your data is very likely to contain quantities, which can be reframed to better expose relevant structures. This may be a transform into a new unit or the decomposition of a rate into time and amount components.</li>
</ul>
</li>
</ul>
<p><br>
<br></p>
<h1 id="data-preprocessing"><a name="data-preprocessing" class="plugin-anchor" href="#data-preprocessing"><i class="fa fa-link" aria-hidden="true"></i></a>Data preprocessing</h1>
<h2 id="dealing-with-missing-data"><a name="dealing-with-missing-data" class="plugin-anchor" href="#dealing-with-missing-data"><i class="fa fa-link" aria-hidden="true"></i></a>Dealing with missing data</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x6784;&#x9020;&#x542B;&#x7F3A;&#x5931;&#x503C;&#x7684;&#x6570;&#x636E;, NaN &#x8868;&#x793A; Not a Number</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

df = pd.DataFrame(np.arange(<span class="hljs-number">1</span>, <span class="hljs-number">13</span>).reshape(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), 
                  columns=[<span class="hljs-string">&apos;A&apos;</span>, <span class="hljs-string">&apos;B&apos;</span>, <span class="hljs-string">&apos;C&apos;</span>, <span class="hljs-string">&apos;D&apos;</span>])

df.loc[<span class="hljs-number">1</span>, <span class="hljs-string">&apos;C&apos;</span>] = <span class="hljs-keyword">None</span>
df.loc[<span class="hljs-number">2</span>, <span class="hljs-string">&apos;D&apos;</span>] = <span class="hljs-keyword">None</span>


df
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>NaN</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># isnull &#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A; DataFrame, &#x91CC;&#x9762;&#x7684; bool &#x503C;&#x8868;&#x793A;&#x539F;&#x59CB;&#x6570;&#x636E;&#x662F;&#x5426;&#x7F3A;&#x5931;</span>
df.isnull()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>False</td>
      <td>False</td>
      <td>True</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>False</td>
      <td>False</td>
      <td>False</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># &#x7ED3;&#x679C;&#x663E;&#x793A; A &#x548C; B &#x5217;&#x6CA1;&#x6709;&#x7F3A;&#x5931;&#x503C;, C &#x548C; D &#x5404;&#x6709;&#x4E00;&#x4E2A;&#x7F3A;&#x5931;&#x503C;</span>
df.isnull().sum()
</code></pre>
<pre><code>A    0
B    0
C    1
D    1
dtype: int64
</code></pre><p><br>
<br></p>
<h3 id="eliminating-samples-or-features-with-missing-values"><a name="eliminating-samples-or-features-with-missing-values" class="plugin-anchor" href="#eliminating-samples-or-features-with-missing-values"><i class="fa fa-link" aria-hidden="true"></i></a>Eliminating samples or features with missing values</h3>
<p>&#x5904;&#x7406;&#x7F3A;&#x5931;&#x503C;&#x6700;&#x7B80;&#x5355;&#x7684;&#x65B9;&#x6CD5;&#x5C31;&#x662F;&#x5220;&#x6389;&#x6709;&#x7F3A;&#x5931;&#x7684;&#x884C;&#x6216;&#x8005;&#x5217;</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python">df.dropna() <span class="hljs-comment"># &#x9ED8;&#x8BA4;&#x5220;&#x9664;&#x884C; axis = 0</span>
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python">df.dropna(axis=<span class="hljs-number">1</span>) <span class="hljs-comment"># &#x5220;&#x9664;&#x5217;</span>
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># &#x53EA;&#x5220;&#x9664;&#x5168;&#x662F;&#x7F3A;&#x5931;&#x503C;&#x7684;&#x884C;</span>
df.dropna(how=<span class="hljs-string">&apos;all&apos;</span>)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>6</td>
      <td>NaN</td>
      <td>8.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># &#x5220;&#x9664;&#x975E;&#x7F3A;&#x5931;&#x503C;&#x5C11;&#x4E8E; thresh &#x7684;&#x884C;</span>
df.dropna(thresh=<span class="hljs-number">4</span>)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># &#x5220;&#x9664;&#x6709;&#x7F3A;&#x5931;&#x503C;&#x51FA;&#x73B0;&#x5728;&#x7279;&#x5B9A;&#x5217;&#x7684;&#x884C;</span>
df.dropna(subset=[<span class="hljs-string">&apos;C&apos;</span>])
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>9</td>
      <td>10</td>
      <td>11.0</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



<p>&#x770B;&#x4E0A;&#x53BB;&#x5220;&#x9664;&#x662F;&#x5F88;&#x7B80;&#x4FBF;&#x7684;&#x5904;&#x7406;&#x65B9;&#x6CD5;, &#x4F46;&#x5B9E;&#x9645;&#x4E0A;&#x76F4;&#x63A5;&#x5220;&#x9664;&#x53EF;&#x80FD;&#x4F1A;&#x4E22;&#x5931;&#x4E0D;&#x5C11;&#x4FE1;&#x606F;, &#x66F4;&#x597D;&#x7684;&#x9009;&#x62E9;&#x662F;&#x586B;&#x8865;&#x7F3A;&#x5931;&#x503C;</p>
<p><br>
<br></p>
<h3 id="imputing-missing-values"><a name="imputing-missing-values" class="plugin-anchor" href="#imputing-missing-values"><i class="fa fa-link" aria-hidden="true"></i></a>Imputing missing values</h3>
<p>&#x4F30;&#x8BA1;&#x7F3A;&#x5931;&#x503C;&#x5E76;&#x586B;&#x5145;, &#x6700;&#x666E;&#x904D;&#x7684;&#x662F; mean imputation, &#x4E5F;&#x5C31;&#x662F;&#x7528;&#x5E73;&#x5747;&#x503C;&#x586B;&#x5145;</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> Imputer

imr = Imputer(missing_values=<span class="hljs-string">&apos;NaN&apos;</span>, strategy=<span class="hljs-string">&apos;mean&apos;</span>, axis=<span class="hljs-number">0</span>) 
<span class="hljs-comment"># If axis=0, then impute along columns.</span>
<span class="hljs-comment"># If axis=1, then impute along rows.</span>
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
imputed_data
</code></pre>
<pre><code>array([[  1.,   2.,   3.,   4.],
       [  5.,   6.,   7.,   8.],
       [  9.,  10.,  11.,   6.]])
</code></pre><pre><code class="lang-python">df.values <span class="hljs-comment"># &#x5E76;&#x6CA1;&#x6709;&#x6539;&#x53D8;&#x539F;&#x5148;&#x7684; df</span>
</code></pre>
<pre><code>array([[  1.,   2.,   3.,   4.],
       [  5.,   6.,  nan,   8.],
       [  9.,  10.,  11.,  nan]])
</code></pre><p><br>
<br></p>
<h2 id="handling-categorical-data"><a name="handling-categorical-data" class="plugin-anchor" href="#handling-categorical-data"><i class="fa fa-link" aria-hidden="true"></i></a>Handling categorical data</h2>
<p>&#x5BF9; categorical &#x9700;&#x8981;&#x533A;&#x5206; nominal &#x548C; ordinal &#x4E24;&#x79CD;&#x7C7B;&#x578B;, nominal &#x662F;&#x65E0;&#x5E8F;&#x7684;, &#x800C; ordinal &#x662F;&#x6709;&#x5E8F;&#x7684;</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
df = pd.DataFrame([
            [<span class="hljs-string">&apos;green&apos;</span>, <span class="hljs-string">&apos;M&apos;</span>, <span class="hljs-number">10.1</span>, <span class="hljs-string">&apos;class1&apos;</span>], 
            [<span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;L&apos;</span>, <span class="hljs-number">13.5</span>, <span class="hljs-string">&apos;class2&apos;</span>], 
            [<span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;XL&apos;</span>, <span class="hljs-number">15.3</span>, <span class="hljs-string">&apos;class1&apos;</span>]])

df.columns = [<span class="hljs-string">&apos;color&apos;</span>, <span class="hljs-string">&apos;size&apos;</span>, <span class="hljs-string">&apos;price&apos;</span>, <span class="hljs-string">&apos;classlabel&apos;</span>]
df
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>color</th>
      <th>size</th>
      <th>price</th>
      <th>classlabel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>M</td>
      <td>10.1</td>
      <td>class1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>L</td>
      <td>13.5</td>
      <td>class2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>XL</td>
      <td>15.3</td>
      <td>class1</td>
    </tr>
  </tbody>
</table>
</div>



<ul>
<li><code>color</code>: nominal feature</li>
<li><code>size</code>: ordinal feature, XL &gt; L &gt; M</li>
<li><code>price</code>: numerical feature</li>
</ul>
<p><br>
<br></p>
<h3 id="mapping-ordinal-features"><a name="mapping-ordinal-features" class="plugin-anchor" href="#mapping-ordinal-features"><i class="fa fa-link" aria-hidden="true"></i></a>Mapping ordinal features</h3>
<p>convert the categorical string values into integers</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># define the mapping manually</span>
size_mapping = {
           <span class="hljs-string">&apos;XL&apos;</span>: <span class="hljs-number">3</span>,
           <span class="hljs-string">&apos;L&apos;</span>: <span class="hljs-number">2</span>,
           <span class="hljs-string">&apos;M&apos;</span>: <span class="hljs-number">1</span>}

df[<span class="hljs-string">&apos;size&apos;</span>] = df[<span class="hljs-string">&apos;size&apos;</span>].map(size_mapping)
df
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>color</th>
      <th>size</th>
      <th>price</th>
      <th>classlabel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>class1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>class2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>class1</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># transform the integer values back to the original string</span>
inv_size_mapping = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> size_mapping.items()}
df[<span class="hljs-string">&apos;size&apos;</span>].map(inv_size_mapping)
</code></pre>
<pre><code>0     M
1     L
2    XL
Name: size, dtype: object
</code></pre><p><br>
<br></p>
<h3 id="encoding-class-labels"><a name="encoding-class-labels" class="plugin-anchor" href="#encoding-class-labels"><i class="fa fa-link" aria-hidden="true"></i></a>Encoding class labels</h3>
<p>&#x5BF9;&#x5E94; nominal &#x7684; class labels, &#x4E5F;&#x9700;&#x8981;&#x5C06;&#x5176;&#x8F6C;&#x6362;&#x4E3A;&#x6570;&#x503C;&#x8868;&#x5F81;&#xFF0C;&#x8BB0;&#x4F4F;&#x6B64;&#x65F6;&#x7684;&#x6570;&#x503C;&#x53EA;&#x4EE3;&#x8868;&#x4E00;&#x4E2A;&#x7C7B;&#x522B;&#xFF0C;&#x5E76;&#x4E0D;&#x8868;&#x5F81;&#x6570;&#x503C;&#x5173;&#x7CFB;</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

class_mapping = {label:idx <span class="hljs-keyword">for</span> idx,label <span class="hljs-keyword">in</span> 
                 enumerate(np.unique(df[<span class="hljs-string">&apos;classlabel&apos;</span>]))}
class_mapping
</code></pre>
<pre><code>{&apos;class1&apos;: 0, &apos;class2&apos;: 1}
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x6700;&#x7EC8;&#x628A; classlabel &#x4E5F;&#x8F6C;&#x5316;&#x4E3A; interger</span>
df[<span class="hljs-string">&apos;classlabel&apos;</span>] = df[<span class="hljs-string">&apos;classlabel&apos;</span>].map(class_mapping)
df
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>color</th>
      <th>size</th>
      <th>price</th>
      <th>classlabel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># &#x8F6C;&#x5316;&#x56DE;&#x6765;&#x4E5F;&#x662F; ok &#x7684;</span>
inv_class_mapping = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> class_mapping.items()}
df[<span class="hljs-string">&apos;classlabel&apos;</span>] = df[<span class="hljs-string">&apos;classlabel&apos;</span>].map(inv_class_mapping)
df
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>color</th>
      <th>size</th>
      <th>price</th>
      <th>classlabel</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>green</td>
      <td>1</td>
      <td>10.1</td>
      <td>class1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>red</td>
      <td>2</td>
      <td>13.5</td>
      <td>class2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>blue</td>
      <td>3</td>
      <td>15.3</td>
      <td>class1</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># sklearn &#x4E2D;&#x4E5F;&#x6709;&#x76F8;&#x5E94;&#x51FD;&#x6570;</span>
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder

class_le = LabelEncoder()
y = class_le.fit_transform(df[<span class="hljs-string">&apos;classlabel&apos;</span>].values)
y
</code></pre>
<pre><code>array([0, 1, 0])
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x540C;&#x6837;&#x4E5F;&#x53EF;&#x4EE5;&#x53CD;&#x5411;&#x8F6C;&#x6362;</span>
class_le.inverse_transform(y)
</code></pre>
<pre><code>array([&apos;class1&apos;, &apos;class2&apos;, &apos;class1&apos;], dtype=object)
</code></pre><p><br>
<br></p>
<h3 id="performing-one-hot-encoding-on-nominal-features"><a name="performing-one-hot-encoding-on-nominal-features" class="plugin-anchor" href="#performing-one-hot-encoding-on-nominal-features"><i class="fa fa-link" aria-hidden="true"></i></a>Performing one-hot encoding on nominal features</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python">X = df[[<span class="hljs-string">&apos;color&apos;</span>, <span class="hljs-string">&apos;size&apos;</span>, <span class="hljs-string">&apos;price&apos;</span>]].values

<span class="hljs-comment"># color column</span>
color_le = LabelEncoder()
X[:, <span class="hljs-number">0</span>] = color_le.fit_transform(X[:, <span class="hljs-number">0</span>])
X

<span class="hljs-comment">#blue 0</span>
<span class="hljs-comment">#green 1</span>
<span class="hljs-comment">#red 2</span>
</code></pre>
<pre><code>array([[1, 1, 10.1],
       [2, 2, 13.5],
       [0, 3, 15.3]], dtype=object)
</code></pre><p>&#x867D;&#x7136; color &#x8F6C;&#x5316;&#x4E3A;&#x4E86; 0, 1, 2, &#x4F46;&#x5E76;&#x4E0D;&#x80FD;&#x76F4;&#x63A5;&#x4F7F;&#x7528;&#x6765;&#x5EFA;&#x6A21;, &#x56E0;&#x4E3A;&#x5728;&#x5B9E;&#x9645;&#x4F7F;&#x7528;&#x4E2D;, &#x4F1A;&#x8BA4;&#x4E3A; 2 &#x5927;&#x4E8E; 1, &#x4E5F;&#x5C31;&#x662F; red &#x5927;&#x4E8E; green.
&#x5B9E;&#x9645;&#x5374;&#x4E0D;&#x662F;&#x8FD9;&#x6837;&#x7684;, &#x6240;&#x4EE5;&#x9700;&#x8981;&#x7528;&#x5230; one-hot encoding, &#x9700;&#x8981;&#x4F7F;&#x7528; dummy variable, &#x6BCF;&#x4E00;&#x4E2A; label &#x6700;&#x540E;&#x88AB;&#x8868;&#x793A;&#x4E3A;&#x4E00;&#x4E2A;&#x5411;&#x91CF;.
&#x4F8B;&#x5982;, blue sample can be encoded as blue=1, green=0, red=0. </p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder

ohe = OneHotEncoder(categorical_features=[<span class="hljs-number">0</span>], sparse=<span class="hljs-keyword">False</span>)
<span class="hljs-comment"># &#x4E0D;&#x8BBE;&#x5B9A; sparse=False &#x7684;&#x8BDD;&#xFF0C;onehot &#x4F1A;&#x8FD4;&#x56DE;&#x4E00;&#x4E2A; sparse matrix&#xFF0C; &#x53EF;&#x4EE5;&#x7528; toarray() &#x5C06;&#x4E4B;&#x53D8;&#x56DE; dense</span>

ohe.fit_transform(X)
<span class="hljs-comment"># &#x524D;&#x4E09;&#x5217;&#x4E3A;dummy</span>
</code></pre>
<pre><code>array([[  0. ,   1. ,   0. ,   1. ,  10.1],
       [  0. ,   0. ,   1. ,   2. ,  13.5],
       [  1. ,   0. ,   0. ,   3. ,  15.3]])
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># pandas &#x4E2D;&#x7684; get_dummies &#x51FD;&#x6570;&#x662F;&#x751F;&#x6210; dummy variable &#x66F4;&#x7B80;&#x5355;&#x7684;&#x65B9;&#x6CD5;</span>
pd.get_dummies(df[[<span class="hljs-string">&apos;price&apos;</span>, <span class="hljs-string">&apos;color&apos;</span>, <span class="hljs-string">&apos;size&apos;</span>]])
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>size</th>
      <th>color_blue</th>
      <th>color_green</th>
      <th>color_red</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10.1</td>
      <td>1</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13.5</td>
      <td>2</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>15.3</td>
      <td>3</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



<p><br>
<br></p>
<h2 id="partitioning-a-dataset-in-training-and-test-sets"><a name="partitioning-a-dataset-in-training-and-test-sets" class="plugin-anchor" href="#partitioning-a-dataset-in-training-and-test-sets"><i class="fa fa-link" aria-hidden="true"></i></a>Partitioning a dataset in training and test sets</h2>
<p>the test set can be understood as the <code>ultimate test</code> of our model before we let it loose on the real world</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x8BFB;&#x53D6;wine&#x6570;&#x636E;</span>
df_wine = pd.read_csv(<span class="hljs-string">&apos;data/wine.data&apos;</span>, header=<span class="hljs-keyword">None</span>)

df_wine.columns = [<span class="hljs-string">&apos;Class label&apos;</span>, <span class="hljs-string">&apos;Alcohol&apos;</span>, <span class="hljs-string">&apos;Malic acid&apos;</span>, <span class="hljs-string">&apos;Ash&apos;</span>, 
<span class="hljs-string">&apos;Alcalinity of ash&apos;</span>, <span class="hljs-string">&apos;Magnesium&apos;</span>, <span class="hljs-string">&apos;Total phenols&apos;</span>, 
<span class="hljs-string">&apos;Flavanoids&apos;</span>, <span class="hljs-string">&apos;Nonflavanoid phenols&apos;</span>, <span class="hljs-string">&apos;Proanthocyanins&apos;</span>, 
<span class="hljs-string">&apos;Color intensity&apos;</span>, <span class="hljs-string">&apos;Hue&apos;</span>, <span class="hljs-string">&apos;OD280/OD315 of diluted wines&apos;</span>, <span class="hljs-string">&apos;Proline&apos;</span>]

print(<span class="hljs-string">&apos;Class labels&apos;</span>, np.unique(df_wine[<span class="hljs-string">&apos;Class label&apos;</span>]))
df_wine.head()

<span class="hljs-comment"># &#x4E00;&#x5171;&#x6709;&#x4E09;&#x79CD; label</span>
</code></pre>
<pre><code>(&apos;Class labels&apos;, array([1, 2, 3]))
</code></pre><div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class label</th>
      <th>Alcohol</th>
      <th>Malic acid</th>
      <th>Ash</th>
      <th>Alcalinity of ash</th>
      <th>Magnesium</th>
      <th>Total phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diluted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>



<p>&#x4F7F;&#x7528; train_test_split &#x51FD;&#x6570;&#x8FDB;&#x884C;&#x8BAD;&#x7EC3;/&#x6D4B;&#x8BD5;&#x96C6;&#x5207;&#x5206;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split

X, y = df_wine.iloc[:, <span class="hljs-number">1</span>:].values, df_wine.iloc[:, <span class="hljs-number">0</span>].values

X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)

<span class="hljs-comment"># 30%&#x662F; test data</span>
</code></pre>
<h2 id="stratified-train-test-split"><a name="stratified-train-test-split" class="plugin-anchor" href="#stratified-train-test-split"><i class="fa fa-link" aria-hidden="true"></i></a>stratified train test split</h2>
<p>stratified &#x5207;&#x5206;&#xFF0C; &#x4F7F;&#x5207;&#x5206;&#x540E;&#x7684;&#x6570;&#x636E;&#x96C6;&#x66F4;&#x597D;&#x5730;&#x4FDD;&#x7559;&#x6807;&#x7B7E;&#x7684;&#x76F8;&#x5BF9;&#x6BD4;&#x4F8B;</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5E2E;&#x52A9;&#x51FD;&#x6570;&#xFF0C;&#x8BA1;&#x7B97;&#x5404;&#x6807;&#x7B7E;&#x6BD4;&#x4F8B;</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">label_frequency</span><span class="hljs-params">(labels)</span>:</span>
    counts = np.unique(labels, return_counts=<span class="hljs-keyword">True</span>)[<span class="hljs-number">1</span>]
    n = len(labels)
    <span class="hljs-keyword">return</span> counts / float(n)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x539F;&#x59CB;&#x6570;&#x636E;&#x4E2D;&#x5404;&#x6807;&#x7B7E;&#x7684;&#x6BD4;&#x4F8B;</span>
label_frequency(y)
</code></pre>
<pre><code>array([ 0.33146067,  0.3988764 ,  0.26966292])
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># train_test_split &#x540E;&#x7684;&#x6BD4;&#x4F8B;</span>
label_frequency(y_train), label_frequency(y_test)
</code></pre>
<pre><code>(array([ 0.32258065,  0.39516129,  0.28225806]),
 array([ 0.35185185,  0.40740741,  0.24074074]))
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># stratified &#x4E4B;&#x540E;&#x7684;&#x6807;&#x7B7E;&#x6BD4;&#x4F8B;&#xFF0C; &#x66F4;&#x63A5;&#x8FD1;&#x539F;&#x59CB;&#x6BD4;&#x4F8B;</span>
X_train, X_test, y_train, y_test = \
        train_test_split(X, y, stratify=y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)
label_frequency(y_train), label_frequency(y_test)
</code></pre>
<pre><code>(array([ 0.33333333,  0.39837398,  0.26829268]),
 array([ 0.32727273,  0.4       ,  0.27272727]))
</code></pre><p><br>
<br></p>
<h2 id="bringing-features-onto-the-same-scale"><a name="bringing-features-onto-the-same-scale" class="plugin-anchor" href="#bringing-features-onto-the-same-scale"><i class="fa fa-link" aria-hidden="true"></i></a>Bringing features onto the same scale</h2>
<p>Feature Scaling &#x5F88;&#x5BB9;&#x6613;&#x88AB;&#x9057;&#x5FD8;, &#x867D;&#x7136;&#x5728; Decision tree&#x548C; random forests &#x65F6;&#x4E0D;&#x7528;&#x62C5;&#x5FC3;&#x8FD9;&#x4E2A;&#x95EE;&#x9898;.
&#x4F46;&#x5728;&#x5F88;&#x591A;&#x7B97;&#x6CD5;&#x548C;&#x6A21;&#x578B;&#x4E0B;&#x90FD;&#x662F; scaling &#x540E;&#x62DF;&#x5408;&#x6548;&#x679C;&#x66F4;&#x597D;.</p>
<p>&#x4E24;&#x7C7B;&#x5E38;&#x7528;&#x65B9;&#x6CD5;: normalization &#x548C; standardization.</p>
<ul>
<li>normalization: rescaling to [0,1], &#x5982; min-max scaling <script type="math/tex; "> x_{norm}^{(i)} = \frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}</script></li>
<li>standardization: more practical, &#x56E0;&#x4E3A;&#x5728;&#x4E00;&#x4E9B;&#x7B97;&#x6CD5;&#x4E2D;, weights &#x521D;&#x59CB;&#x503C;&#x90FD;&#x8BBE;&#x7F6E;&#x4E3A; 0, &#x6216;&#x8005;&#x63A5;&#x8FD1; 0. standardization &#x4E4B;&#x540E;&#x4F1A;&#x66F4;&#x5229;&#x7528;&#x66F4;&#x65B0; weights. &#x5E76;&#x4E14; standardize &#x5BF9; outlier &#x66F4;&#x4E0D;&#x654F;&#x611F;&#xFF0C;&#x53D7;&#x5F71;&#x54CD;&#x66F4;&#x5C0F; <script type="math/tex; "> x_{std}^{(i)} = \frac{x^{(i)} - \mu_x}{\sigma_x}</script></li>
</ul>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># min-max rescaling</span>
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)  <span class="hljs-comment"># &#x6CE8;&#x610F;&#x6D4B;&#x8BD5;&#x96C6;&#x662F;&#x6309;&#x7167;&#x8BAD;&#x7EC3;&#x96C6;&#x7684;&#x53C2;&#x6570;&#x8FDB;&#x884C;&#x8F6C;&#x6362;</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># standarzation</span>
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
</code></pre>
<p>A visual example:</p>
<pre><code class="lang-python">ex = pd.DataFrame([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span> ,<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])

<span class="hljs-comment"># standardize</span>
ex[<span class="hljs-number">1</span>] = (ex[<span class="hljs-number">0</span>] - ex[<span class="hljs-number">0</span>].mean()) / ex[<span class="hljs-number">0</span>].std()
<span class="hljs-comment"># normalize</span>
ex[<span class="hljs-number">2</span>] = (ex[<span class="hljs-number">0</span>] - ex[<span class="hljs-number">0</span>].min()) / (ex[<span class="hljs-number">0</span>].max() - ex[<span class="hljs-number">0</span>].min())
ex.columns = [<span class="hljs-string">&apos;input&apos;</span>, <span class="hljs-string">&apos;standardized&apos;</span>, <span class="hljs-string">&apos;normalized&apos;</span>]
ex
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>input</th>
      <th>standardized</th>
      <th>normalized</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>-1.336306</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-0.801784</td>
      <td>0.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>-0.267261</td>
      <td>0.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>0.267261</td>
      <td>0.6</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.801784</td>
      <td>0.8</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>1.336306</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>



<p><br>
<br></p>
<h1 id="feature-selection"><a name="feature-selection" class="plugin-anchor" href="#feature-selection"><i class="fa fa-link" aria-hidden="true"></i></a>Feature selection</h1>
<p>Often we collected many features that might be related to a supervised prediction task, but we don&apos;t know which of them are actually predictive. To improve interpretability, and sometimes also generalization performance, we can use feature selection to select a subset of the original features. </p>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x6839;&#x636E; <a href="http://scholar.google.com/scholar?q=%22Irrelevant+Features+and+the+Subset+Selection+Problem" target="_blank">John, Kohavi, and Pfleger (1994)</a>&#xFF0C;&#x53EF;&#x5C06;&#x7279;&#x5F81;&#x9009;&#x62E9;&#x7684;&#x65B9;&#x6CD5;&#x5206;&#x4E3A;&#x4E24;&#x7C7B;:</p>
<ul>
<li>Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized.</li>
<li>Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model. <a href="http://scholar.google.com/scholar?q=%22A+review+of+feature+selection+techniques+in+bioinformatics" target="_blank">Saeys, Inza, and Larranaga (2007)</a> surveys filter methods.</li>
</ul>
<p>Both approaches have advantages and drawbacks. Filter methods are usually more computationally efficient than wrapper methods, but the selection criterion is not directly related to the effectiveness of the model. Also, most filter methods evaluate each predictor separately and, consequently, redundant (i.e. highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified. The downside of the wrapper method is that many models are evaluated (which may also require parameter tuning) and thus an increase in computation time. There is also an increased risk of over-fitting with wrappers.</p>
<p>Sklearn &#x4E2D;&#x4E3B;&#x8981;&#x4F7F;&#x7528; Filter methods. &#x4E0B;&#x9762;&#x5C06;&#x4ECB;&#x7ECD;&#x5982;&#x4F55;&#x7528; sklearn &#x8FDB;&#x884C;&#x7279;&#x5F81;&#x9009;&#x62E9;&#x3002;</p>
<p><br>
<br></p>
<h2 id="univariate-statistics"><a name="univariate-statistics" class="plugin-anchor" href="#univariate-statistics"><i class="fa fa-link" aria-hidden="true"></i></a>Univariate statistics</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>The simplest method to select features is using univariate statistics, that is by looking at each feature individually and running a statistical test to see whether it is related to the target.</p>
<p>sklearn &#x4E2D;&#x53EF;&#x4EE5;&#x7528;&#x5230;&#x7684; Univariate statistics &#x6709;&#xFF1A;</p>
<ul>
<li>for regression: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression" target="_blank">f_regression</a></li>
<li>for classification: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" target="_blank">chi2</a> or <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" target="_blank">f_classif</a></li>
</ul>
<p>&#x5F97;&#x5230;&#x7EDF;&#x8BA1;&#x91CF;&#x548C; p &#x503C;&#x4E4B;&#x540E;&#xFF0C;sklearn &#x53C8;&#x914D;&#x5957;&#x4E86;&#x4E0D;&#x540C;&#x7684;&#x9009;&#x62E9;&#x65B9;&#x6CD5;&#xFF1A;</p>
<ul>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" target="_blank">SelectKBest</a> removes all but the k highest scoring features</li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" target="_blank">SelectPercentile</a> removes all but a user-specified highest scoring percentage of features</li>
<li>using common univariate statistical tests for each feature: false positive rate <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr" target="_blank">SelectFpr</a>, 
false discovery rate <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr" target="_blank">SelectFdr</a>, or family wise error <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe" target="_blank">SelectFwe</a>.</li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect" target="_blank">GenericUnivariateSelect</a> allows to perform univariate feature
selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># &#x4EE5; chi2 &#x548C; SelectKbest &#x4E3A;&#x4F8B;</span>
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> chi2
<span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectKBest

select = SelectKBest(chi2, k=<span class="hljs-number">6</span>)
X_uni_selected = select.fit_transform(X_train, y_train)

print(X_train.shape)
print(X_uni_selected.shape)
</code></pre>
<pre><code>(123, 13)
(123, 6)
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline
<span class="hljs-comment"># &#x67E5;&#x770B;&#x9009;&#x51FA;&#x4E86;&#x54EA;&#x51E0;&#x4E2A; feature, &#x9ED1;&#x8272;&#x662F;&#x9009;&#x51FA;&#x6765;&#x7684; </span>
mask = select.get_support()
print(mask)
<span class="hljs-comment"># visualize the mask. black is True, white is False</span>
plt.matshow(mask.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>), cmap=<span class="hljs-string">&apos;gray_r&apos;</span>);
</code></pre>
<pre><code>[False  True False  True  True False  True False False  True False False
  True]
</code></pre><p><img src="output_85_1.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="recursive-feature-elimination"><a name="recursive-feature-elimination" class="plugin-anchor" href="#recursive-feature-elimination"><i class="fa fa-link" aria-hidden="true"></i></a>Recursive feature elimination</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE
<span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC

svc = SVC(kernel=<span class="hljs-string">&quot;linear&quot;</span>, C=<span class="hljs-number">1</span>)
rfe = RFE(estimator=svc, 
          n_features_to_select=<span class="hljs-number">6</span>,  <span class="hljs-comment"># &#x8981;&#x9009;&#x51FA;&#x51E0;&#x4E2A; feature</span>
          step=<span class="hljs-number">1</span>)  <span class="hljs-comment"># &#x6BCF;&#x6B21;&#x5254;&#x9664;&#x51FA;&#x51E0;&#x4E2A;feature</span>
rfe.fit(X_train_std, y_train)

X_rfe_selected = rfe.transform(X_train_std)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x67E5;&#x770B;&#x9009;&#x51FA;&#x4E86;&#x54EA;&#x51E0;&#x4E2A; feature</span>
mask = rfe.get_support()
print(mask)
plt.matshow(mask.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>), cmap=<span class="hljs-string">&apos;gray_r&apos;</span>);
</code></pre>
<pre><code>[ True False False  True False False  True False False False  True  True
  True]
</code></pre><p><img src="output_91_1.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="feature-selection-using-selectfrommodel"><a name="feature-selection-using-selectfrommodel" class="plugin-anchor" href="#feature-selection-using-selectfrommodel"><i class="fa fa-link" aria-hidden="true"></i></a>Feature selection using SelectFromModel</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" target="_blank">SelectFromModel</a> is a meta-transformer that can be used along with any estimator that has a coef<em> or feature_importances</em> attribute after fitting. The features are considered unimportant and removed, if the corresponding coef<em> or feature_importances</em> values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are build-in heuristics for finding a threshold using a string argument. Available heuristics are &#x201C;mean&#x201D;, &#x201C;median&#x201D; and float multiples of these like &#x201C;0.1*mean&#x201D;.</p>
<p>&#x4E00;&#x4E9B;&#x6A21;&#x578B;&#x80FD;&#x6BD4;&#x8F83;&#x6BCF;&#x4E2A; feature &#x7684;&#x91CD;&#x8981;&#x7A0B;&#x5EA6;&#xFF0C;&#x4F8B;&#x5982;
&#x7EBF;&#x6027;&#x6A21;&#x578B;&#x52A0;&#x4E0A; L1 &#x6B63;&#x5219;&#x9879;&#x4E4B;&#x540E;&#x4E0D;&#x91CD;&#x8981;&#x7684;&#x7279;&#x5F81;&#x7684;&#x7CFB;&#x6570;&#x4F1A;&#x60E9;&#x7F5A;&#x4E3A;0&#xFF0C;&#x968F;&#x673A;&#x68EE;&#x6797;&#x6A21;&#x578B;&#x80FD;&#x8BA1;&#x7B97;&#x6BCF;&#x4E2A; feature &#x7684;&#x91CD;&#x8981;&#x7A0B;&#x5EA6;&#x3002;<br>&#x7136;&#x540E; sklearn &#x6709;&#x4E2A; <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" target="_blank">SelectFromModel</a> &#x51FD;&#x6570;&#x53EF;&#x4EE5;&#x914D;&#x5408;&#x8FD9;&#x4E9B;&#x6A21;&#x578B;&#x8FDB;&#x884C;&#x7279;&#x5F81;&#x9009;&#x62E9;</p>
<h3 id="l1-based-feature-selection"><a name="l1-based-feature-selection" class="plugin-anchor" href="#l1-based-feature-selection"><i class="fa fa-link" aria-hidden="true"></i></a>L1-based feature selection</h3>
<ul>
<li>L2 norm: <script type="math/tex; "> ||w||_2^2 = \sum_{j=1}^m w_j^2 </script></li>
<li>L1 norm: <script type="math/tex; "> ||w||_1 = \sum_{j=1}^m |w_j| </script> <ul>
<li>&#x4E0E; L2 &#x6B63;&#x5219;&#x76F8;&#x6BD4;&#xFF0C;L1 &#x6B63;&#x5219;&#x4F1A;&#x8BA9;&#x66F4;&#x591A;&#x7CFB;&#x6570;&#x4E3A; 0</li>
<li>&#x5982;&#x679C;&#x6709;&#x4E2A;&#x9AD8;&#x7EF4;&#x6570;&#x636E;, &#x6709;&#x5F88;&#x591A;&#x7279;&#x5F81;&#x662F;&#x65E0;&#x7528;&#x7684;, &#x90A3;&#x4E48; L1 regularization &#x5C31;&#x53EF;&#x4EE5;&#x88AB;&#x5F53;&#x505A;&#x4E00;&#x79CD;&#x7279;&#x5F81;&#x9009;&#x62E9;&#x7684;&#x65B9;&#x6CD5;.</li>
</ul>
</li>
</ul>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-comment"># sklearn &#x91CC;&#x60F3;&#x7528; L1 &#x6B63;&#x5219;&#xFF0C;&#x628A; penalty &#x53C2;&#x6570;&#x8BBE;&#x4E3A; &apos;l1&apos; &#x5373;&#x53EF;</span>
lr = LogisticRegression(penalty=<span class="hljs-string">&apos;l1&apos;</span>, C=<span class="hljs-number">0.1</span>)
lr.fit(X_train_std, y_train)
print(<span class="hljs-string">&apos;Training accuracy:&apos;</span>, lr.score(X_train_std, y_train))
print(<span class="hljs-string">&apos;Test accuracy:&apos;</span>, lr.score(X_test_std, y_test))
</code></pre>
<pre><code>(&apos;Training accuracy:&apos;, 0.98373983739837401)
(&apos;Test accuracy:&apos;, 0.96363636363636362)
</code></pre><p>&#x52A0;&#x4E0A; L1 &#x6B63;&#x5219;&#x9879;&#x540E;&#xFF0C;&#x8BAD;&#x7EC3;&#x96C6;&#x548C;&#x6D4B;&#x8BD5;&#x96C6;&#x4E0A;&#x7684;&#x8868;&#x73B0;&#x76F8;&#x8FD1;&#xFF0C;&#x6CA1;&#x6709;&#x8FC7;&#x62DF;&#x5408;</p>
<pre><code class="lang-python">lr.intercept_
</code></pre>
<pre><code>array([-0.26943618, -0.12656436, -0.79402866])
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x7528;&#x4E86; One-vs-Rest (OvR) &#x65B9;&#x6CD5;&#xFF0C;&#x6240;&#x4EE5;&#x4F1A;&#x51FA;&#x73B0;&#x4E09;&#x884C;&#x7CFB;&#x6570;</span>
lr.coef_
</code></pre>
<pre><code>array([[ 0.18750685,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        ,  0.56622652,  0.        ,  0.        ,  0.        ,
         0.        ,  0.        ,  1.60382013],
       [-0.74867392, -0.04330592, -0.00242426,  0.        ,  0.        ,
         0.        ,  0.        ,  0.        ,  0.        , -0.80946123,
         0.        ,  0.04873335, -0.44621713],
       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
         0.        , -0.7299406 ,  0.        ,  0.        ,  0.42356047,
        -0.33037171, -0.52828297,  0.        ]])
</code></pre><p>&#x53EF;&#x4EE5;&#x770B;&#x51FA;&#x7CFB;&#x6570;&#x77E9;&#x9635;&#x662F;&#x7A00;&#x758F;&#x7684; (&#x53EA;&#x6709;&#x5C11;&#x6570;&#x975E;&#x96F6;&#x7CFB;&#x6570;)</p>
<pre><code class="lang-python"><span class="hljs-comment"># weights coeff of the different features for different regularization strengths</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

fig = plt.figure()
ax = plt.subplot(<span class="hljs-number">111</span>)

colors = [<span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;green&apos;</span>, <span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;cyan&apos;</span>, 
         <span class="hljs-string">&apos;magenta&apos;</span>, <span class="hljs-string">&apos;yellow&apos;</span>, <span class="hljs-string">&apos;black&apos;</span>, 
          <span class="hljs-string">&apos;pink&apos;</span>, <span class="hljs-string">&apos;lightgreen&apos;</span>, <span class="hljs-string">&apos;lightblue&apos;</span>, 
          <span class="hljs-string">&apos;gray&apos;</span>, <span class="hljs-string">&apos;indigo&apos;</span>, <span class="hljs-string">&apos;orange&apos;</span>]

weights, params = [], []
<span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">-4</span>, <span class="hljs-number">6</span>):
    lr = LogisticRegression(penalty=<span class="hljs-string">&apos;l1&apos;</span>, C=<span class="hljs-number">10</span>**c, random_state=<span class="hljs-number">0</span>)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[<span class="hljs-number">1</span>])
    params.append(<span class="hljs-number">10</span>**c)

weights = np.array(weights)

<span class="hljs-keyword">for</span> column, color <span class="hljs-keyword">in</span> zip(range(weights.shape[<span class="hljs-number">1</span>]), colors):
    plt.plot(params, weights[:, column],
             label=df_wine.columns[column+<span class="hljs-number">1</span>],
             color=color)
plt.axhline(<span class="hljs-number">0</span>, color=<span class="hljs-string">&apos;black&apos;</span>, linestyle=<span class="hljs-string">&apos;--&apos;</span>, linewidth=<span class="hljs-number">3</span>)
plt.xlim([<span class="hljs-number">10</span>**(<span class="hljs-number">-5</span>), <span class="hljs-number">10</span>**<span class="hljs-number">5</span>])
plt.ylabel(<span class="hljs-string">&apos;weight coefficient&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;C&apos;</span>)
plt.xscale(<span class="hljs-string">&apos;log&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
ax.legend(loc=<span class="hljs-string">&apos;upper center&apos;</span>, 
          bbox_to_anchor=(<span class="hljs-number">1.38</span>, <span class="hljs-number">1.03</span>),
          ncol=<span class="hljs-number">1</span>, fancybox=<span class="hljs-keyword">True</span>);
<span class="hljs-comment"># plt.savefig(&apos;./figures/l1_path.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_104_0.png" alt="png"></p>
<p>&#x968F;&#x7740; L1 &#x6B63;&#x5219;&#x9879;&#x589E;&#x5927;&#xFF0C;&#x65E0;&#x5173;&#x7279;&#x5F81;&#x522B;&#x6392;&#x9664;&#x51FA;&#x6A21;&#x578B; (&#x7CFB;&#x6570;&#x53D8;&#x4E3A; 0)&#xFF0C;&#x56E0;&#x6B64; L1 &#x6B63;&#x5219;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x7279;&#x5F81;&#x9009;&#x62E9;&#x7684;&#x4E00;&#x79CD;&#x65B9;&#x6CD5;</p>
<p>&#x7ED3;&#x5408; sklearn &#x7684; SelectFromModel &#x8FDB;&#x884C;&#x9009;&#x62E9;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel

model_l1 = SelectFromModel(lr, threshold=<span class="hljs-string">&apos;median&apos;</span>, prefit=<span class="hljs-keyword">True</span>)
X_l1_selected = model_l1.transform(X)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x67E5;&#x770B;&#x9009;&#x51FA;&#x4E86;&#x54EA;&#x51E0;&#x4E2A; feature, &#x9ED1;&#x8272;&#x662F;&#x9009;&#x51FA;&#x6765;&#x7684; </span>
mask = model_l1.get_support()
print(mask)
plt.matshow(mask.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>), cmap=<span class="hljs-string">&apos;gray_r&apos;</span>);
</code></pre>
<pre><code>[ True False  True  True False False  True False False  True  True False
  True]
</code></pre><p><img src="output_108_1.png" alt="png"></p>
<p><br>
<br></p>
<h3 id="tree-based-feature-selection"><a name="tree-based-feature-selection" class="plugin-anchor" href="#tree-based-feature-selection"><i class="fa fa-link" aria-hidden="true"></i></a>Tree-based feature selection</h3>
<p>&#x968F;&#x673A;&#x68EE;&#x6797;&#x7B97;&#x6CD5;&#x53EF;&#x4EE5;&#x6D4B;&#x91CF;&#x5404;&#x4E2A;&#x7279;&#x5F81;&#x7684;&#x91CD;&#x8981;&#x6027;&#xFF0C;&#x56E0;&#x6B64;&#x53EF;&#x4EE5;&#x4F5C;&#x4E3A;&#x7279;&#x5F81;&#x9009;&#x62E9;&#x7684;&#x4E00;&#x79CD;&#x624B;&#x6BB5;</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

feat_labels = df_wine.columns[<span class="hljs-number">1</span>:]

<span class="hljs-comment"># &#x4F7F;&#x7528; decision tree &#x6216; random forests &#x4E0D;&#x9700;&#x8981; standardization&#x6216; normalization</span>
forest = RandomForestClassifier(n_estimators=<span class="hljs-number">1000</span>,  
                                random_state=<span class="hljs-number">0</span>,
                                n_jobs=<span class="hljs-number">-1</span>)
forest.fit(X_train, y_train)

<span class="hljs-comment"># random forest &#x6BD4;&#x8F83;&#x7279;&#x6B8A;, &#x6709; feature_importances &#x8FD9;&#x4E2A; attribute</span>
importances = forest.feature_importances_

indices = np.argsort(importances)[::<span class="hljs-number">-1</span>]
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">for</span> i, idx <span class="hljs-keyword">in</span> enumerate(indices):
    print(<span class="hljs-string">&quot;%2d) %-*s %f&quot;</span> % (i + <span class="hljs-number">1</span>, <span class="hljs-number">30</span>, 
                            feat_labels[idx], 
                            importances[idx]))
</code></pre>
<pre><code> 1) Proline                        0.185412
 2) Flavanoids                     0.169830
 3) Color intensity                0.149659
 4) OD280/OD315 of diluted wines   0.127238
 5) Alcohol                        0.117432
 6) Hue                            0.057148
 7) Total phenols                  0.053042
 8) Magnesium                      0.034654
 9) Malic acid                     0.027965
10) Proanthocyanins                0.025731
11) Alcalinity of ash              0.021699
12) Nonflavanoid phenols           0.017372
13) Ash                            0.012818
</code></pre><pre><code class="lang-python">plt.title(<span class="hljs-string">&apos;Feature Importances&apos;</span>)
plt.bar(range(X_train.shape[<span class="hljs-number">1</span>]), 
        importances[indices],
        color=<span class="hljs-string">&apos;lightblue&apos;</span>, 
        align=<span class="hljs-string">&apos;center&apos;</span>)

plt.xticks(range(X_train.shape[<span class="hljs-number">1</span>]), 
           feat_labels[indices], rotation=<span class="hljs-number">90</span>)
plt.xlim([<span class="hljs-number">-1</span>, X_train.shape[<span class="hljs-number">1</span>]])
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./random_forest.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_114_0.png" alt="png"></p>
<p>&#x7ED3;&#x5408; Sklearn &#x7684; SelectFromModel &#x8FDB;&#x884C;&#x7279;&#x5F81;&#x9009;&#x62E9;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> SelectFromModel
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier

select_rf = SelectFromModel(forest, threshold=<span class="hljs-number">0.1</span>, prefit=<span class="hljs-keyword">True</span>)

<span class="hljs-comment"># &#x6216;&#x8005;&#x91CD;&#x65B0;&#x8BAD;&#x7EC3;&#x4E00;&#x4E2A;&#x6A21;&#x578B;</span>
<span class="hljs-comment"># select = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1), threshold=0.15, prefit=True)</span>
<span class="hljs-comment"># select.fit(X_train, y_train)</span>

X_train_rf = select_rf.transform(X_train)

print(X_train.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># &#x539F;&#x59CB;&#x7279;&#x5F81;&#x7EF4;&#x5EA6;</span>
print(X_train_rf.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># &#x7279;&#x5F81;&#x9009;&#x62E9;&#x540E;&#x7279;&#x5F81;&#x7EF4;&#x5EA6;</span>
</code></pre>
<pre><code>13
5
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x67E5;&#x770B;&#x9009;&#x51FA;&#x7684;&#x7279;&#x5F81;</span>
mask = select_rf.get_support()
<span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> feat_labels[mask]:
    print(f)
</code></pre>
<pre><code>Alcohol
Flavanoids
Color intensity
OD280/OD315 of diluted wines
Proline
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x53EF;&#x89C6;&#x5316;&#x7279;&#x5F81;&#x9009;&#x62E9;&#x7ED3;&#x679C;&#xFF0C;&#x9ED1;&#x8272;&#x7684;&#x662F;&#x9009;&#x4E2D;&#x7684;&#xFF0C;&#x767D;&#x8272;&#x7684;&#x662F;&#x6EE4;&#x8FC7;&#x7684;</span>
mask = select_rf.get_support()
print(mask)
plt.matshow(mask.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>), cmap=<span class="hljs-string">&apos;gray_r&apos;</span>);
</code></pre>
<pre><code>[ True False False False False False  True False False  True False  True
  True]
</code></pre><p><img src="output_118_1.png" alt="png"></p>
<p>&#x4E5F;&#x80FD;&#x5C06;&#x968F;&#x673A;&#x68EE;&#x6797;&#x548C; Sequential selection &#x7ED3;&#x5408;&#x8D77;&#x6765;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.feature_selection <span class="hljs-keyword">import</span> RFE
select = RFE(RandomForestClassifier(n_estimators=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">0</span>), 
             n_features_to_select=<span class="hljs-number">3</span>)

select.fit(X_train, y_train)
<span class="hljs-comment"># visualize the selected features:</span>
mask = select.get_support()
plt.matshow(mask.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>), cmap=<span class="hljs-string">&apos;gray_r&apos;</span>);
</code></pre>
<p><img src="output_120_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="feature-extraction"><a name="feature-extraction" class="plugin-anchor" href="#feature-extraction"><i class="fa fa-link" aria-hidden="true"></i></a>Feature extraction</h1>
<p>&#x4E0A;&#x4E00;&#x8282;&#x6211;&#x4EEC;&#x5B66;&#x4E60;&#x4E86; feature selection, &#x8FD9;&#x4E00;&#x8282;&#x6211;&#x4EEC;&#x8981;&#x5B66;&#x964D;&#x7EF4;&#x7684;&#x53E6;&#x4E00;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;feature extraction</p>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="unsupervised-dimensionality-reduction-via-principal-component-analysis"><a name="unsupervised-dimensionality-reduction-via-principal-component-analysis" class="plugin-anchor" href="#unsupervised-dimensionality-reduction-via-principal-component-analysis"><i class="fa fa-link" aria-hidden="true"></i></a>Unsupervised dimensionality reduction via principal component analysis</h2>
<ul>
<li>improve computational efficiency</li>
<li>help to reduce the <strong>curse of dimensionality</strong></li>
<li>unsupervised linear transformation technique</li>
<li>identify patterns in data based on the correlation between features</li>
<li>PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one.</li>
</ul>
<p>summarize PCA algorithm:</p>
<ol>
<li>Standardize the d-dimensional dataset.</li>
<li>Construct the covariance matrix.</li>
<li>Decompose the covariance matrix into its eigenvectors and eigenvalues.</li>
<li>Select k eigenvectors that correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace ( k &#x2264; d ).</li>
<li>Construct a projection matrix W from the &quot;top&quot; k eigenvectors.</li>
<li>Transform the d -dimensional input dataset X using the projection matrix W to obtain the new k -dimensional feature subspace.</li>
</ol>
<p>&#x7B80;&#x5355;&#x6765;&#x8BF4;&#xFF0C;PCA &#x662F;&#x5728;&#x627E;&#x5BFB; variance &#x6700;&#x5927;&#x7684;&#x65B9;&#x5411;</p>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x4ECD;&#x7136;&#x4F7F;&#x7528; <em>Wine</em> dataset</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

df_wine = pd.read_csv(<span class="hljs-string">&apos;data/wine.data&apos;</span>, header=<span class="hljs-keyword">None</span>)

df_wine.columns = [<span class="hljs-string">&apos;Class label&apos;</span>, <span class="hljs-string">&apos;Alcohol&apos;</span>, <span class="hljs-string">&apos;Malic acid&apos;</span>, <span class="hljs-string">&apos;Ash&apos;</span>, 
<span class="hljs-string">&apos;Alcalinity of ash&apos;</span>, <span class="hljs-string">&apos;Magnesium&apos;</span>, <span class="hljs-string">&apos;Total phenols&apos;</span>, 
<span class="hljs-string">&apos;Flavanoids&apos;</span>, <span class="hljs-string">&apos;Nonflavanoid phenols&apos;</span>, <span class="hljs-string">&apos;Proanthocyanins&apos;</span>, 
<span class="hljs-string">&apos;Color intensity&apos;</span>, <span class="hljs-string">&apos;Hue&apos;</span>, <span class="hljs-string">&apos;OD280/OD315 of diluted wines&apos;</span>, <span class="hljs-string">&apos;Proline&apos;</span>]

df_wine.head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Class label</th>
      <th>Alcohol</th>
      <th>Malic acid</th>
      <th>Ash</th>
      <th>Alcalinity of ash</th>
      <th>Magnesium</th>
      <th>Total phenols</th>
      <th>Flavanoids</th>
      <th>Nonflavanoid phenols</th>
      <th>Proanthocyanins</th>
      <th>Color intensity</th>
      <th>Hue</th>
      <th>OD280/OD315 of diluted wines</th>
      <th>Proline</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>14.23</td>
      <td>1.71</td>
      <td>2.43</td>
      <td>15.6</td>
      <td>127</td>
      <td>2.80</td>
      <td>3.06</td>
      <td>0.28</td>
      <td>2.29</td>
      <td>5.64</td>
      <td>1.04</td>
      <td>3.92</td>
      <td>1065</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>13.20</td>
      <td>1.78</td>
      <td>2.14</td>
      <td>11.2</td>
      <td>100</td>
      <td>2.65</td>
      <td>2.76</td>
      <td>0.26</td>
      <td>1.28</td>
      <td>4.38</td>
      <td>1.05</td>
      <td>3.40</td>
      <td>1050</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>13.16</td>
      <td>2.36</td>
      <td>2.67</td>
      <td>18.6</td>
      <td>101</td>
      <td>2.80</td>
      <td>3.24</td>
      <td>0.30</td>
      <td>2.81</td>
      <td>5.68</td>
      <td>1.03</td>
      <td>3.17</td>
      <td>1185</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>14.37</td>
      <td>1.95</td>
      <td>2.50</td>
      <td>16.8</td>
      <td>113</td>
      <td>3.85</td>
      <td>3.49</td>
      <td>0.24</td>
      <td>2.18</td>
      <td>7.80</td>
      <td>0.86</td>
      <td>3.45</td>
      <td>1480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>13.24</td>
      <td>2.59</td>
      <td>2.87</td>
      <td>21.0</td>
      <td>118</td>
      <td>2.80</td>
      <td>2.69</td>
      <td>0.39</td>
      <td>1.82</td>
      <td>4.32</td>
      <td>1.04</td>
      <td>2.93</td>
      <td>735</td>
    </tr>
  </tbody>
</table>
</div>



<p>Splitting the data into 70% training and 30% test subsets.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split

X, y = df_wine.iloc[:, <span class="hljs-number">1</span>:].values, df_wine.iloc[:, <span class="hljs-number">0</span>].values

X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)
</code></pre>
<p>Standardizing the data.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

sc = StandardScaler()
X_train_std = sc.fit_transform(X_train)
X_test_std = sc.transform(X_test)
</code></pre>
<p>&#x8BA1;&#x7B97;&#x534F;&#x65B9;&#x5DEE;&#x77E9;&#x9635;&#xFF1A;
<script type="math/tex; "> \sigma_{jk}=\frac 1 n \sum^n_{i=1}(x_j^{(i)} - \mu_j)(x_k^{(i)} - \mu_k) </script>
&#x901A;&#x8FC7;&#x7279;&#x5F81;&#x5206;&#x89E3;&#x5F97;&#x5230;&#x7279;&#x5F81;&#x503C; <script type="math/tex; "> \lambda </script> &#x548C;&#x7279;&#x5F81;&#x5411;&#x91CF; <script type="math/tex; "> v </script></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-comment"># compute covariance matrix</span>
cov_mat = np.cov(X_train_std.T)

<span class="hljs-comment"># get eigenvalues and eigenvectors</span>
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
<span class="hljs-comment"># eigen_vecs 13*13</span>
print(<span class="hljs-string">&apos;Eigenvalues \n %s&apos;</span> % eigen_vals)
</code></pre>
<pre><code>Eigenvalues 
 [ 4.8923083   2.46635032  1.42809973  1.01233462  0.84906459  0.60181514
  0.52251546  0.08414846  0.33051429  0.29595018  0.16831254  0.21432212
  0.2399553 ]
</code></pre><p><br>
<br></p>
<h3 id="total-and-explained-variance"><a name="total-and-explained-variance" class="plugin-anchor" href="#total-and-explained-variance"><i class="fa fa-link" aria-hidden="true"></i></a>Total and explained variance</h3>
<p>The variance explained ratio of an eigenvalue is simply the fraction of an eigenvalue and the total sum of the eigenvalues:
<script type="math/tex; ">\frac{\lambda_j}{\sum^d_{i=1}\lambda_i}</script></p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python">tot = sum(eigen_vals)
var_exp = [(i / tot) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sorted(eigen_vals, reverse=<span class="hljs-keyword">True</span>)]
cum_var_exp = np.cumsum(var_exp)  <span class="hljs-comment"># cumulative sum of explained variance</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># plot variance</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

plt.bar(range(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), var_exp, alpha=<span class="hljs-number">0.5</span>, align=<span class="hljs-string">&apos;center&apos;</span>,
        label=<span class="hljs-string">&apos;individual explained variance&apos;</span>)
plt.step(range(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), cum_var_exp, where=<span class="hljs-string">&apos;mid&apos;</span>,
         label=<span class="hljs-string">&apos;cumulative explained variance&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Explained variance ratio&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Principal components&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;best&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/pca1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_138_0.png" alt="png"></p>
<p>&#x7B2C;&#x4E00;&#x4E2A; component &#x80FD;&#x89E3;&#x91CA;&#x5C06;&#x8FD1; 40% &#x7684; variance, &#x524D;&#x4E24;&#x4E2A; components &#x80FD;&#x89E3;&#x91CA;&#x8FD1; 60%</p>
<p><br>
<br></p>
<h3 id="feature-transformation"><a name="feature-transformation" class="plugin-anchor" href="#feature-transformation"><i class="fa fa-link" aria-hidden="true"></i></a>Feature transformation</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># Make a list of (eigenvalue, eigenvector) tuples</span>
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(eigen_vals))]

<span class="hljs-comment"># Sort the (eigenvalue, eigenvector) tuples from high to low</span>
eigen_pairs.sort(reverse=<span class="hljs-keyword">True</span>)
</code></pre>
<p>we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection.</p>
<p>In practice, the number of principal components has to be determined from a trade-off between computational efficiency and the performance of the classifier.</p>
<pre><code class="lang-python">w = np.column_stack([eigen_pairs[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], eigen_pairs[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]])
print(w)  <span class="hljs-comment"># 13*2 projection matrix from the top two eigenvectors</span>
</code></pre>
<pre><code>[[ 0.14669811 -0.50417079]
 [-0.24224554 -0.24216889]
 [-0.02993442 -0.28698484]
 [-0.25519002  0.06468718]
 [ 0.12079772 -0.22995385]
 [ 0.38934455 -0.09363991]
 [ 0.42326486 -0.01088622]
 [-0.30634956 -0.01870216]
 [ 0.30572219 -0.03040352]
 [-0.09869191 -0.54527081]
 [ 0.30032535  0.27924322]
 [ 0.36821154  0.174365  ]
 [ 0.29259713 -0.36315461]]
</code></pre><p>&#x5229;&#x7528; projection matrix <script type="math/tex; ">W</script>, &#x6211;&#x4EEC;&#x53EF;&#x4EE5;&#x5F97;&#x5230;&#x8F6C;&#x6362;&#x540E;&#x7684;&#x6570;&#x636E; <script type="math/tex; ">x^\prime</script>
<script type="math/tex; ">x^\prime = xW</script></p>
<pre><code class="lang-python"><span class="hljs-comment"># transform the entire 124&#xD7;13-dimensional training dataset onto the two principal components</span>
X_train_pca = X_train_std.dot(w)
colors = [<span class="hljs-string">&apos;r&apos;</span>, <span class="hljs-string">&apos;b&apos;</span>, <span class="hljs-string">&apos;g&apos;</span>]
markers = [<span class="hljs-string">&apos;s&apos;</span>, <span class="hljs-string">&apos;x&apos;</span>, <span class="hljs-string">&apos;o&apos;</span>]

<span class="hljs-keyword">for</span> l, c, m <span class="hljs-keyword">in</span> zip(np.unique(y_train), colors, markers):
    plt.scatter(X_train_pca[y_train==l, <span class="hljs-number">0</span>], 
                X_train_pca[y_train==l, <span class="hljs-number">1</span>], 
                c=c, label=l, marker=m)

plt.xlabel(<span class="hljs-string">&apos;PC 1&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;PC 2&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;lower left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/pca2.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_147_0.png" alt="png"></p>
<p>data is more spread along the x-axis, a linear classier will likely be able to separate the classes well</p>
<p><br>
<br></p>
<h3 id="principal-component-analysis-in-scikit-learn"><a name="principal-component-analysis-in-scikit-learn" class="plugin-anchor" href="#principal-component-analysis-in-scikit-learn"><i class="fa fa-link" aria-hidden="true"></i></a>Principal component analysis in scikit-learn</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA

pca = PCA()
X_train_pca = pca.fit_transform(X_train_std)
pca.explained_variance_ratio_
</code></pre>
<pre><code>array([ 0.37329648,  0.18818926,  0.10896791,  0.07724389,  0.06478595,
        0.04592014,  0.03986936,  0.02521914,  0.02258181,  0.01830924,
        0.01635336,  0.01284271,  0.00642076])
</code></pre><pre><code class="lang-python">plt.bar(range(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), pca.explained_variance_ratio_, alpha=<span class="hljs-number">0.5</span>, align=<span class="hljs-string">&apos;center&apos;</span>)
plt.step(range(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), np.cumsum(pca.explained_variance_ratio_), where=<span class="hljs-string">&apos;mid&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Explained variance ratio&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Principal components&apos;</span>);
</code></pre>
<p><img src="output_153_0.png" alt="png"></p>
<pre><code class="lang-python">pca = PCA(n_components=<span class="hljs-number">2</span>)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)
</code></pre>
<pre><code class="lang-python">plt.scatter(X_train_pca[:,<span class="hljs-number">0</span>], X_train_pca[:,<span class="hljs-number">1</span>])
plt.xlabel(<span class="hljs-string">&apos;PC 1&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;PC 2&apos;</span>);
</code></pre>
<p><img src="output_155_0.png" alt="png"></p>
<p>If we compare the PCA projection via scikit-learn with our own PCA implementation, we notice that the plot above is a mirror image of the previous PCA via our step-by-step approach.<br>Note that this is not due to an error in any of those two implementations, but the reason for this difference is that, depending on the eigensolver, eigenvectors can have either negative or positive signs.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_decision_regions</span><span class="hljs-params">(X, y, classifier, resolution=<span class="hljs-number">0.02</span>)</span>:</span>

    <span class="hljs-comment"># setup marker generator and color map</span>
    markers = (<span class="hljs-string">&apos;s&apos;</span>, <span class="hljs-string">&apos;x&apos;</span>, <span class="hljs-string">&apos;o&apos;</span>, <span class="hljs-string">&apos;^&apos;</span>, <span class="hljs-string">&apos;v&apos;</span>)
    colors = (<span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;lightgreen&apos;</span>, <span class="hljs-string">&apos;gray&apos;</span>, <span class="hljs-string">&apos;cyan&apos;</span>)
    cmap = ListedColormap(colors[:len(np.unique(y))])

    <span class="hljs-comment"># plot the decision surface</span>
    x1_min, x1_max = X[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
    x2_min, x2_max = X[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                         np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=<span class="hljs-number">0.4</span>, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    <span class="hljs-comment"># plot class samples</span>
    <span class="hljs-keyword">for</span> idx, cl <span class="hljs-keyword">in</span> enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, <span class="hljs-number">0</span>], y=X[y == cl, <span class="hljs-number">1</span>],
                    alpha=<span class="hljs-number">0.8</span>, c=cmap(idx),
                    marker=markers[idx], label=cl)
</code></pre>
<p>Training logistic regression classifier using the first 2 principal components.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression

lr = LogisticRegression()
lr = lr.fit(X_train_pca, y_train)
</code></pre>
<pre><code class="lang-python">plot_decision_regions(X_train_pca, y_train, classifier=lr)
plt.xlabel(<span class="hljs-string">&apos;PC 1&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;PC 2&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;lower left&apos;</span>)
plt.tight_layout();
<span class="hljs-comment"># plt.savefig(&apos;./figures/pca3.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_160_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5728;&#x6D4B;&#x8BD5;&#x96C6;&#x4E0A;&#x6D4B;&#x8BD5;</span>
plot_decision_regions(X_test_pca, y_test, classifier=lr)
plt.xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;PC2&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;lower left&apos;</span>);
<span class="hljs-comment"># &#x5206;&#x7C7B;&#x6548;&#x679C;&#x4E5F;&#x5F88;&#x4E0D;&#x9519;</span>
</code></pre>
<p><img src="output_161_0.png" alt="png"></p>
<p><br>
<br></p>
<p><br>
<br></p>
<h1 id="using-kernel-principal-component-analysis-for-nonlinear-mappings"><a name="using-kernel-principal-component-analysis-for-nonlinear-mappings" class="plugin-anchor" href="#using-kernel-principal-component-analysis-for-nonlinear-mappings"><i class="fa fa-link" aria-hidden="true"></i></a>Using kernel principal component analysis for nonlinear mappings</h1>
<p>via kernel PCA, we perform a nonlinear mapping that transforms the data onto a higher-dimensional space and use standard PCA in this higher-dimensional space to project the data back onto a lower-dimensional space where the samples can be separated by a linear classifier</p>
<p>[<a href="#sections">back to top</a>]</p>
<p>most commonly used kernel:</p>
<ul>
<li>polynomial kernel</li>
<li>hyperbolic  tangent (sigmoid) kernel</li>
<li>Radial Basis Function (RBF)</li>
</ul>
<p>to implement RBF kernel PCA:</p>
<ol>
<li>compute the kernel (similarity) matrix k</li>
<li>center the kernel matrix k </li>
<li>collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, ranked by decreasing magnitude.</li>
</ol>
<p><br>
<br></p>
<h2 id="implementing-a-kernel-principal-component-analysis-in-python"><a name="implementing-a-kernel-principal-component-analysis-in-python" class="plugin-anchor" href="#implementing-a-kernel-principal-component-analysis-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implementing a kernel principal component analysis in Python</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>Radial Basis Function (RBF) or Gaussian kernel:
\begin{align}
k(x^{(i)}, x^{(j)}) =&amp; exp(-\frac{||x^{(i)} - x^{(j)}||^2}{2\sigma^2}) \
                    =&amp; exp(-\gamma ||x^{(i)} - x^{(j)}||^2)
\end{align}</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> pdist, squareform
<span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> exp
<span class="hljs-keyword">from</span> scipy.linalg <span class="hljs-keyword">import</span> eigh
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rbf_kernel_pca</span><span class="hljs-params">(X, gamma, n_components)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;
    RBF kernel PCA implementation.

    Parameters
    ------------
    X: {NumPy ndarray}, shape = [n_samples, n_features]

    gamma: float
      Tuning parameter of the RBF kernel

    n_components: int
      Number of principal components to return

    Returns
    ------------
     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]
       Projected dataset   

    &quot;&quot;&quot;</span>
    <span class="hljs-comment"># Calculate pairwise squared Euclidean distances</span>
    <span class="hljs-comment"># in the MxN dimensional dataset.</span>
    sq_dists = pdist(X, <span class="hljs-string">&apos;sqeuclidean&apos;</span>)

    <span class="hljs-comment"># Convert pairwise distances into a square matrix.</span>
    mat_sq_dists = squareform(sq_dists)

    <span class="hljs-comment"># Compute the symmetric kernel matrix.</span>
    K = exp(-gamma * mat_sq_dists)

    <span class="hljs-comment"># Center the kernel matrix.</span>
    N = K.shape[<span class="hljs-number">0</span>]
    one_n = np.ones((N,N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)

    <span class="hljs-comment"># Obtaining eigenpairs from the centered kernel matrix</span>
    <span class="hljs-comment"># numpy.eigh returns them in sorted order</span>
    eigvals, eigvecs = eigh(K)

    <span class="hljs-comment"># Collect the top k eigenvectors (projected samples)</span>
    X_pc = np.column_stack((eigvecs[:, -i]
                            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, n_components + <span class="hljs-number">1</span>)))

    <span class="hljs-keyword">return</span> X_pc
</code></pre>
<h3 id="example-1-separating-half-moon-shapes"><a name="example-1-separating-half-moon-shapes" class="plugin-anchor" href="#example-1-separating-half-moon-shapes"><i class="fa fa-link" aria-hidden="true"></i></a>Example 1: Separating half-moon shapes</h3>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x5EFA;&#x9020;&#x6708;&#x5F62;&#x6570;&#x636E;&#xFF0C;&#x7528;&#x4EE5;&#x6F14;&#x793A;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons

X, y = make_moons(n_samples=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">123</span>)

plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/half_moon_1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_175_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># standardize PCA</span>
<span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

scaler = StandardScaler()
X_std = scaler.fit_transform(X)

scikit_pca = PCA(n_components=<span class="hljs-number">2</span>)
X_spca = scikit_pca.fit_transform(X_std)

fig, ax = plt.subplots(nrows=<span class="hljs-number">1</span>,ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>))

ax[<span class="hljs-number">0</span>].scatter(X_spca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_spca[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">0</span>].scatter(X_spca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_spca[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">1</span>].scatter(X_spca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">50</span>,<span class="hljs-number">1</span>)), 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">1</span>].scatter(X_spca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">50</span>,<span class="hljs-number">1</span>)),
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
ax[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;PC2&apos;</span>)
ax[<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>])
ax[<span class="hljs-number">1</span>].set_yticks([])
ax[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/half_moon_2.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_176_0.png" alt="png"></p>
<p>a linear classier would not be able to perform well</p>
<pre><code class="lang-python"><span class="hljs-comment"># kernel PCA function rbf_kernel_pca</span>
<span class="hljs-keyword">from</span> matplotlib.ticker <span class="hljs-keyword">import</span> FormatStrFormatter

X_kpca = rbf_kernel_pca(X, gamma=<span class="hljs-number">15</span>, n_components=<span class="hljs-number">2</span>)

fig, ax = plt.subplots(nrows=<span class="hljs-number">1</span>,ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>))
ax[<span class="hljs-number">0</span>].scatter(X_kpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_kpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">0</span>].scatter(X_kpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_kpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">1</span>].scatter(X_kpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">50</span>,<span class="hljs-number">1</span>)), 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">1</span>].scatter(X_kpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">50</span>,<span class="hljs-number">1</span>)),
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
ax[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;PC2&apos;</span>)
ax[<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>])
ax[<span class="hljs-number">1</span>].set_yticks([])
ax[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
ax[<span class="hljs-number">0</span>].xaxis.set_major_formatter(FormatStrFormatter(<span class="hljs-string">&apos;%0.1f&apos;</span>))
ax[<span class="hljs-number">1</span>].xaxis.set_major_formatter(FormatStrFormatter(<span class="hljs-string">&apos;%0.1f&apos;</span>))

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/half_moon_3.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_178_0.png" alt="png"></p>
<p>two classes (circles and triangles) are linearly well separated</p>
<h3 id="example-2-separating-concentric-circles"><a name="example-2-separating-concentric-circles" class="plugin-anchor" href="#example-2-separating-concentric-circles"><i class="fa fa-link" aria-hidden="true"></i></a>Example 2: Separating concentric circles</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles

X, y = make_circles(n_samples=<span class="hljs-number">1000</span>, random_state=<span class="hljs-number">123</span>, noise=<span class="hljs-number">0.1</span>, factor=<span class="hljs-number">0.2</span>)

plt.scatter(X[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
plt.scatter(X[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/circles_1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_182_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># standard PCA</span>
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

scikit_pca = PCA(n_components=<span class="hljs-number">2</span>)
X_spca = scikit_pca.fit_transform(X_std)

fig, ax = plt.subplots(nrows=<span class="hljs-number">1</span>,ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>))

ax[<span class="hljs-number">0</span>].scatter(X_spca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_spca[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">0</span>].scatter(X_spca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_spca[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">1</span>].scatter(X_spca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">500</span>,<span class="hljs-number">1</span>)), 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">1</span>].scatter(X_spca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">500</span>,<span class="hljs-number">1</span>)),
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
ax[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;PC2&apos;</span>)
ax[<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>])
ax[<span class="hljs-number">1</span>].set_yticks([])
ax[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/circles_2.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_183_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># kernel RBF</span>
X_kpca = rbf_kernel_pca(X, gamma=<span class="hljs-number">15</span>, n_components=<span class="hljs-number">2</span>)

fig, ax = plt.subplots(nrows=<span class="hljs-number">1</span>,ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">7</span>,<span class="hljs-number">3</span>))
ax[<span class="hljs-number">0</span>].scatter(X_kpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_kpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">0</span>].scatter(X_kpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_kpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">1</span>].scatter(X_kpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">500</span>,<span class="hljs-number">1</span>)), 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
ax[<span class="hljs-number">1</span>].scatter(X_kpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">500</span>,<span class="hljs-number">1</span>)),
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

ax[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
ax[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;PC2&apos;</span>)
ax[<span class="hljs-number">1</span>].set_ylim([<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>])
ax[<span class="hljs-number">1</span>].set_yticks([])
ax[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/circles_3.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_184_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="projecting-new-data-points"><a name="projecting-new-data-points" class="plugin-anchor" href="#projecting-new-data-points"><i class="fa fa-link" aria-hidden="true"></i></a>Projecting new data points</h2>
<p>learn how to project data points that were not part of the training dataset</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> pdist, squareform
<span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> exp
<span class="hljs-keyword">from</span> scipy.linalg <span class="hljs-keyword">import</span> eigh
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rbf_kernel_pca</span><span class="hljs-params">(X, gamma, n_components)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;
    RBF kernel PCA implementation.

    Parameters
    ------------
    X: {NumPy ndarray}, shape = [n_samples, n_features]

    gamma: float
      Tuning parameter of the RBF kernel

    n_components: int
      Number of principal components to return

    Returns
    ------------
     X_pc: {NumPy ndarray}, shape = [n_samples, k_features]
       Projected dataset   

     lambdas: list
       Eigenvalues

    &quot;&quot;&quot;</span>
    <span class="hljs-comment"># Calculate pairwise squared Euclidean distances</span>
    <span class="hljs-comment"># in the MxN dimensional dataset.</span>
    sq_dists = pdist(X, <span class="hljs-string">&apos;sqeuclidean&apos;</span>)

    <span class="hljs-comment"># Convert pairwise distances into a square matrix.</span>
    mat_sq_dists = squareform(sq_dists)

    <span class="hljs-comment"># Compute the symmetric kernel matrix.</span>
    K = exp(-gamma * mat_sq_dists)

    <span class="hljs-comment"># Center the kernel matrix.</span>
    N = K.shape[<span class="hljs-number">0</span>]
    one_n = np.ones((N,N)) / N
    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)

    <span class="hljs-comment"># Obtaining eigenpairs from the centered kernel matrix</span>
    <span class="hljs-comment"># numpy.eigh returns them in sorted order</span>
    eigvals, eigvecs = eigh(K)

    <span class="hljs-comment"># Collect the top k eigenvectors (projected samples)</span>
    alphas = np.column_stack((eigvecs[:,-i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,n_components+<span class="hljs-number">1</span>)))

    <span class="hljs-comment"># Collect the corresponding eigenvalues</span>
    lambdas = [eigvals[-i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,n_components+<span class="hljs-number">1</span>)]

    <span class="hljs-keyword">return</span> alphas, lambdas
</code></pre>
<pre><code class="lang-python">X, y = make_moons(n_samples=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">123</span>)
alphas, lambdas = rbf_kernel_pca(X, gamma=<span class="hljs-number">15</span>, n_components=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python">x_new = X[<span class="hljs-number">25</span>]
x_new
</code></pre>
<pre><code>array([ 1.8713,  0.0093])
</code></pre><pre><code class="lang-python">x_proj = alphas[<span class="hljs-number">25</span>] <span class="hljs-comment"># original projection</span>
x_proj
</code></pre>
<pre><code>array([ 0.0788])
</code></pre><pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">project_x</span><span class="hljs-params">(x_new, X, gamma, alphas, lambdas)</span>:</span>
    pair_dist = np.array([np.sum((x_new-row)**<span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> X])
    k = np.exp(-gamma * pair_dist)
    <span class="hljs-keyword">return</span> k.dot(alphas / lambdas)

<span class="hljs-comment"># projection of the &quot;new&quot; datapoint</span>
x_reproj = project_x(x_new, X, gamma=<span class="hljs-number">15</span>, alphas=alphas, lambdas=lambdas)
x_reproj
</code></pre>
<pre><code>array([ 0.0788])
</code></pre><pre><code class="lang-python">plt.scatter(alphas[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">50</span>)), 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>,alpha=<span class="hljs-number">0.5</span>)
plt.scatter(alphas[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], np.zeros((<span class="hljs-number">50</span>)), 
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
plt.scatter(x_proj, <span class="hljs-number">0</span>, color=<span class="hljs-string">&apos;black&apos;</span>, label=<span class="hljs-string">&apos;original projection of point X[25]&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, s=<span class="hljs-number">100</span>)
plt.scatter(x_reproj, <span class="hljs-number">0</span>, color=<span class="hljs-string">&apos;green&apos;</span>, label=<span class="hljs-string">&apos;remapped point X[25]&apos;</span>, marker=<span class="hljs-string">&apos;x&apos;</span>, s=<span class="hljs-number">500</span>)
plt.legend(scatterpoints=<span class="hljs-number">1</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/reproject.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_193_0.png" alt="png"></p>
<p>project correctly</p>
<p><br>
<br></p>
<h2 id="kernel-principal-component-analysis-in-scikit-learn"><a name="kernel-principal-component-analysis-in-scikit-learn" class="plugin-anchor" href="#kernel-principal-component-analysis-in-scikit-learn"><i class="fa fa-link" aria-hidden="true"></i></a>Kernel principal component analysis in scikit-learn</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> KernelPCA

X, y = make_moons(n_samples=<span class="hljs-number">100</span>, random_state=<span class="hljs-number">123</span>)
scikit_kpca = KernelPCA(n_components=<span class="hljs-number">2</span>, kernel=<span class="hljs-string">&apos;rbf&apos;</span>, gamma=<span class="hljs-number">15</span>)
X_skernpca = scikit_kpca.fit_transform(X)

plt.scatter(X_skernpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], X_skernpca[y==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, alpha=<span class="hljs-number">0.5</span>)
plt.scatter(X_skernpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_skernpca[y==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, alpha=<span class="hljs-number">0.5</span>)

plt.xlabel(<span class="hljs-string">&apos;PC1&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;PC2&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/scikit_kpca.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_198_0.png" alt="png"></p>
<h3 id="&#x7279;&#x5F81;&#x5DE5;&#x7A0B;checklist"><a name="&#x7279;&#x5F81;&#x5DE5;&#x7A0B;checklist" class="plugin-anchor" href="#&#x7279;&#x5F81;&#x5DE5;&#x7A0B;checklist"><i class="fa fa-link" aria-hidden="true"></i></a>&#x7279;&#x5F81;&#x5DE5;&#x7A0B;checklist</h3>
<ul>
<li>Do you have domain knowledge? If yes, construct a better set of ad hoc&#x201D;&#x201D; features</li>
<li>Are your features commensurate? If no, consider normalizing them.</li>
<li>Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you.</li>
<li>Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)? If no, construct disjunctive features or weighted sums of feature</li>
<li>Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results.</li>
<li>Do you need a predictor? If no, stop</li>
<li>Do you suspect your data is &#x201C;dirty&#x201D; (has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them.</li>
<li>Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the &#x201C;probe&#x201D; method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset.</li>
<li>Do you have new ideas, time, computational resources, and enough examples? If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection</li>
<li>Do you want a stable solution (to improve performance and/or understanding)? If yes, subsample your data and redo your analysis for several &#x201C;bootstrap&#x201D;.</li>
</ul>
<p><br>
<br></p>
<h2 id="&#x4E00;&#x4E2A;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#x8FDB;&#x884C;&#x53D8;&#x91CF;&#x9009;&#x62E9;&#x7684;&#x4F8B;&#x5B50;"><a name="&#x4E00;&#x4E2A;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#x8FDB;&#x884C;&#x53D8;&#x91CF;&#x9009;&#x62E9;&#x7684;&#x4F8B;&#x5B50;" class="plugin-anchor" href="#&#x4E00;&#x4E2A;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#x8FDB;&#x884C;&#x53D8;&#x91CF;&#x9009;&#x62E9;&#x7684;&#x4F8B;&#x5B50;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x4E00;&#x4E2A;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#x8FDB;&#x884C;&#x53D8;&#x91CF;&#x9009;&#x62E9;&#x7684;&#x4F8B;&#x5B50;</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/overfitting_underfitting_cartoon.svg" alt="overfitting underfitting cartoon"></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> cross_validation
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> linear_model
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> neighbors
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> ensemble
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> cluster

<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
</code></pre>
<pre><code class="lang-python">np.random.seed(<span class="hljs-number">123</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x6784;&#x5EFA; dataset, 50&#x4E2A; sample, 50&#x4E2A; feature</span>
X_all, y_all = datasets.make_regression(n_samples=<span class="hljs-number">50</span>, n_features=<span class="hljs-number">50</span>, n_informative=<span class="hljs-number">10</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># 50% train, 50% test</span>
X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, train_size=<span class="hljs-number">0.5</span>)
</code></pre>
<pre><code class="lang-python">X_train.shape, y_train.shape
</code></pre>
<pre><code>((25, 50), (25,))
</code></pre><pre><code class="lang-python">X_test.shape, y_test.shape
</code></pre>
<pre><code>((25, 50), (25,))
</code></pre><p><br>
<br></p>
<h2 id="linear-regression"><a name="linear-regression" class="plugin-anchor" href="#linear-regression"><i class="fa fa-link" aria-hidden="true"></i></a>Linear Regression</h2>
<p><script type="math/tex; "> \text{min}_{w, b} \sum_i || w^\mathsf{T}x_i + b  - y_i||^2 </script></p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># linear reg</span>
model = linear_model.LinearRegression()
</code></pre>
<pre><code class="lang-python">model.fit(X_train, y_train)
</code></pre>
<pre><code>/Users/alan/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to &apos;gelss&apos; driver.
  warnings.warn(mesg, RuntimeWarning)





LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre><pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sse</span><span class="hljs-params">(resid)</span>:</span>
    <span class="hljs-keyword">return</span> sum(resid**<span class="hljs-number">2</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x8BA1;&#x7B97; train data &#x7684; SSE </span>
resid_train = y_train - model.predict(X_train)
sse_train = sse(resid_train)
sse_train
</code></pre>
<pre><code>7.9634561748974877e-25
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x9884;&#x6D4B; test &#x518D;&#x8BA1;&#x7B97; test data &#x7684;SSE</span>
resid_test = y_test - model.predict(X_test)
sse_test = sse(resid_test)
sse_test
</code></pre>
<pre><code>213555.61203039085
</code></pre><p>&#x7ED3;&#x679C; test data &#x663E;&#x793A;&#x9884;&#x6D4B;&#x6548;&#x679C;&#x5F88;&#x5DEE;, &#x53EF;&#x80FD; overfitting</p>
<pre><code class="lang-python">model.score(X_train, y_train)
</code></pre>
<pre><code>1.0
</code></pre><pre><code class="lang-python">model.score(X_test, y_test)
</code></pre>
<pre><code>0.31407400675201724
</code></pre><pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_residuals_and_coeff</span><span class="hljs-params">(resid_train, resid_test, coeff)</span>:</span>
    fig, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">3</span>))
    axes[<span class="hljs-number">0</span>].bar(np.arange(len(resid_train)), resid_train)
    axes[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">&quot;sample number&quot;</span>)
    axes[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&quot;residual&quot;</span>)
    axes[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">&quot;training data&quot;</span>)
    axes[<span class="hljs-number">1</span>].bar(np.arange(len(resid_test)), resid_test)
    axes[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">&quot;sample number&quot;</span>)
    axes[<span class="hljs-number">1</span>].set_ylabel(<span class="hljs-string">&quot;residual&quot;</span>)
    axes[<span class="hljs-number">1</span>].set_title(<span class="hljs-string">&quot;testing data&quot;</span>)
    axes[<span class="hljs-number">2</span>].bar(np.arange(len(coeff)), coeff)
    axes[<span class="hljs-number">2</span>].set_xlabel(<span class="hljs-string">&quot;coefficient number&quot;</span>)
    axes[<span class="hljs-number">2</span>].set_ylabel(<span class="hljs-string">&quot;coefficient&quot;</span>)
    fig.tight_layout()
    <span class="hljs-keyword">return</span> fig, axes
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x753B;&#x51FA; residual</span>
fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_);
</code></pre>
<p><img src="output_222_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="ridge-regression"><a name="ridge-regression" class="plugin-anchor" href="#ridge-regression"><i class="fa fa-link" aria-hidden="true"></i></a>Ridge Regression</h2>
<p>L2 penalized, add squared sum of the weights to least-squares cost function</p>
<p><script type="math/tex; "> \text{min}_{w,b}  \sum_i || w^\mathsf{T}x_i + b  - y_i||^2  + \alpha ||w||_2^2</script> </p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x4F7F;&#x7528; Ridge &#x6B63;&#x5219;&#x5316;</span>
model = linear_model.Ridge(alpha=<span class="hljs-number">5</span>)
</code></pre>
<pre><code class="lang-python">model.fit(X_train, y_train)
</code></pre>
<pre><code>Ridge(alpha=5, copy_X=True, fit_intercept=True, max_iter=None,
   normalize=False, random_state=None, solver=&apos;auto&apos;, tol=0.001)
</code></pre><pre><code class="lang-python">resid_train = y_train - model.predict(X_train)
sse_train = sum(resid_train**<span class="hljs-number">2</span>)
sse_train
</code></pre>
<pre><code>3292.9620358692705
</code></pre><pre><code class="lang-python">resid_test = y_test - model.predict(X_test)
sse_test = sum(resid_test**<span class="hljs-number">2</span>)
sse_test
</code></pre>
<pre><code>209557.58585055024
</code></pre><p>train data&#x7684; SSE &#x63D0;&#x5347;&#x5F88;&#x591A;</p>
<pre><code class="lang-python"><span class="hljs-comment"># test model score &#x4ECD;&#x7136;&#x4E0D;&#x9AD8;</span>
model.score(X_train, y_train), model.score(X_test, y_test)
</code></pre>
<pre><code>(0.99003021243324718, 0.32691539290134652)
</code></pre><pre><code class="lang-python">fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
</code></pre>
<p><img src="output_232_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="lasso-regression"><a name="lasso-regression" class="plugin-anchor" href="#lasso-regression"><i class="fa fa-link" aria-hidden="true"></i></a>LASSO Regression</h2>
<p>L1-norm certain weights can become zero, useful as a supervised feature selection technique.</p>
<p><script type="math/tex; "> \text{min}_{w, b} \sum_i || w^\mathsf{T}x_i + b  - y_i||^2  + \alpha ||w||_1</script> </p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python">model = linear_model.Lasso(alpha=<span class="hljs-number">1.0</span>)
</code></pre>
<pre><code class="lang-python">model.fit(X_train, y_train)
</code></pre>
<pre><code>Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection=&apos;cyclic&apos;, tol=0.0001, warm_start=False)
</code></pre><pre><code class="lang-python">resid_train = y_train - model.predict(X_train)
sse_train = sse(resid_train)
sse_train
</code></pre>
<pre><code>309.74971389532328
</code></pre><pre><code class="lang-python">resid_test = y_test - model.predict(X_test)
sse_test = sse(resid_test)
sse_test
</code></pre>
<pre><code>1489.117606500263
</code></pre><p>&#x76F8;&#x8F83; Ridge, SSE &#x90FD;&#x51CF;&#x5C11;&#x5F88;&#x591A;</p>
<pre><code class="lang-python">fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
</code></pre>
<p><img src="output_241_0.png" alt="png"></p>
<p>&#x4E0A;&#x56FE;&#x53D1;&#x73B0;, coeff &#x6709;&#x5F88;&#x591A;&#x90FD;&#x662F;0</p>
<pre><code class="lang-python">alphas = np.logspace(<span class="hljs-number">-4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">100</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5BFB;&#x627E; LASSO &#x7684;&#x6700;&#x4F18;&#x53C2;&#x6570; alpha </span>
coeffs = np.zeros((len(alphas), X_train.shape[<span class="hljs-number">1</span>]))
sse_train = np.zeros_like(alphas)
sse_test = np.zeros_like(alphas)

<span class="hljs-keyword">for</span> n, alpha <span class="hljs-keyword">in</span> enumerate(alphas):
    model = linear_model.Lasso(alpha=alpha)
    model.fit(X_train, y_train)
    coeffs[n, :] = model.coef_
    resid = y_train - model.predict(X_train)
    sse_train[n] = sum(resid**<span class="hljs-number">2</span>)
    resid = y_test - model.predict(X_test)
    sse_test[n] = sum(resid**<span class="hljs-number">2</span>)
</code></pre>
<pre><code>/Users/alan/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations
  ConvergenceWarning)
</code></pre><pre><code class="lang-python">fig, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">12</span>, <span class="hljs-number">4</span>), sharex=<span class="hljs-keyword">True</span>)

<span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> range(coeffs.shape[<span class="hljs-number">1</span>]):
    axes[<span class="hljs-number">0</span>].plot(np.log10(alphas), coeffs[:, n], color=<span class="hljs-string">&apos;k&apos;</span>, lw=<span class="hljs-number">0.5</span>)

axes[<span class="hljs-number">1</span>].semilogy(np.log10(alphas), sse_train, label=<span class="hljs-string">&quot;train&quot;</span>)
axes[<span class="hljs-number">1</span>].semilogy(np.log10(alphas), sse_test, label=<span class="hljs-string">&quot;test&quot;</span>)
axes[<span class="hljs-number">1</span>].legend(loc=<span class="hljs-number">0</span>)

axes[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">r&quot;${\log_{10}}\alpha$&quot;</span>, fontsize=<span class="hljs-number">18</span>)
axes[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">r&quot;coefficients&quot;</span>, fontsize=<span class="hljs-number">18</span>)
axes[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">r&quot;${\log_{10}}\alpha$&quot;</span>, fontsize=<span class="hljs-number">18</span>)
axes[<span class="hljs-number">1</span>].set_ylabel(<span class="hljs-string">r&quot;sse&quot;</span>, fontsize=<span class="hljs-number">18</span>)
fig.tight_layout()
</code></pre>
<p><img src="output_245_0.png" alt="png"></p>
<p>alpha &#x8D8A;&#x5927;, coeff &#x6700;&#x7EC8;&#x90FD;&#x4F1A;&#x53D8;&#x6210;0, &#x800C; train SSE &#x4F1A;&#x5148;&#x51CF;&#x5C0F;&#x518D;&#x589E;&#x52A0;, &#x800C; test &#x662F;&#x4E00;&#x76F4;&#x5728;&#x589E;&#x52A0;. </p>
<p>&#x5728;-1&#x9644;&#x8FD1;, train SSE &#x6700;&#x5C0F;, &#x800C; coeff &#x5927;&#x6982;&#x6709;8&#x4E2A;&#x4E0D;&#x662F;0.</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x4F7F;&#x7528;LassoCV: Lasso linear model with iterative fitting along a regularization path</span>
model = linear_model.LassoCV()
</code></pre>
<pre><code class="lang-python">model.fit(X_all, y_all)
</code></pre>
<pre><code>LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
    precompute=&apos;auto&apos;, random_state=None, selection=&apos;cyclic&apos;, tol=0.0001,
    verbose=False)
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x8BA1;&#x7B97;&#x51FA;&#x7684;&#x6700;&#x4F73; alphs</span>
model.alpha_
</code></pre>
<pre><code>0.06559238747534718
</code></pre><pre><code class="lang-python">resid_train = y_train - model.predict(X_train)
sse_train = sse(resid_train)
sse_train
</code></pre>
<pre><code>1.5450589323148352
</code></pre><pre><code class="lang-python">resid_test = y_test - model.predict(X_test)
sse_test = sse(resid_test)
sse_test
</code></pre>
<pre><code>1.5321417406216176
</code></pre><p>&#x53D1;&#x73B0; SSE &#x90FD;&#x5DF2;&#x7ECF;&#x6BD4;&#x8F83;&#x63A5;&#x8FD1;0&#x4E86;</p>
<pre><code class="lang-python">model.score(X_train, y_train), model.score(X_test, y_test)
<span class="hljs-comment"># score &#x90FD;&#x5F88;&#x9AD8;</span>
</code></pre>
<pre><code>(0.99999532217220677, 0.99999507886570982)
</code></pre><pre><code class="lang-python">fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
<span class="hljs-comment"># 9&#x4E2A; non-zero coeff</span>
</code></pre>
<p><img src="output_254_0.png" alt="png"></p>
<pre><code class="lang-python">

</code></pre>
<h2 id="&#x7EC3;&#x4E60;&#xFF1A;&#x5229;&#x7528;&#x672C;&#x7AE0;&#x5B66;&#x5230;&#x7684;&#x65B9;&#x6CD5;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x7279;&#x5F81;&#x5DE5;&#x7A0B;"><a name="&#x7EC3;&#x4E60;&#xFF1A;&#x5229;&#x7528;&#x672C;&#x7AE0;&#x5B66;&#x5230;&#x7684;&#x65B9;&#x6CD5;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x7279;&#x5F81;&#x5DE5;&#x7A0B;" class="plugin-anchor" href="#&#x7EC3;&#x4E60;&#xFF1A;&#x5229;&#x7528;&#x672C;&#x7AE0;&#x5B66;&#x5230;&#x7684;&#x65B9;&#x6CD5;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x7279;&#x5F81;&#x5DE5;&#x7A0B;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x7EC3;&#x4E60;&#xFF1A;&#x5229;&#x7528;&#x672C;&#x7AE0;&#x5B66;&#x5230;&#x7684;&#x65B9;&#x6CD5;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x7279;&#x5F81;&#x5DE5;&#x7A0B;</h2>
<p><br>
<br>
(#sections)</p>
<script type="text/javascript">var className='atoc';</script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html" class="navigation navigation-prev " aria-label="Previous page: 决策树和集成学习">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../w5-tuning-parameter/5w.html" class="navigation navigation-next " aria-label="Next page: 参数调优">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"特征工程","level":"1.6","depth":1,"next":{"title":"参数调优","level":"1.7","depth":1,"path":"w5-tuning-parameter/5w.md","ref":"w5-tuning-parameter/5w.md","articles":[]},"previous":{"title":"决策树和集成学习","level":"1.5","depth":1,"path":"w3-decision-tree-and-ensemble-learning/3w.md","ref":"w3-decision-tree-and-ensemble-learning/3w.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","anchors","github","splitter","sharing","atoc","comment"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/lyltj2010/DataMining"},"atoc":{"addClass":true,"className":"atoc"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":true,"instapaper":false,"twitter":true,"vk":false,"weibo":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"comment":{"highlightCommented":true}},"theme":"default","author":"wizardforcel","pdf":{"pageNumbers":true,"fontSize":16,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"数据挖掘开源书","language":"zh","links":{"sidebar":{"数据挖掘开源书":"https://www.gitbook.com/book/wizardforcel/data-mining-book"},"gitbook":true},"gitbook":"*","description":"数据挖掘开源书"},"file":{"path":"w4-feature-engineering/4w.md","mtime":"2016-12-24T21:25:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-04-29T11:04:29.751Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-atoc/atoc.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-comment/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

