
<!DOCTYPE HTML>
<html lang="zh" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>无监督学习 · 数据挖掘开源书</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        <meta name="author" content="wizardforcel">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-atoc/atoc.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-comment/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../w7-text-mining/7w.html" />
    
    
    <link rel="prev" href="../w5-tuning-parameter/5w.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="輸入並搜尋" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://www.gitbook.com/book/wizardforcel/data-mining-book" target="_blank" class="custom-link">数据挖掘开源书</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../w0-introduction/0w.html">
            
                <a href="../w0-introduction/0w.html">
            
                    
                    数据挖掘导论和信贷模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../w1-regression/1w.html">
            
                <a href="../w1-regression/1w.html">
            
                    
                    回归模型和房价预测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../w2-preceptron-and-logistic-regression/2w.html">
            
                <a href="../w2-preceptron-and-logistic-regression/2w.html">
            
                    
                    感知机和逻辑回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                    
                    决策树和集成学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../w4-feature-engineering/4w.html">
            
                <a href="../w4-feature-engineering/4w.html">
            
                    
                    特征工程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../w5-tuning-parameter/5w.html">
            
                <a href="../w5-tuning-parameter/5w.html">
            
                    
                    参数调优
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.8" data-path="6w.html">
            
                <a href="6w.html">
            
                    
                    无监督学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../w7-text-mining/7w.html">
            
                <a href="../w7-text-mining/7w.html">
            
                    
                    文本挖掘
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../w8-neural-network/8w.html">
            
                <a href="../w8-neural-network/8w.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../w9-deep-learning/9w.html">
            
                <a href="../w9-deep-learning/9w.html">
            
                    
                    深度学习
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本書使用 GitBook 釋出
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >无监督学习</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="&#x7B2C;6&#x7AE0;&#xFF1A;&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;"><a name="&#x7B2C;6&#x7AE0;&#xFF1A;&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;" class="plugin-anchor" href="#&#x7B2C;6&#x7AE0;&#xFF1A;&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x7B2C;6&#x7AE0;&#xFF1A;&#x975E;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;</h1>
<h2 id="sections"><a name="sections" class="plugin-anchor" href="#sections"><i class="fa fa-link" aria-hidden="true"></i></a>Sections</h2>
<ul>
<li><a href="#some-notable-clustering-routines">Some notable clustering routines</a></li>
<li><a href="#grouping-objects-by-similarity-using-k-means">Grouping objects by similarity using k-means</a><ul>
<li><a href="#k-means-in-sklearn">k-means in Sklearn</a></li>
<li><a href="#k-means">k-means++</a></li>
<li><a href="#implementing-k-means-in-python">Implementing k-means in Python</a></li>
<li><a href="#using-the-elbow-method-to-find-the-optimal-number-of-clusters">Using the elbow method to find the optimal number of clusters</a></li>
<li><a href="#quantifying-the-quality-of-clustering-via-silhouette-plots">Quantifying the quality of clustering via silhouette plots</a></li>
</ul>
</li>
<li><a href="#organizing-clusters-as-a-hierarchical-tree">Organizing clusters as a hierarchical tree</a><ul>
<li><a href="#performing-hierarchical-clustering-on-a-distance-matrix">Performing hierarchical clustering on a distance matrix</a></li>
<li><a href="#attaching-dendrograms-to-a-heat-map">Attaching dendrograms to a heat map</a></li>
<li><a href="#applying-agglomerative-clustering-via-scikit-learn">Applying agglomerative clustering via scikit-learn</a></li>
<li><a href="#applying-agglomerative-clustering-with-iris-dataset">Applying agglomerative clustering with iris dataset</a></li>
</ul>
</li>
<li><a href="#locating-regions-of-high-density-via-dbscan">Locating regions of high density via DBSCAN</a></li>
<li><a href="#learning-from-labeled-and-unlabeled-data-with-label-propagation">Learning from labeled and unlabeled data with label propagation</a></li>
</ul>
<p>Clustering is the task of gathering samples into groups of similar
samples according to some predefined similarity or distance (dissimilarity)
measure, such as the Euclidean distance.</p>
<p>Here are some common applications of clustering algorithms:</p>
<ul>
<li>Compression for data reduction</li>
<li>Summarizing data as a reprocessing step for recommender systems</li>
<li>Similarly:<ul>
<li>grouping related web news (e.g. Google News) and web search results</li>
<li>grouping related stock quotes for investment portfolio management</li>
<li>building customer profiles for market analysis</li>
</ul>
</li>
<li>Building a code book of prototype samples for unsupervised feature extraction</li>
</ul>
<p><br>
<br></p>
<h1 id="some-notable-clustering-routines"><a name="some-notable-clustering-routines" class="plugin-anchor" href="#some-notable-clustering-routines"><i class="fa fa-link" aria-hidden="true"></i></a>Some notable clustering routines</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>The following are two well-known clustering algorithms. </p>
<ul>
<li><code>sklearn.cluster.KMeans</code>: <br>
  The simplest, yet effective clustering algorithm. Needs to be provided with the
  number of clusters in advance, and assumes that the data is normalized as input
  (but use a PCA model as preprocessor).</li>
<li><code>sklearn.cluster.MeanShift</code>: <br>
  Can find better looking clusters than KMeans but is not scalable to high number of samples.</li>
<li><code>sklearn.cluster.DBSCAN</code>: <br>
  Can detect irregularly shaped clusters based on density, i.e. sparse regions in
  the input space are likely to become inter-cluster boundaries. Can also detect
  outliers (samples that are not part of a cluster).</li>
<li><code>sklearn.cluster.AffinityPropagation</code>: <br>
  Clustering algorithm based on message passing between data points.</li>
<li><code>sklearn.cluster.SpectralClustering</code>: <br>
  KMeans applied to a projection of the normalized graph Laplacian: finds
  normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph.</li>
<li><code>sklearn.cluster.Ward</code>: <br>
  Ward implements hierarchical clustering based on the Ward algorithm,
  a variance-minimizing approach. At each step, it minimizes the sum of
  squared differences within all clusters (inertia criterion).</li>
</ul>
<p>Of these, Ward, SpectralClustering, DBSCAN and Affinity propagation can also work with precomputed similarity matrices.</p>
<p><img src="../figures/cluster_comparison.png" alt="cluster comparison"></p>
<p><br>
<br></p>
<h1 id="grouping-objects-by-similarity-using-k-means"><a name="grouping-objects-by-similarity-using-k-means" class="plugin-anchor" href="#grouping-objects-by-similarity-using-k-means"><i class="fa fa-link" aria-hidden="true"></i></a>Grouping objects by similarity using k-means</h1>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># make dataset</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_blobs
X, y = make_blobs(n_samples=<span class="hljs-number">150</span>, 
                  n_features=<span class="hljs-number">2</span>, 
                  centers=<span class="hljs-number">3</span>, <span class="hljs-comment"># &#x4ECE;&#x4E09;&#x7C7B;&#x4E2D;&#x62BD;&#x53D6;</span>
                  cluster_std=<span class="hljs-number">0.5</span>, 
                  shuffle=<span class="hljs-keyword">True</span>, 
                  random_state=<span class="hljs-number">0</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x753B;&#x4E2A;&#x56FE;&#x770B;&#x770B;</span>
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline
plt.scatter(X[:,<span class="hljs-number">0</span>], X[:,<span class="hljs-number">1</span>],  c=<span class="hljs-string">&apos;white&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, s=<span class="hljs-number">50</span>)
plt.grid()
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/spheres.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_12_0.png" alt="png"></p>
<h2 id="k-means-&#x7B97;&#x6CD5;"><a name="k-means-&#x7B97;&#x6CD5;" class="plugin-anchor" href="#k-means-&#x7B97;&#x6CD5;"><i class="fa fa-link" aria-hidden="true"></i></a>k-means &#x7B97;&#x6CD5;</h2>
<ol>
<li>Randomly pick <code>k</code> centroids from the sample points as initial cluster centers.</li>
<li>Assign each sample to the nearest centroid <script type="math/tex; ">\mu^{(j)}, j∈\{1,...,k\}</script>.</li>
<li>Move the centroids to the center of the samples that were assigned to it.</li>
<li>Repeat the steps 2 and 3 until the cluster assignment do not change or a user-defined tolerance or a maximum number of iterations is reached.</li>
</ol>
<p><a href="http://www.naftaliharris.com/blog/visualizing-k-means-clustering/" target="_blank">Visualizing K-Means Clustering</a></p>
<p>&#x5982;&#x4F55;&#x6765;&#x6D4B;&#x91CF;&#x4E24;&#x4E2A;&#x7269;&#x4F53;&#x4E4B;&#x95F4;&#x7684;&#x76F8;&#x4F3C;&#x5EA6;, similarity<br>&#x6216;&#x8005;&#x5982;&#x4F55;&#x8868;&#x793A;&#x4E24;&#x4E2A;&#x7269;&#x4F53;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;, distance</p>
<p>&#x6700;&#x5E38;&#x89C1;&#x7684;&#x4E00;&#x79CD;&#x8DDD;&#x79BB;&#x5EA6;&#x91CF;&#x662F; squared Euclidean distance: 
<script type="math/tex; "> d(x,y)^2 = \sum_{j=1}^{m} (x_j -y_j)^2 = \lVert x - y \rVert_2^2 </script></p>
<p>k-means &#x53EF;&#x8F6C;&#x5316;&#x4E3A;&#x6700;&#x4F18;&#x5316;&#x7684;&#x95EE;&#x9898;, &#x6700;&#x5C0F;&#x5316; within-cluster sum of squared errors (SSE), &#x53C8;&#x79F0;&#x4E3A; cluster inertia
<script type="math/tex; "> SSE = \sum_{i=1}^n\sum_{j=1}^k w^{(i,j)}\lVert x^{(i)}-\mu^{(j)}\rVert_2^2 </script>
&#x5176;&#x4E2D; <script type="math/tex; ">\mu^{(j)}</script> &#x662F;&#x7B2C; j &#x4E2A;&#x805A;&#x7C7B;&#x7684;&#x4E2D;&#x5FC3;&#xFF0C;&#x5982;&#x679C;&#x6837;&#x672C; i &#x5728;&#x805A;&#x7C7B; j &#x4E2D;&#xFF0C;<script type="math/tex; ">w^{(i,j)}=1</script>&#xFF0C;&#x5426;&#x5219; <script type="math/tex; ">w^{(i,j)}=0</script></p>
<p><br></p>
<h2 id="k-means-in-sklearn"><a name="k-means-in-sklearn" class="plugin-anchor" href="#k-means-in-sklearn"><i class="fa fa-link" aria-hidden="true"></i></a>k-means in Sklearn</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x4F7F;&#x7528; sklearn &#x4E2D; KMeans &#x6765;&#x805A;&#x7C7B;</span>
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans
km = KMeans(n_clusters=<span class="hljs-number">3</span>,  <span class="hljs-comment"># k &#x662F;&#x9700;&#x8981;&#x81EA;&#x5DF1;&#x8BBE;&#x5B9A;&#x7684;, &#x9519;&#x8BEF;&#x7684; k &#x53EF;&#x80FD;&#x7ED3;&#x679C;&#x5C31;&#x4E0D;&#x5BF9;</span>
            init=<span class="hljs-string">&apos;random&apos;</span>,
            n_init=<span class="hljs-number">10</span>,  <span class="hljs-comment"># &#x91CD;&#x590D;&#x8FD0;&#x884C;&#x7B97;&#x6CD5; 10 &#x6B21;&#xFF0C;&#x9009;&#x5176;&#x4E2D;&#x6700;&#x597D;&#x7684;&#x805A;&#x7C7B;&#x6A21;&#x578B;&#xFF0C;&#x907F;&#x514D;&#x4E0D;&#x597D;&#x7684;&#x521D;&#x59CB;&#x5316;&#x503C;&#x5E26;&#x6765;&#x7684;&#x5F71;&#x54CD;</span>
            max_iter=<span class="hljs-number">300</span>,
            tol=<span class="hljs-number">1e-04</span>,
            random_state=<span class="hljs-number">0</span>)

y_km = km.fit_predict(X)
</code></pre>
<pre><code class="lang-python">plt.scatter(X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;lightgreen&apos;</span>, 
            marker=<span class="hljs-string">&apos;s&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 1&apos;</span>)
plt.scatter(X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;orange&apos;</span>, 
            marker=<span class="hljs-string">&apos;o&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 2&apos;</span>)
plt.scatter(X[y_km==<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">2</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;lightblue&apos;</span>, 
            marker=<span class="hljs-string">&apos;v&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 3&apos;</span>)
plt.scatter(km.cluster_centers_[:,<span class="hljs-number">0</span>], 
            km.cluster_centers_[:,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">250</span>, 
            marker=<span class="hljs-string">&apos;*&apos;</span>, 
            c=<span class="hljs-string">&apos;red&apos;</span>, 
            label=<span class="hljs-string">&apos;centroids&apos;</span>)
plt.legend()
plt.grid()
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/centroids.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_20_0.png" alt="png"></p>
<p><br></p>
<h2 id="k-means"><a name="k-means" class="plugin-anchor" href="#k-means"><i class="fa fa-link" aria-hidden="true"></i></a>k-means++</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>k-means++ &#x7B97;&#x6CD5;&#x901A;&#x8FC7;&#x6539;&#x5584; centroids &#x521D;&#x59CB;&#x503C;&#x7684;&#x8BBE;&#x7F6E;&#xFF0C;&#x6765;&#x4F18;&#x5316; k-means</p>
<ol>
<li>Initialize an empty set <script type="math/tex; ">M</script> to store the k centroids being selected</li>
<li>Randomly choose the first centroid <script type="math/tex; ">\mu^{(j)}</script> from the input samples and assign it to <script type="math/tex; ">M</script></li>
<li>For each sample <script type="math/tex; ">x^{(i)}</script> that is not in M, find the minimum squared distance <script type="math/tex; ">d(x^{(i)}, M)^2</script> to any of the centroids in M</li>
<li>To randomly select the next centroid <script type="math/tex; ">\mu^{(p)}</script>, use a weighted probability distribution equal to <script type="math/tex; ">\frac{d(\mu^{(p)}, M)^2}{\sum_i d(x^{(i)}, M)^2}</script></li>
<li>Repeat steps 2 and 3 until <script type="math/tex; ">k</script> centroids are chosen</li>
<li>Proceed with the classic k-means algorithm</li>
</ol>
<p>sklearn &#x91CC;&#x4F7F;&#x7528; k-means++ &#x7B97;&#x6CD5;&#x53EA;&#x9700;&#x5728; KMeans() &#x91CC;&#x8BBE;&#x7F6E; init=&quot;k-means++&quot;</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5BF9;&#x6BD4; k-means &#x548C; k-means++</span>
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> shuffle
<span class="hljs-keyword">from</span> sklearn.utils <span class="hljs-keyword">import</span> check_random_state
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans

random_state = np.random.RandomState(<span class="hljs-number">0</span>)

<span class="hljs-comment"># Number of run (with randomly generated dataset) for each strategy so as</span>
<span class="hljs-comment"># to be able to compute an estimate of the standard deviation</span>
n_runs = <span class="hljs-number">5</span>

<span class="hljs-comment"># k-means models can do several random inits so as to be able to trade</span>
<span class="hljs-comment"># CPU time for convergence robustness</span>
n_init_range = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>])

<span class="hljs-comment"># Datasets generation parameters</span>
n_samples_per_center = <span class="hljs-number">100</span>
grid_size = <span class="hljs-number">3</span>
scale = <span class="hljs-number">0.1</span>
n_clusters = grid_size ** <span class="hljs-number">2</span>


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">make_data</span><span class="hljs-params">(random_state, n_samples_per_center, grid_size, scale)</span>:</span>
    random_state = check_random_state(random_state)
    centers = np.array([[i, j]
                        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(grid_size)
                        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(grid_size)])
    n_clusters_true, n_features = centers.shape

    noise = random_state.normal(
        scale=scale, size=(n_samples_per_center, centers.shape[<span class="hljs-number">1</span>]))

    X = np.concatenate([c + noise <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> centers])
    y = np.concatenate([[i] * n_samples_per_center
                        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n_clusters_true)])
    <span class="hljs-keyword">return</span> shuffle(X, y, random_state=random_state)


fig = plt.figure()
plots = []
legends = []

cases = [
    (KMeans, <span class="hljs-string">&apos;k-means++&apos;</span>),
    (KMeans, <span class="hljs-string">&apos;random&apos;</span>)
]

<span class="hljs-keyword">for</span> factory, init <span class="hljs-keyword">in</span> cases:
    print(<span class="hljs-string">&quot;Evaluation of %s with %s init&quot;</span> % (factory.__name__, init))
    inertia = np.empty((len(n_init_range), n_runs))

    <span class="hljs-keyword">for</span> run_id <span class="hljs-keyword">in</span> range(n_runs):
        X_data, y = make_data(run_id, n_samples_per_center, grid_size, scale)
        <span class="hljs-keyword">for</span> i, n_init <span class="hljs-keyword">in</span> enumerate(n_init_range):
            kmean = factory(n_clusters=n_clusters, init=init, random_state=run_id,
                         n_init=n_init).fit(X_data)
            inertia[i, run_id] = kmean.inertia_
    p = plt.errorbar(n_init_range, inertia.mean(axis=<span class="hljs-number">1</span>), inertia.std(axis=<span class="hljs-number">1</span>))
    plots.append(p[<span class="hljs-number">0</span>])
    legends.append(<span class="hljs-string">&quot;%s with %s init&quot;</span> % (factory.__name__, init))

plt.xlabel(<span class="hljs-string">&apos;n_init&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;inertia&apos;</span>)
plt.legend(plots, legends)
plt.title(<span class="hljs-string">&quot;Mean inertia for various k-means init across %d runs&quot;</span> % n_runs);
</code></pre>
<pre><code>Evaluation of KMeans with k-means++ init
Evaluation of KMeans with random init
</code></pre><p><img src="output_26_1.png" alt="png"></p>
<p><br></p>
<h2 id="implementing-k-means-in-python"><a name="implementing-k-means-in-python" class="plugin-anchor" href="#implementing-k-means-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implementing k-means in Python</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> pairwise_distances


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_initial_centroids</span><span class="hljs-params">(data, k, seed=None)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;Randomly choose k data points as initial centroids&apos;&apos;&apos;</span>
    <span class="hljs-keyword">if</span> seed <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>: <span class="hljs-comment"># useful for obtaining consistent results</span>
        np.random.seed(seed)
    n = data.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># number of data points</span>

    <span class="hljs-comment"># Pick K indices from range [0, N).</span>
    rand_indices = np.random.randint(<span class="hljs-number">0</span>, n, k)

    <span class="hljs-comment"># Keep centroids as dense format, as many entries will be nonzero due to averaging.</span>
    <span class="hljs-comment"># As long as at least one document in a cluster contains a word,</span>
    <span class="hljs-comment"># it will carry a nonzero weight in the TF-IDF vector of the centroid.</span>
    centroids = data[rand_indices,:]

    <span class="hljs-keyword">return</span> centroids


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">smart_initialize</span><span class="hljs-params">(data, k, seed=None)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;Use k-means++ to initialize a good set of centroids&apos;&apos;&apos;</span>
    <span class="hljs-keyword">if</span> seed <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>: <span class="hljs-comment"># useful for obtaining consistent results</span>
        np.random.seed(seed)
    centroids = np.zeros((k, data.shape[<span class="hljs-number">1</span>]))

    <span class="hljs-comment"># Randomly choose the first centroid.</span>
    <span class="hljs-comment"># Since we have no prior knowledge, choose uniformly at random</span>
    idx = np.random.randint(data.shape[<span class="hljs-number">0</span>])
    centroids[<span class="hljs-number">0</span>] = data[idx,:]
    <span class="hljs-comment"># Compute distances from the first centroid chosen to all the other data points</span>
    distances = pairwise_distances(data, centroids[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>], metric=<span class="hljs-string">&apos;euclidean&apos;</span>).flatten()

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">1</span>, k):
        <span class="hljs-comment"># Choose the next centroid randomly, so that the probability for each data point to be chosen</span>
        <span class="hljs-comment"># is directly proportional to its squared distance from the nearest centroid.</span>
        <span class="hljs-comment"># Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.</span>
        idx = np.random.choice(data.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, p=distances/sum(distances))
        centroids[i] = data[idx,:]
        <span class="hljs-comment"># Now compute distances from the centroids to all data points</span>
        distances = np.min(pairwise_distances(data, centroids[<span class="hljs-number">0</span>:i+<span class="hljs-number">1</span>], metric=<span class="hljs-string">&apos;euclidean&apos;</span>),axis=<span class="hljs-number">1</span>)

    <span class="hljs-keyword">return</span> centroids


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">assign_clusters</span><span class="hljs-params">(data, centroids)</span>:</span>
    <span class="hljs-comment"># Compute distances between each data point and the set of centroids:</span>
    distances_from_centroids = pairwise_distances(data, centroids)

    <span class="hljs-comment"># Compute cluster assignments for each data point:</span>
    cluster_assignment = np.argmin(distances_from_centroids, axis=<span class="hljs-number">1</span>)

    <span class="hljs-keyword">return</span> cluster_assignment


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">revise_centroids</span><span class="hljs-params">(data, k, cluster_assignment)</span>:</span>
    new_centroids = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> xrange(k):
        <span class="hljs-comment"># Select all data points that belong to cluster i. Fill in the blank (RHS only)</span>
        member_data_points = data[cluster_assignment == i]
        <span class="hljs-comment"># Compute the mean of the data points. Fill in the blank (RHS only)</span>
        centroid = member_data_points.mean(axis=<span class="hljs-number">0</span>)
        new_centroids.append(centroid)
    new_centroids = np.array(new_centroids)
    <span class="hljs-keyword">return</span> new_centroids


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kmeans</span><span class="hljs-params">(data, k, init=<span class="hljs-string">&apos;kmeans++&apos;</span>, maxiter=<span class="hljs-number">100</span>, seed=None)</span>:</span>
    <span class="hljs-comment"># Initialize centroids</span>
    <span class="hljs-keyword">if</span> init == <span class="hljs-string">&apos;kmeans++&apos;</span>:
        centroids = smart_initialize(data, k, seed)
    <span class="hljs-keyword">else</span>:
        centroids = get_initial_centroids(data, k, seed)
    prev_cluster_assignment = <span class="hljs-keyword">None</span>

    <span class="hljs-keyword">for</span> itr <span class="hljs-keyword">in</span> xrange(maxiter):                
        <span class="hljs-comment"># 1. Make cluster assignments using nearest centroids</span>
        cluster_assignment = assign_clusters(data, centroids)

        <span class="hljs-comment"># 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.</span>
        centroids = revise_centroids(data, k, cluster_assignment)

        <span class="hljs-comment"># Check for convergence: if none of the assignments changed, stop</span>
        <span class="hljs-keyword">if</span> prev_cluster_assignment <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">and</span> \
          (prev_cluster_assignment==cluster_assignment).all():
            <span class="hljs-keyword">break</span>

        prev_cluster_assignment = cluster_assignment[:]

    <span class="hljs-keyword">return</span> centroids, cluster_assignment
</code></pre>
<pre><code class="lang-python">centers, y_km = kmeans(X, <span class="hljs-number">3</span>, seed=<span class="hljs-number">0</span>)
</code></pre>
<pre><code class="lang-python">plt.scatter(X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;lightgreen&apos;</span>, 
            marker=<span class="hljs-string">&apos;s&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 1&apos;</span>)
plt.scatter(X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;orange&apos;</span>, 
            marker=<span class="hljs-string">&apos;o&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 2&apos;</span>)
plt.scatter(X[y_km==<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">2</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;lightblue&apos;</span>, 
            marker=<span class="hljs-string">&apos;v&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 3&apos;</span>)
plt.scatter(centers[:,<span class="hljs-number">0</span>], 
            centers[:,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">250</span>, 
            marker=<span class="hljs-string">&apos;*&apos;</span>, 
            c=<span class="hljs-string">&apos;red&apos;</span>, 
            label=<span class="hljs-string">&apos;centroids&apos;</span>)
plt.legend()
plt.grid()
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/centroids.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_32_0.png" alt="png"></p>
<p><br></p>
<h3 id="using-the-elbow-method-to-find-the-optimal-number-of-clusters"><a name="using-the-elbow-method-to-find-the-optimal-number-of-clusters" class="plugin-anchor" href="#using-the-elbow-method-to-find-the-optimal-number-of-clusters"><i class="fa fa-link" aria-hidden="true"></i></a>Using the elbow method to find the optimal number of clusters</h3>
<p>&#x901A;&#x5E38;&#x6211;&#x4EEC;&#x5E76;&#x4E0D;&#x77E5;&#x9053;&#x6570;&#x636E;&#x80FD;&#x5206;&#x6210;&#x51E0;&#x4E2A;&#x805A;&#x7C7B;&#xFF0C;&#x6240;&#x4EE5;&#x80FD;&#x6709;&#x529E;&#x6CD5;&#x9009;&#x62E9;&#x5408;&#x9002; k &#x503C;&#x975E;&#x5E38;&#x91CD;&#x8981;</p>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x5224;&#x65AD;&#x805A;&#x7C7B;&#x6548;&#x679C;&#x53EF;&#x7528; within-cluster SSE (Distortion)&#xFF0C;&#x8FD9;&#x4E2A;&#x53EF;&#x7531; KMeans() &#x4E2D;&#x7684; inertia_ &#x5C5E;&#x6027;&#x83B7;&#x5F97;</p>
<pre><code class="lang-python">print(<span class="hljs-string">&apos;Distortion: %.2f&apos;</span> % km.inertia_)
</code></pre>
<pre><code>Distortion: 72.48
</code></pre><p>The Elbow method is a &quot;rule-of-thumb&quot; approach to finding the optimal number of clusters.</p>
<pre><code class="lang-python"><span class="hljs-comment"># elbow method &#x65B9;&#x6CD5;&#x5C31;&#x662F;&#x9700;&#x8981;&#x627E;&#x51FA;&#x5F53; distortion &#x53D8;&#x5316;&#x975E;&#x5E38;&#x5FEB;&#x65F6;&#x7684; k &#x503C;&#xFF0C;&#x4E5F;&#x5373;&#x627E;&#x62D0;&#x70B9;</span>
distortions = []
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>):
    km = KMeans(n_clusters=i, 
                init=<span class="hljs-string">&apos;k-means++&apos;</span>, 
                n_init=<span class="hljs-number">10</span>, 
                max_iter=<span class="hljs-number">300</span>, 
                random_state=<span class="hljs-number">0</span>)
    km.fit(X)
    distortions.append(km.inertia_)
plt.plot(range(<span class="hljs-number">1</span>,<span class="hljs-number">11</span>), distortions , marker=<span class="hljs-string">&apos;o&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Number of clusters&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Distortion&apos;</span>)
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/elbow.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_39_0.png" alt="png"></p>
<p>&#x4ECE;&#x56FE;&#x4E2D;&#x5C31;&#x53EF;&#x4EE5;&#x770B;&#x51FA;, 3 &#x662F;&#x62D0;&#x70B9;, &#x6240;&#x4EE5; k=3 &#x662F;&#x6700;&#x597D;&#x7684;&#x9009;&#x62E9;</p>
<p><br></p>
<h2 id="quantifying-the-quality-of-clustering-via-silhouette-plots"><a name="quantifying-the-quality-of-clustering-via-silhouette-plots" class="plugin-anchor" href="#quantifying-the-quality-of-clustering-via-silhouette-plots"><i class="fa fa-link" aria-hidden="true"></i></a>Quantifying the quality of clustering via silhouette plots</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x53E6;&#x4E00;&#x79CD;&#x8BC4;&#x4EF7;&#x805A;&#x7C7B;&#x6548;&#x679C;&#x7684;&#x65B9;&#x6CD5;&#x662F; silhouette analysis, &#x8861;&#x91CF;&#x7684;&#x662F;&#x4E00;&#x4E2A;&#x7C7B;&#x522B;&#x4E2D;&#x7684;&#x6837;&#x672C;&#x662F;&#x5426;&#x8DB3;&#x591F;&#x7D27;&#x51D1;&#x7EC4;&#x5408;</p>
<p>&#x8BA1;&#x7B97; <script type="math/tex; ">x^{(i)}</script> &#x7684; silhouette coefficient &#x7684;&#x6B65;&#x9AA4;&#x4E3A;:</p>
<ol>
<li>Calculate the cluster cohesion <script type="math/tex; ">a^{(i)}</script> as the average distance between a sample <script type="math/tex; ">x^{(i)}</script> and all other points in the same cluster.</li>
<li>Calculate the cluster separation <script type="math/tex; ">b^{(i)}</script> from the next closest cluster as the average distance between the sample <script type="math/tex; ">x^{(i)}</script> and all samples in the nearest cluster.</li>
<li>Calculate the silhouette <script type="math/tex; ">s^{(i)}</script> as the difference between cluster cohesion and separation divided by the greater of the two, as shown here:
<script type="math/tex; "> s^{(i)} = \frac{b^{(i)} - a^{(i)}}{max\big\{b^{(i)}, a^{(i)}\big\}} </script></li>
</ol>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> cm
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> silhouette_samples  <span class="hljs-comment"># silhouette coefficient</span>

km = KMeans(n_clusters=<span class="hljs-number">3</span>, 
            init=<span class="hljs-string">&apos;k-means++&apos;</span>, 
            n_init=<span class="hljs-number">10</span>, 
            max_iter=<span class="hljs-number">300</span>,
            tol=<span class="hljs-number">1e-04</span>,
            random_state=<span class="hljs-number">0</span>)
y_km = km.fit_predict(X)

cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[<span class="hljs-number">0</span>]
silhouette_vals = silhouette_samples(X, y_km, metric=<span class="hljs-string">&apos;euclidean&apos;</span>)
y_ax_lower, y_ax_upper = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
yticks = []
<span class="hljs-keyword">for</span> i, c <span class="hljs-keyword">in</span> enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(i / float(n_clusters))
    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=<span class="hljs-number">1.0</span>, 
            edgecolor=<span class="hljs-string">&apos;none&apos;</span>, color=color)

    yticks.append((y_ax_lower + y_ax_upper) / <span class="hljs-number">2</span>)
    y_ax_lower += len(c_silhouette_vals)

silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg, color=<span class="hljs-string">&quot;red&quot;</span>, linestyle=<span class="hljs-string">&quot;--&quot;</span>) 

plt.yticks(yticks, cluster_labels + <span class="hljs-number">1</span>)
plt.ylabel(<span class="hljs-string">&apos;Cluster&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Silhouette coefficient&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/silhouette.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_45_0.png" alt="png"></p>
<p>&#x7EA2;&#x7EBF;&#x8868;&#x793A;&#x6240;&#x6709;&#x6570;&#x636E; silhouette coef &#x7684;&#x5E73;&#x5747;&#x503C;&#xFF0C;&#x5B83;&#x53EF;&#x4F5C;&#x4E3A;&#x805A;&#x7C7B;&#x6A21;&#x578B;&#x7684;&#x4E00;&#x4E2A;&#x5EA6;&#x91CF;&#x6307;&#x6807;</p>
<p>Comparison to &quot;bad&quot; clustering:</p>
<pre><code class="lang-python">km = KMeans(n_clusters=<span class="hljs-number">2</span>, <span class="hljs-comment"># &#x8BBE;&#x5B9A;&#x4E3A;2</span>
            init=<span class="hljs-string">&apos;k-means++&apos;</span>, 
            n_init=<span class="hljs-number">10</span>, 
            max_iter=<span class="hljs-number">300</span>,
            tol=<span class="hljs-number">1e-04</span>,
            random_state=<span class="hljs-number">0</span>)
y_km = km.fit_predict(X)

plt.scatter(X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;lightgreen&apos;</span>, 
            marker=<span class="hljs-string">&apos;s&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 1&apos;</span>)
plt.scatter(X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], 
            X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], 
            s=<span class="hljs-number">50</span>, 
            c=<span class="hljs-string">&apos;orange&apos;</span>, 
            marker=<span class="hljs-string">&apos;o&apos;</span>, 
            label=<span class="hljs-string">&apos;cluster 2&apos;</span>)

plt.scatter(km.cluster_centers_[:,<span class="hljs-number">0</span>], km.cluster_centers_[:,<span class="hljs-number">1</span>], s=<span class="hljs-number">250</span>, marker=<span class="hljs-string">&apos;*&apos;</span>, c=<span class="hljs-string">&apos;red&apos;</span>, label=<span class="hljs-string">&apos;centroids&apos;</span>)
plt.legend()
plt.grid()
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/centroids_bad.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_48_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># evaluate the results</span>
cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[<span class="hljs-number">0</span>]
silhouette_vals = silhouette_samples(X, y_km, metric=<span class="hljs-string">&apos;euclidean&apos;</span>)
y_ax_lower, y_ax_upper = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>
yticks = []
<span class="hljs-keyword">for</span> i, c <span class="hljs-keyword">in</span> enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(i / float(n_clusters))
    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=<span class="hljs-number">1.0</span>, 
            edgecolor=<span class="hljs-string">&apos;none&apos;</span>, color=color)

    yticks.append((y_ax_lower + y_ax_upper) / <span class="hljs-number">2</span>)
    y_ax_lower += len(c_silhouette_vals)

silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg, color=<span class="hljs-string">&quot;red&quot;</span>, linestyle=<span class="hljs-string">&quot;--&quot;</span>) 

plt.yticks(yticks, cluster_labels + <span class="hljs-number">1</span>)
plt.ylabel(<span class="hljs-string">&apos;Cluster&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Silhouette coefficient&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/silhouette_bad.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_49_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="organizing-clusters-as-a-hierarchical-tree"><a name="organizing-clusters-as-a-hierarchical-tree" class="plugin-anchor" href="#organizing-clusters-as-a-hierarchical-tree"><i class="fa fa-link" aria-hidden="true"></i></a>Organizing clusters as a hierarchical tree</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>One nice feature of hierachical clustering is that we can visualize the results as a dendrogram, a hierachical tree. Using the visualization, we can then decide how &quot;deep&quot; we want to cluster the dataset by setting a &quot;depth&quot; threshold. Or in other words, we don&apos;t need to make a decision about the number of clusters upfront.</p>
<p><strong>Agglomerative and divisive hierarchical clustering</strong></p>
<p>Furthermore, we can distinguish between 2 main approaches to hierarchical clustering: Divisive clustering and agglomerative clustering. In agglomerative clustering, we start with a single sample from our dataset and iteratively merge it with other samples to form clusters -- we can see it as a bottom-up approach for building the clustering dendrogram.<br>In divisive clustering, however, we start with the whole dataset as one cluster, and we iteratively split it into smaller subclusters -- a top-down approach.  </p>
<p>In this notebook, we will use <strong>agglomerative</strong> clustering.</p>
<p><strong>Single and complete linkage</strong></p>
<p>Now, the next question is how we measure the similarity between samples. One approach is the familiar Euclidean distance metric that we already used via the K-Means algorithm. as a refresher, the distance between 2 m-dimensional vectors <script type="math/tex; ">\mathbf{p}</script> and <script type="math/tex; ">\mathbf{q}</script> can be computed as:</p>
<p>\begin{align} \mathrm{d}(\mathbf{q},\mathbf{p}) &amp; = \sqrt{(q<em>1-p_1)^2 + (q_2-p_2)^2 + \cdots + (q_m-p_m)^2} \[8pt]
&amp; = \sqrt{\sum</em>{j=1}^m (q_j-p_j)^2}.\end{align}    </p>
<p>However, that&apos;s the distance between 2 samples. Now, how do we compute the similarity between subclusters of samples in order to decide which clusters to merge when constructing the dendrogram? I.e., our goal is to iteratively merge the most similar pairs of clusters until only one big cluster remains. There are many different approaches to this, for example single and complete linkage. </p>
<p>In single linkage, we take the pair of the most similar samples (based on the Euclidean distance, for example) in each cluster, and merge the two clusters which have the most similar 2 members into one new, bigger cluster.</p>
<p>In complete linkage, we compare the pairs of the two most dissimilar members of each cluster with each other, and we merge the 2 clusters where the distance between its 2 most dissimilar members is smallest.</p>
<p><img src="../figures/clustering-linkage.png" alt=""></p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x751F;&#x6210;&#x6570;&#x636E;</span>
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

np.random.seed(<span class="hljs-number">123</span>)

variables = [<span class="hljs-string">&apos;X&apos;</span>, <span class="hljs-string">&apos;Y&apos;</span>, <span class="hljs-string">&apos;Z&apos;</span>]
labels = [<span class="hljs-string">&apos;ID_0&apos;</span>,<span class="hljs-string">&apos;ID_1&apos;</span>,<span class="hljs-string">&apos;ID_2&apos;</span>,<span class="hljs-string">&apos;ID_3&apos;</span>,<span class="hljs-string">&apos;ID_4&apos;</span>]

X = np.random.random_sample([<span class="hljs-number">5</span>,<span class="hljs-number">3</span>])*<span class="hljs-number">10</span>
df = pd.DataFrame(X, columns=variables, index=labels)
df
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X</th>
      <th>Y</th>
      <th>Z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ID_0</th>
      <td>6.964692</td>
      <td>2.861393</td>
      <td>2.268515</td>
    </tr>
    <tr>
      <th>ID_1</th>
      <td>5.513148</td>
      <td>7.194690</td>
      <td>4.231065</td>
    </tr>
    <tr>
      <th>ID_2</th>
      <td>9.807642</td>
      <td>6.848297</td>
      <td>4.809319</td>
    </tr>
    <tr>
      <th>ID_3</th>
      <td>3.921175</td>
      <td>3.431780</td>
      <td>7.290497</td>
    </tr>
    <tr>
      <th>ID_4</th>
      <td>4.385722</td>
      <td>0.596779</td>
      <td>3.980443</td>
    </tr>
  </tbody>
</table>
</div>



<p><br></p>
<h2 id="performing-hierarchical-clustering-on-a-distance-matrix"><a name="performing-hierarchical-clustering-on-a-distance-matrix" class="plugin-anchor" href="#performing-hierarchical-clustering-on-a-distance-matrix"><i class="fa fa-link" aria-hidden="true"></i></a>Performing hierarchical clustering on a distance matrix</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># calculate the distance matrix as input for the hierarchical clustering algorithm</span>
<span class="hljs-keyword">from</span> scipy.spatial.distance <span class="hljs-keyword">import</span> pdist,squareform

row_dist = pd.DataFrame(squareform(pdist(df, metric=<span class="hljs-string">&apos;euclidean&apos;</span>)), columns=labels, index=labels)
row_dist
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ID_0</th>
      <th>ID_1</th>
      <th>ID_2</th>
      <th>ID_3</th>
      <th>ID_4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>ID_0</th>
      <td>0.000000</td>
      <td>4.973534</td>
      <td>5.516653</td>
      <td>5.899885</td>
      <td>3.835396</td>
    </tr>
    <tr>
      <th>ID_1</th>
      <td>4.973534</td>
      <td>0.000000</td>
      <td>4.347073</td>
      <td>5.104311</td>
      <td>6.698233</td>
    </tr>
    <tr>
      <th>ID_2</th>
      <td>5.516653</td>
      <td>4.347073</td>
      <td>0.000000</td>
      <td>7.244262</td>
      <td>8.316594</td>
    </tr>
    <tr>
      <th>ID_3</th>
      <td>5.899885</td>
      <td>5.104311</td>
      <td>7.244262</td>
      <td>0.000000</td>
      <td>4.382864</td>
    </tr>
    <tr>
      <th>ID_4</th>
      <td>3.835396</td>
      <td>6.698233</td>
      <td>8.316594</td>
      <td>4.382864</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>



<p>We can either pass a condensed distance matrix (upper triangular) from the pdist function, or we can pass the &quot;original&quot; data array and define the metric=&apos;euclidean&apos; argument in linkage. However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same.</p>
<pre><code class="lang-python"><span class="hljs-comment"># 1. incorrect approach: Squareform distance matrix</span>

<span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> linkage

row_clusters = linkage(row_dist, method=<span class="hljs-string">&apos;complete&apos;</span>, metric=<span class="hljs-string">&apos;euclidean&apos;</span>)
pd.DataFrame(row_clusters, 
             columns=[<span class="hljs-string">&apos;row label 1&apos;</span>, <span class="hljs-string">&apos;row label 2&apos;</span>, <span class="hljs-string">&apos;distance&apos;</span>, <span class="hljs-string">&apos;no. of items in clust.&apos;</span>],
             index=[<span class="hljs-string">&apos;cluster %d&apos;</span> %(i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(row_clusters.shape[<span class="hljs-number">0</span>])])
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>row label 1</th>
      <th>row label 2</th>
      <th>distance</th>
      <th>no. of items in clust.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cluster 1</th>
      <td>0.0</td>
      <td>4.0</td>
      <td>6.521973</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>cluster 2</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>6.729603</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>cluster 3</th>
      <td>3.0</td>
      <td>5.0</td>
      <td>8.539247</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>cluster 4</th>
      <td>6.0</td>
      <td>7.0</td>
      <td>12.444824</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># 2. correct approach: Condensed distance matrix</span>

row_clusters = linkage(pdist(df, metric=<span class="hljs-string">&apos;euclidean&apos;</span>), method=<span class="hljs-string">&apos;complete&apos;</span>)
pd.DataFrame(row_clusters, 
             columns=[<span class="hljs-string">&apos;row label 1&apos;</span>, <span class="hljs-string">&apos;row label 2&apos;</span>, <span class="hljs-string">&apos;distance&apos;</span>, <span class="hljs-string">&apos;no. of items in clust.&apos;</span>],
             index=[<span class="hljs-string">&apos;cluster %d&apos;</span> %(i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(row_clusters.shape[<span class="hljs-number">0</span>])])
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>row label 1</th>
      <th>row label 2</th>
      <th>distance</th>
      <th>no. of items in clust.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cluster 1</th>
      <td>0.0</td>
      <td>4.0</td>
      <td>3.835396</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>cluster 2</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>4.347073</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>cluster 3</th>
      <td>3.0</td>
      <td>5.0</td>
      <td>5.899885</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>cluster 4</th>
      <td>6.0</td>
      <td>7.0</td>
      <td>8.316594</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># 3. correct approach: Input sample matrix</span>

row_clusters = linkage(df.values, method=<span class="hljs-string">&apos;complete&apos;</span>, metric=<span class="hljs-string">&apos;euclidean&apos;</span>)
pd.DataFrame(row_clusters, 
             columns=[<span class="hljs-string">&apos;row label 1&apos;</span>, <span class="hljs-string">&apos;row label 2&apos;</span>, <span class="hljs-string">&apos;distance&apos;</span>, <span class="hljs-string">&apos;no. of items in clust.&apos;</span>],
             index=[<span class="hljs-string">&apos;cluster %d&apos;</span> %(i+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(row_clusters.shape[<span class="hljs-number">0</span>])])
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>row label 1</th>
      <th>row label 2</th>
      <th>distance</th>
      <th>no. of items in clust.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cluster 1</th>
      <td>0.0</td>
      <td>4.0</td>
      <td>3.835396</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>cluster 2</th>
      <td>1.0</td>
      <td>2.0</td>
      <td>4.347073</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>cluster 3</th>
      <td>3.0</td>
      <td>5.0</td>
      <td>5.899885</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>cluster 4</th>
      <td>6.0</td>
      <td>7.0</td>
      <td>8.316594</td>
      <td>5.0</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment"># &#x53EF;&#x89C6;&#x5316;&#x7ED3;&#x679C;, &#x4F7F;&#x7528; dendrogram</span>
<span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> dendrogram

<span class="hljs-comment"># make dendrogram black (part 1/2)</span>
<span class="hljs-comment"># from scipy.cluster.hierarchy import set_link_color_palette</span>
<span class="hljs-comment"># set_link_color_palette([&apos;black&apos;])</span>

row_dendr = dendrogram(row_clusters, 
                       labels=labels,
                       <span class="hljs-comment"># make dendrogram black (part 2/2)</span>
                       <span class="hljs-comment"># color_threshold=np.inf</span>
                       )
plt.tight_layout()
plt.ylabel(<span class="hljs-string">&apos;Euclidean distance&apos;</span>);
<span class="hljs-comment">#plt.savefig(&apos;./figures/dendrogram.png&apos;, dpi=300, bbox_inches=&apos;tight&apos;)</span>
</code></pre>
<p><img src="output_65_0.png" alt="png"></p>
<p><br></p>
<h2 id="attaching-dendrograms-to-a-heat-map"><a name="attaching-dendrograms-to-a-heat-map" class="plugin-anchor" href="#attaching-dendrograms-to-a-heat-map"><i class="fa fa-link" aria-hidden="true"></i></a>Attaching dendrograms to a heat map</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># plot row dendrogram</span>
fig = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>), facecolor=<span class="hljs-string">&apos;white&apos;</span>)
axd = fig.add_axes([<span class="hljs-number">0.09</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>])

<span class="hljs-comment"># note: for matplotlib &lt; v1.5.1, please use orientation=&apos;right&apos;</span>
row_dendr = dendrogram(row_clusters, orientation=<span class="hljs-string">&apos;left&apos;</span>)

<span class="hljs-comment"># reorder data with respect to clustering</span>
df_rowclust = df.ix[row_dendr[<span class="hljs-string">&apos;leaves&apos;</span>][::<span class="hljs-number">-1</span>]]

axd.set_xticks([])
axd.set_yticks([])

<span class="hljs-comment"># remove axes spines from dendrogram</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> axd.spines.values():
        i.set_visible(<span class="hljs-keyword">False</span>)

<span class="hljs-comment"># plot heatmap</span>
axm = fig.add_axes([<span class="hljs-number">0.23</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.6</span>])  <span class="hljs-comment"># x-pos, y-pos, width, height</span>
cax = axm.matshow(df_rowclust, interpolation=<span class="hljs-string">&apos;nearest&apos;</span>, cmap=<span class="hljs-string">&apos;hot_r&apos;</span>)
fig.colorbar(cax)
axm.set_xticklabels([<span class="hljs-string">&apos;&apos;</span>] + list(df_rowclust.columns))
axm.set_yticklabels([<span class="hljs-string">&apos;&apos;</span>] + list(df_rowclust.index));

<span class="hljs-comment"># plt.savefig(&apos;./figures/heatmap.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_69_0.png" alt="png"></p>
<p><br></p>
<h2 id="applying-agglomerative-clustering-via-scikit-learn"><a name="applying-agglomerative-clustering-via-scikit-learn" class="plugin-anchor" href="#applying-agglomerative-clustering-via-scikit-learn"><i class="fa fa-link" aria-hidden="true"></i></a>Applying agglomerative clustering via scikit-learn</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x524D;&#x9762;&#x662F;&#x7528; scipy, &#x73B0;&#x5728;&#x7528; sklearn &#x6765;&#x805A;&#x7C7B;</span>
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering

ac = AgglomerativeClustering(n_clusters=<span class="hljs-number">2</span>, affinity=<span class="hljs-string">&apos;euclidean&apos;</span>, linkage=<span class="hljs-string">&apos;complete&apos;</span>)
labels = ac.fit_predict(X)
print(<span class="hljs-string">&apos;Cluster labels: %s&apos;</span> % labels)
</code></pre>
<pre><code>Cluster labels: [0 1 1 0 0]
</code></pre><p>&#x7ED3;&#x679C;&#x4E0E;&#x524D;&#x9762;&#x4E00;&#x81F4;</p>
<p><br>
<br></p>
<h2 id="applying-agglomerative-clustering-with-iris-dataset"><a name="applying-agglomerative-clustering-with-iris-dataset" class="plugin-anchor" href="#applying-agglomerative-clustering-with-iris-dataset"><i class="fa fa-link" aria-hidden="true"></i></a>Applying agglomerative clustering with iris dataset</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets

iris = datasets.load_iris()
X = iris.data[:, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]
y = iris.target
n_samples, n_features = X.shape

plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y);
</code></pre>
<p><img src="output_78_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> linkage
<span class="hljs-keyword">from</span> scipy.cluster.hierarchy <span class="hljs-keyword">import</span> dendrogram

clusters = linkage(X, 
                   metric=<span class="hljs-string">&apos;euclidean&apos;</span>,
                   method=<span class="hljs-string">&apos;complete&apos;</span>)

dendr = dendrogram(clusters)

plt.ylabel(<span class="hljs-string">&apos;Euclidean Distance&apos;</span>);
</code></pre>
<p><img src="output_79_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering

ac = AgglomerativeClustering(n_clusters=<span class="hljs-number">3</span>,
                             affinity=<span class="hljs-string">&apos;euclidean&apos;</span>,
                             linkage=<span class="hljs-string">&apos;complete&apos;</span>)

prediction = ac.fit_predict(X)
print(<span class="hljs-string">&apos;Cluster labels:\n %s\n&apos;</span> % prediction)
</code></pre>
<pre><code>Cluster labels:
 [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0
 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0]
</code></pre><pre><code class="lang-python">plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=prediction);
</code></pre>
<p><img src="output_81_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="locating-regions-of-high-density-via-dbscan"><a name="locating-regions-of-high-density-via-dbscan" class="plugin-anchor" href="#locating-regions-of-high-density-via-dbscan"><i class="fa fa-link" aria-hidden="true"></i></a>Locating regions of high density via DBSCAN</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>Another useful approach to clustering is <em>Density-based Spatial Clustering of Applications with Noise</em> (DBSCAN). In essence, we can think of DBSCAN as an algorithm that divides the dataset into subgroup based on dense regions of point.</p>
<p>In DBSCAN, we distinguish between 3 different &quot;points&quot;:</p>
<ul>
<li>Core points: A core point is a point that has at least a minimum number of other points (MinPts) in its radius epsilon.</li>
<li>Border points: A border point is a point that is not a core point, since it doesn&apos;t have enough MinPts in its neighborhood, but lies within the radius epsilon of a core point.</li>
<li>Noise points: All other points that are neither core points nor border points.</li>
</ul>
<p><img src="../figures/dbscan.png" alt=""></p>
<p>&#x7ED9;&#x6BCF;&#x4E2A;&#x70B9; label &#x4E4B;&#x540E;, DBSCAN &#x7B97;&#x6CD5;&#x5C31;&#x662F;&#x4E0B;&#x9762;&#x4E24;&#x6B65;:</p>
<ol>
<li>Form a separate cluster for each core point or a connected group of core points (core points are connected if they are no farther away than <script type="math/tex; ">\epsilon</script> ).</li>
<li>Assign each border point to the cluster of its corresponding core point.</li>
</ol>
<p>A nice feature about DBSCAN is that we don&apos;t have to specify a number of clusters upfront. However, it requires the setting of additional hyperparameters such as the value for MinPts and the radius epsilon.</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x751F;&#x6210;&#x534A;&#x6708;&#x5F62;&#x6570;&#x636E;</span>
<span class="hljs-comment"># two visible, half-moon-shaped groups consisting of 100 sample points each</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_moons

X, y = make_moons(n_samples=<span class="hljs-number">200</span>, noise=<span class="hljs-number">0.05</span>, random_state=<span class="hljs-number">0</span>)
plt.scatter(X[:,<span class="hljs-number">0</span>], X[:,<span class="hljs-number">1</span>])
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/moons.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_86_0.png" alt="png"></p>
<p>K-means and hierarchical clustering:</p>
<pre><code class="lang-python"><span class="hljs-comment"># complete linkage clustering</span>
f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, figsize=(<span class="hljs-number">8</span>,<span class="hljs-number">3</span>))

km = KMeans(n_clusters=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>)
y_km = km.fit_predict(X)
ax1.scatter(X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], X[y_km==<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;lightblue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, s=<span class="hljs-number">40</span>, label=<span class="hljs-string">&apos;cluster 1&apos;</span>)
ax1.scatter(X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], X[y_km==<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;s&apos;</span>, s=<span class="hljs-number">40</span>, label=<span class="hljs-string">&apos;cluster 2&apos;</span>)
ax1.set_title(<span class="hljs-string">&apos;K-means clustering&apos;</span>)

ac = AgglomerativeClustering(n_clusters=<span class="hljs-number">2</span>, affinity=<span class="hljs-string">&apos;euclidean&apos;</span>, linkage=<span class="hljs-string">&apos;complete&apos;</span>)
y_ac = ac.fit_predict(X)
ax2.scatter(X[y_ac==<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], X[y_ac==<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;lightblue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, s=<span class="hljs-number">40</span>, label=<span class="hljs-string">&apos;cluster 1&apos;</span>)
ax2.scatter(X[y_ac==<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], X[y_ac==<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;s&apos;</span>, s=<span class="hljs-number">40</span>, label=<span class="hljs-string">&apos;cluster 2&apos;</span>)
ax2.set_title(<span class="hljs-string">&apos;Agglomerative clustering&apos;</span>)

plt.legend()
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/kmeans_and_ac.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_88_0.png" alt="png"></p>
<p>&#x6548;&#x679C;&#x5E76;&#x4E0D;&#x597D;, &#x5E76;&#x4E0D;&#x80FD;&#x5B8C;&#x5168; separated</p>
<p>Density-based clustering:</p>
<pre><code class="lang-python"><span class="hljs-comment"># DBSCAN</span>
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN

db = DBSCAN(eps=<span class="hljs-number">0.2</span>, min_samples=<span class="hljs-number">5</span>, metric=<span class="hljs-string">&apos;euclidean&apos;</span>)
y_db = db.fit_predict(X)
plt.scatter(X[y_db==<span class="hljs-number">0</span>,<span class="hljs-number">0</span>], X[y_db==<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;lightblue&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, s=<span class="hljs-number">40</span>, label=<span class="hljs-string">&apos;cluster 1&apos;</span>)
plt.scatter(X[y_db==<span class="hljs-number">1</span>,<span class="hljs-number">0</span>], X[y_db==<span class="hljs-number">1</span>,<span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;s&apos;</span>, s=<span class="hljs-number">40</span>, label=<span class="hljs-string">&apos;cluster 2&apos;</span>)
plt.legend()
plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/moons_dbscan.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_91_0.png" alt="png"></p>
<p>DBSCAN &#x80FD;&#x5F88;&#x597D;&#x5730;&#x5BF9;&#x534A;&#x6708;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x805A;&#x7C7B;</p>
<p>&#x53EF;&#x4EE5;&#x518D;&#x8BD5;&#x8BD5;&#x5706;&#x73AF;&#x56FE;&#x5F62;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles

X, y = make_circles(n_samples=<span class="hljs-number">500</span>, 
                    factor=<span class="hljs-number">.6</span>, 
                    noise=<span class="hljs-number">.05</span>)

plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=y);
</code></pre>
<p><img src="output_94_0.png" alt="png"></p>
<p>k-means</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans
km = KMeans(n_clusters=<span class="hljs-number">2</span>)
predict = km.fit_predict(X)
plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=predict);
</code></pre>
<p><img src="output_96_0.png" alt="png"></p>
<p>allomerative clustering</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> AgglomerativeClustering
ac = AgglomerativeClustering()
predict = ac.fit_predict(X)
plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=predict);
</code></pre>
<p><img src="output_98_0.png" alt="png"></p>
<p>DBSCAN</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN
db = DBSCAN(eps=<span class="hljs-number">0.15</span>,
            min_samples=<span class="hljs-number">9</span>,
            metric=<span class="hljs-string">&apos;euclidean&apos;</span>)
predict = db.fit_predict(X)
plt.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>], c=predict);
</code></pre>
<p><img src="output_100_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="learning-from-labeled-and-unlabeled-data-with-label-propagation"><a name="learning-from-labeled-and-unlabeled-data-with-label-propagation" class="plugin-anchor" href="#learning-from-labeled-and-unlabeled-data-with-label-propagation"><i class="fa fa-link" aria-hidden="true"></i></a>Learning from labeled and unlabeled data with label propagation</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x56E0;&#x4E3A;&#x6807;&#x6CE8;&#x6210;&#x672C;&#x6BD4;&#x8F83;&#x9AD8;&#xFF0C;&#x5F53;&#x4F60;&#x7684;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x96C6;&#x53EA;&#x6709;&#x4E00;&#x90E8;&#x5206;&#x6570;&#x636E;&#x662F;&#x6709;&#x6807;&#x6CE8;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x4F7F;&#x7528;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x4F60;&#x53EA;&#x80FD;&#x6254;&#x6389;&#x90A3;&#x4E9B;&#x6CA1;&#x6709;&#x6807;&#x6CE8;&#x7684; X &#x3002;
&#x800C;&#x5B9E;&#x9645;&#x4E0A;&#xFF0C;&#x6709;&#x6807;&#x6CE8;&#x7684;&#x6837;&#x672C;&#x548C;&#x65E0;&#x6807;&#x6CE8;&#x7684;&#x6837;&#x672C;&#x4E4B;&#x95F4;&#x662F;&#x6709;&#x5173;&#x7CFB;&#x7684;&#xFF0C;&#x8FD9;&#x79CD;&#x5173;&#x7CFB;&#x4FE1;&#x606F;&#x4E5F;&#x53EF;&#x4EE5;&#x7528;&#x6765;&#x5E2E;&#x52A9;&#x5B66;&#x4E60;&#x3002;&#x8FD9;&#x5C31;&#x662F;&#x534A;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#x6807;&#x7B7E;&#x4F20;&#x64AD;&#xFF08;Label Propagation&#xFF09;&#x7B97;&#x6CD5;&#x7684;&#x601D;&#x8DEF;&#x3002;</p>
<p>&#x5B83;&#x7684;&#x57FA;&#x672C;&#x903B;&#x8F91;&#x662F;&#x501F;&#x52A9;&#x4E8E;&#x8FD1;&#x6731;&#x8005;&#x8D64;&#x7684;&#x601D;&#x8DEF;&#xFF0C;&#x4E5F;&#x5C31;&#x662F; KNN &#x7684;&#x601D;&#x8DEF;&#xFF0C;&#x5982;&#x679C; A &#x548C; B &#x5728; X &#x7A7A;&#x95F4;&#x4E0A;&#x5F88;&#x63A5;&#x8FD1;&#xFF0C;&#x90A3;&#x4E48; A &#x7684; y &#x6807;&#x7B7E;&#x5C31;&#x53EF;&#x4EE5;&#x4F20;&#x7ED9; B&#x3002;
&#x8FDB;&#x4E00;&#x6B65;&#x8FED;&#x4EE3;&#x4E0B;&#x53BB;&#xFF0C;&#x5982;&#x679C; C &#x548C; B &#x4E5F;&#x5F88;&#x63A5;&#x8FD1;&#xFF0C;C &#x7684;&#x6807;&#x7B7E;&#x4E5F;&#x5E94;&#x8BE5;&#x548C; B &#x4E00;&#x6837;&#x3002;
&#x6240;&#x4EE5;&#x57FA;&#x672C;&#x8BA1;&#x7B97;&#x903B;&#x8F91;&#x5C31;&#x662F;&#x4E24;&#x6B65;&#xFF0C;&#x7B2C;&#x4E00;&#x6B65;&#x662F;&#x8BA1;&#x7B97;&#x6837;&#x672C;&#x95F4;&#x7684;&#x8DDD;&#x79BB;&#xFF0C;&#x6784;&#x5EFA;&#x8F6C;&#x79FB;&#x77E9;&#x9635;&#xFF0C;&#x7B2C;&#x4E8C;&#x6B65;&#x662F;&#x5C06;&#x8F6C;&#x79FB;&#x77E9;&#x9635;&#x548C; Y &#x77E9;&#x9635;&#x76F8;&#x4E58;&#xFF0C;Y &#x91CC;&#x9762;&#x5305;&#x62EC;&#x4E86;&#x5DF2;&#x6807;&#x6CE8;&#x548C;&#x672A;&#x6807;&#x6CE8;&#x7684;&#x4E24;&#x90E8;&#x5206;&#xFF0C;
&#x901A;&#x8FC7;&#x76F8;&#x4E58;&#x53EF;&#x4EE5;&#x5C06;&#x5DF2;&#x6807;&#x6CE8;&#x7684; Y &#x8F6C;&#x64AD;&#x7ED9;&#x672A;&#x6807;&#x6CE8;&#x7684; Y&#x3002;
&#x5177;&#x4F53;&#x8BBA;&#x6587;&#x53EF;&#x4EE5;<a href="http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf" target="_blank">&#x770B;&#x8FD9;&#x91CC;</a>&#x3002;
&#x5728; Sklearn &#x6A21;&#x5757;&#x4E2D;&#x5DF2;&#x7ECF;&#x5185;&#x7F6E;&#x4E86;&#x8FD9;&#x79CD;&#x7B97;&#x6CD5;&#xFF0C;&#x6587;&#x6863;&#x793A;&#x4F8B;&#x53EF;&#x4EE5;<a href="http://scikit-learn.org/stable/modules/label_propagation.html" target="_blank">&#x770B;&#x8FD9;&#x91CC;</a>&#x3002;
&#x4E0B;&#x9762;&#x662F;&#x7528; Python &#x7684; NumPy &#x6A21;&#x5757;&#x5B9E;&#x73B0;&#x7684;&#x4E00;&#x4E2A; toy demo&#x3002;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x8BFB;&#x5165; iris &#x6570;&#x636E;</span>
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris
iris = load_iris()
X = iris.data
y = iris.target
n  = len(y)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5207;&#x5206;&#x6570;&#x636E;</span>
np.random.seed(<span class="hljs-number">42</span>)
train_index = np.random.choice(n, int(<span class="hljs-number">0.6</span>*n), replace = <span class="hljs-keyword">False</span>)
test_index = np.setdiff1d(np.arange(n), train_index)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x8BA1;&#x7B97;&#x6743;&#x91CD;&#x77E9;&#x9635;</span>
sigma = X.var(axis = <span class="hljs-number">0</span>)
weights = np.zeros((n,n))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">weight_func</span><span class="hljs-params">(ind1, ind2, X=X, sigma=sigma)</span>:</span>
    <span class="hljs-keyword">return</span> np.exp(-np.sum((X[ind1] - X[ind2])**<span class="hljs-number">2</span>/sigma))

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n):
    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(n):
        weights[i,j] = weight_func(i,j)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">##  &#x6807;&#x51C6;&#x5316;&#x4E3A;&#x8F6C;&#x79FB;&#x77E9;&#x9635;</span>
t = weights / weights.sum(axis=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># y&#x8F6C;&#x6362;&#x5F62;&#x5F0F;</span>
y_m = np.zeros((n, len(np.unique(y))))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n):
    y_m[i,y[i]] = <span class="hljs-number">1</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">## unlabel&#x521D;&#x59CB;&#x5316;, label&#x8BB0;&#x4F4F;</span>
y_m[test_index] = np.random.random(y_m[test_index].shape)
clamp = y_m[train_index]
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">## &#x8FED;&#x4EE3;&#x8BA1;&#x7B97;</span>
iter_n = <span class="hljs-number">50</span>
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(iter_n):
    y_m = t.dot(y_m)  <span class="hljs-comment"># LP</span>
    y_m = (y_m.T / y_m.sum(axis=<span class="hljs-number">1</span>)).T <span class="hljs-comment"># normalize</span>
    y_m[train_index] = clamp  <span class="hljs-comment"># clamp</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#  &#x9884;&#x6D4B;&#x51C6;&#x786E;&#x7387;</span>
predict = y_m[test_index].argmax(axis=<span class="hljs-number">1</span>)
np.sum(y[test_index] == predict) / float(len(predict))
</code></pre>
<pre><code>0.91666666666666663
</code></pre><p><br>
<br></p>
<h3 id="label-propagation-learning-a-complex-structure"><a name="label-propagation-learning-a-complex-structure" class="plugin-anchor" href="#label-propagation-learning-a-complex-structure"><i class="fa fa-link" aria-hidden="true"></i></a>Label Propagation learning a complex structure</h3>
<p>Example of LabelPropagation learning a complex internal structure to demonstrate &#x201C;manifold learning&#x201D;. The outer circle should be labeled &#x201C;red&#x201D; and the inner circle &#x201C;blue&#x201D;. Because both label groups lie inside their own distinct shape, we can see that the labels propagate correctly around the circle.</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline
<span class="hljs-keyword">from</span> sklearn.semi_supervised <span class="hljs-keyword">import</span> label_propagation
<span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_circles

<span class="hljs-comment"># generate ring with inner box</span>
n_samples = <span class="hljs-number">200</span>
X, y = make_circles(n_samples=n_samples, shuffle=<span class="hljs-keyword">False</span>)
outer, inner = <span class="hljs-number">0</span>, <span class="hljs-number">1</span>
labels = -np.ones(n_samples)
labels[<span class="hljs-number">0</span>] = outer
labels[<span class="hljs-number">-1</span>] = inner

<span class="hljs-comment"># Learn with LabelSpreading</span>
label_spread = label_propagation.LabelSpreading(kernel=<span class="hljs-string">&apos;knn&apos;</span>, alpha=<span class="hljs-number">1.0</span>)
label_spread.fit(X, labels)

<span class="hljs-comment"># Plot output labels</span>
output_labels = label_spread.transduction_
plt.figure(figsize=(<span class="hljs-number">8.5</span>, <span class="hljs-number">4</span>))
plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
plt.scatter(X[labels == outer, <span class="hljs-number">0</span>], X[labels == outer, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;navy&apos;</span>,
            marker=<span class="hljs-string">&apos;s&apos;</span>, lw=<span class="hljs-number">0</span>, label=<span class="hljs-string">&quot;outer labeled&quot;</span>, s=<span class="hljs-number">10</span>)
plt.scatter(X[labels == inner, <span class="hljs-number">0</span>], X[labels == inner, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;c&apos;</span>,
            marker=<span class="hljs-string">&apos;s&apos;</span>, lw=<span class="hljs-number">0</span>, label=<span class="hljs-string">&apos;inner labeled&apos;</span>, s=<span class="hljs-number">10</span>)
plt.scatter(X[labels == <span class="hljs-number">-1</span>, <span class="hljs-number">0</span>], X[labels == <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;darkorange&apos;</span>,
            marker=<span class="hljs-string">&apos;.&apos;</span>, label=<span class="hljs-string">&apos;unlabeled&apos;</span>)
plt.legend(scatterpoints=<span class="hljs-number">1</span>, shadow=<span class="hljs-keyword">False</span>, loc=<span class="hljs-string">&apos;upper right&apos;</span>)
plt.title(<span class="hljs-string">&quot;Raw data (2 classes=outer and inner)&quot;</span>)

plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
output_label_array = np.asarray(output_labels)
outer_numbers = np.where(output_label_array == outer)[<span class="hljs-number">0</span>]
inner_numbers = np.where(output_label_array == inner)[<span class="hljs-number">0</span>]
plt.scatter(X[outer_numbers, <span class="hljs-number">0</span>], X[outer_numbers, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;navy&apos;</span>,
            marker=<span class="hljs-string">&apos;s&apos;</span>, lw=<span class="hljs-number">0</span>, s=<span class="hljs-number">10</span>, label=<span class="hljs-string">&quot;outer learned&quot;</span>)
plt.scatter(X[inner_numbers, <span class="hljs-number">0</span>], X[inner_numbers, <span class="hljs-number">1</span>], color=<span class="hljs-string">&apos;c&apos;</span>,
            marker=<span class="hljs-string">&apos;s&apos;</span>, lw=<span class="hljs-number">0</span>, s=<span class="hljs-number">10</span>, label=<span class="hljs-string">&quot;inner learned&quot;</span>)
plt.legend(scatterpoints=<span class="hljs-number">1</span>, shadow=<span class="hljs-keyword">False</span>, loc=<span class="hljs-string">&apos;upper right&apos;</span>)
plt.title(<span class="hljs-string">&quot;Labels learned with Label Spreading (KNN)&quot;</span>)

plt.subplots_adjust(left=<span class="hljs-number">0.07</span>, bottom=<span class="hljs-number">0.07</span>, right=<span class="hljs-number">0.93</span>, top=<span class="hljs-number">0.92</span>)
plt.show()
</code></pre>
<p><img src="output_117_0.png" alt="png"></p>
<h2 id="&#x7EC3;&#x4E60;&#xFF1A;&#x4F7F;&#x7528;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x81EA;&#x53D8;&#x91CF;&#xFF0C;&#x6765;&#x5BF9;&#x8FD9;&#x4E9B;&#x7528;&#x6237;&#x8FDB;&#x884C;&#x805A;&#x7C7B;"><a name="&#x7EC3;&#x4E60;&#xFF1A;&#x4F7F;&#x7528;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x81EA;&#x53D8;&#x91CF;&#xFF0C;&#x6765;&#x5BF9;&#x8FD9;&#x4E9B;&#x7528;&#x6237;&#x8FDB;&#x884C;&#x805A;&#x7C7B;" class="plugin-anchor" href="#&#x7EC3;&#x4E60;&#xFF1A;&#x4F7F;&#x7528;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x81EA;&#x53D8;&#x91CF;&#xFF0C;&#x6765;&#x5BF9;&#x8FD9;&#x4E9B;&#x7528;&#x6237;&#x8FDB;&#x884C;&#x805A;&#x7C7B;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x7EC3;&#x4E60;&#xFF1A;&#x4F7F;&#x7528;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x7684;&#x81EA;&#x53D8;&#x91CF;&#xFF0C;&#x6765;&#x5BF9;&#x8FD9;&#x4E9B;&#x7528;&#x6237;&#x8FDB;&#x884C;&#x805A;&#x7C7B;</h2>
<pre><code class="lang-python">

</code></pre>
<script type="text/javascript">var className='atoc';</script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../w5-tuning-parameter/5w.html" class="navigation navigation-prev " aria-label="Previous page: 参数调优">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../w7-text-mining/7w.html" class="navigation navigation-next " aria-label="Next page: 文本挖掘">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"无监督学习","level":"1.8","depth":1,"next":{"title":"文本挖掘","level":"1.9","depth":1,"path":"w7-text-mining/7w.md","ref":"w7-text-mining/7w.md","articles":[]},"previous":{"title":"参数调优","level":"1.7","depth":1,"path":"w5-tuning-parameter/5w.md","ref":"w5-tuning-parameter/5w.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","anchors","github","splitter","sharing","atoc","comment"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/lyltj2010/DataMining"},"atoc":{"addClass":true,"className":"atoc"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":true,"instapaper":false,"twitter":true,"vk":false,"weibo":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"comment":{"highlightCommented":true}},"theme":"default","author":"wizardforcel","pdf":{"pageNumbers":true,"fontSize":16,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"数据挖掘开源书","language":"zh","links":{"sidebar":{"数据挖掘开源书":"https://www.gitbook.com/book/wizardforcel/data-mining-book"},"gitbook":true},"gitbook":"*","description":"数据挖掘开源书"},"file":{"path":"w6-unsupervised-learning/6w.md","mtime":"2016-12-24T21:25:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-04-29T11:04:29.751Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-atoc/atoc.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-comment/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

