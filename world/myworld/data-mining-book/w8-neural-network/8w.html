
<!DOCTYPE HTML>
<html lang="zh" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>神经网络 · 数据挖掘开源书</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        <meta name="author" content="wizardforcel">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-atoc/atoc.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-comment/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../w9-deep-learning/9w.html" />
    
    
    <link rel="prev" href="../w7-text-mining/7w.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="輸入並搜尋" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://www.gitbook.com/book/wizardforcel/data-mining-book" target="_blank" class="custom-link">数据挖掘开源书</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../w0-introduction/0w.html">
            
                <a href="../w0-introduction/0w.html">
            
                    
                    数据挖掘导论和信贷模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../w1-regression/1w.html">
            
                <a href="../w1-regression/1w.html">
            
                    
                    回归模型和房价预测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../w2-preceptron-and-logistic-regression/2w.html">
            
                <a href="../w2-preceptron-and-logistic-regression/2w.html">
            
                    
                    感知机和逻辑回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                    
                    决策树和集成学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../w4-feature-engineering/4w.html">
            
                <a href="../w4-feature-engineering/4w.html">
            
                    
                    特征工程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../w5-tuning-parameter/5w.html">
            
                <a href="../w5-tuning-parameter/5w.html">
            
                    
                    参数调优
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../w6-unsupervised-learning/6w.html">
            
                <a href="../w6-unsupervised-learning/6w.html">
            
                    
                    无监督学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../w7-text-mining/7w.html">
            
                <a href="../w7-text-mining/7w.html">
            
                    
                    文本挖掘
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.10" data-path="8w.html">
            
                <a href="8w.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../w9-deep-learning/9w.html">
            
                <a href="../w9-deep-learning/9w.html">
            
                    
                    深度学习
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本書使用 GitBook 釋出
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >神经网络</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="sections"><a name="sections" class="plugin-anchor" href="#sections"><i class="fa fa-link" aria-hidden="true"></i></a>Sections</h2>
<ul>
<li><a href="#modeling-complex-functions-with-artificial-neural-networks">Modeling complex functions with artificial neural networks</a><ul>
<li><a href="#single-layer-neural-network-recap">Single-layer neural network recap</a></li>
<li><a href="#introducing-the-multi-layer-neural-network-architecture">Introducing the multi-layer neural network architecture</a></li>
<li><a href="#mlp-learning-procedure">MLP learning procedure</a></li>
<li><a href="#activating-a-neural-network-via-forward-propagation">Activating a neural network via forward propagation</a></li>
<li><a href="#loss-functions">Loss functions</a></li>
<li><a href="#training-neural-networks-via-backpropagation">Training neural networks via backpropagation</a></li>
<li><a href="#optimizaiton-methods">Optimizaiton methods</a></li>
</ul>
</li>
<li><a href="#classifying-handwritten-digits">Classifying handwritten digits</a><ul>
<li><a href="#obtaining-the-mnist-dataset">Obtaining the MNIST dataset</a></li>
<li><a href="#implementing-a-multi-layer-perceptron">Implementing a multi-layer perceptron</a></li>
</ul>
</li>
<li><a href="#training-an-artificial-neural-network">Training an artificial neural network</a></li>
<li><a href="#debugging-neural-networks-with-gradient-checking">Debugging neural networks with gradient checking</a></li>
<li><a href="#other-neural-network-architectures">Other neural network architectures</a><ul>
<li><a href="#convolutional-neural-networks">Convolutional Neural Networks</a></li>
<li><a href="#recurrent-neural-networks">Recurrent Neural Networks</a></li>
</ul>
</li>
</ul>
<h1 id="modeling-complex-functions-with-artificial-neural-networks"><a name="modeling-complex-functions-with-artificial-neural-networks" class="plugin-anchor" href="#modeling-complex-functions-with-artificial-neural-networks"><i class="fa fa-link" aria-hidden="true"></i></a>Modeling complex functions with artificial neural networks</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="single-layer-neural-network-recap"><a name="single-layer-neural-network-recap" class="plugin-anchor" href="#single-layer-neural-network-recap"><i class="fa fa-link" aria-hidden="true"></i></a>Single-layer neural network recap</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/single_layer_nn.png" style="width:600px"></p>
<h4 id="linear-combination"><a name="linear-combination" class="plugin-anchor" href="#linear-combination"><i class="fa fa-link" aria-hidden="true"></i></a>linear combination</h4>
<p>Linear combination<script type="math/tex; ">z=\sum_{i=1}^m x_i\times w_i + w_0 </script>&#xFF0C;&#x5176;&#x4E2D;<script type="math/tex; ">w_0</script>&#x662F;bias &#x9879;&#x3002;</p>
<h4 id="activation-functions"><a name="activation-functions" class="plugin-anchor" href="#activation-functions"><i class="fa fa-link" aria-hidden="true"></i></a>activation functions</h4>
<p><img src="../figures/activations.jpg" alt=""></p>
<p><br>
<br></p>
<h2 id="introducing-the-multi-layer-neural-network-architecture"><a name="introducing-the-multi-layer-neural-network-architecture" class="plugin-anchor" href="#introducing-the-multi-layer-neural-network-architecture"><i class="fa fa-link" aria-hidden="true"></i></a>Introducing the multi-layer neural network architecture</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>Multi-layer perceptron (MLP)</p>
<p>MLP &#x53EF;&#x4EE5;&#x770B;&#x6210;&#x591A;&#x6B21;&#x7EBF;&#x6027;&#x8F6C;&#x6362;&#x548C;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x7684;&#x5806;&#x53E0;, &#x4E0B;&#x56FE;&#x7ED3;&#x6784;&#x53EF;&#x4EE5;&#x5199;&#x6210;
<script type="math/tex; ">y = \sigma({\sigma (XW^{(1)} + b^{(1)})}W^{(2)} + b^{(2)})</script></p>
<p><img src="../figures/multi_layer_nn.png" style="width:500px"></p>
<p><br>
<br></p>
<h2 id="mlp-learning-procedure"><a name="mlp-learning-procedure" class="plugin-anchor" href="#mlp-learning-procedure"><i class="fa fa-link" aria-hidden="true"></i></a>MLP learning procedure</h2>
<p>[<a href="#sections">back to top</a>]</p>
<ol>
<li>Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output.</li>
<li>Based on the network&apos;s output, we calculate the error that we want to minimize using a cost function that we will describe later.</li>
<li>We backpropagate the error, find its derivative with respect to each weight in the network, and update the model.</li>
</ol>
<p><br>
<br></p>
<h2 id="activating-a-neural-network-via-forward-propagation"><a name="activating-a-neural-network-via-forward-propagation" class="plugin-anchor" href="#activating-a-neural-network-via-forward-propagation"><i class="fa fa-link" aria-hidden="true"></i></a>Activating a neural network via forward propagation</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/feedforward.png" style="width:500px"></p>
<p><br>
<br></p>
<h2 id="loss-functions"><a name="loss-functions" class="plugin-anchor" href="#loss-functions"><i class="fa fa-link" aria-hidden="true"></i></a>Loss functions</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/lossf.png" style="width:700px"></p>
<p><br>
<br></p>
<h2 id="training-neural-networks-via-backpropagation"><a name="training-neural-networks-via-backpropagation" class="plugin-anchor" href="#training-neural-networks-via-backpropagation"><i class="fa fa-link" aria-hidden="true"></i></a>Training neural networks via backpropagation</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/backpropagation.png" style="width:500px"></p>
<p><br>
<br></p>
<h2 id="optimizaiton-methods"><a name="optimizaiton-methods" class="plugin-anchor" href="#optimizaiton-methods"><i class="fa fa-link" aria-hidden="true"></i></a>Optimizaiton methods</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/optim.jpg" style="width:700px"></p>
<p><br>
<br></p>
<h1 id="classifying-handwritten-digits"><a name="classifying-handwritten-digits" class="plugin-anchor" href="#classifying-handwritten-digits"><i class="fa fa-link" aria-hidden="true"></i></a>Classifying handwritten digits</h1>
<h2 id="obtaining-the-mnist-dataset"><a name="obtaining-the-mnist-dataset" class="plugin-anchor" href="#obtaining-the-mnist-dataset"><i class="fa fa-link" aria-hidden="true"></i></a>Obtaining the MNIST dataset</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>The MNIST dataset is publicly available at <a href="http://yann.lecun.com/exdb/mnist/" target="_blank">http://yann.lecun.com/exdb/mnist/</a> and consists of the following four parts:</p>
<ul>
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples)</li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)</li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples)</li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)</li>
</ul>
<p>In this section, we will only be working with a subset of MNIST, thus, we only need to download the training set images and training set labels. After downloading the files, I recommend unzipping the files using the Unix/Linux gzip tool from the terminal for efficiency, e.g., using the command</p>
<pre><code>gzip *ubyte.gz -d
</code></pre><p>in your local MNIST download directory, or, using your favorite unzipping tool if you are working with a machine running on Microsoft Windows. The images are stored in byte form, and using the following function, we will read them into NumPy arrays that we will use to train our MLP.</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> struct
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_mnist</span><span class="hljs-params">(path, kind=<span class="hljs-string">&apos;train&apos;</span>)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Load MNIST data from `path`&quot;&quot;&quot;</span>
    labels_path = os.path.join(path,
                               <span class="hljs-string">&apos;%s-labels-idx1-ubyte&apos;</span>
                                % kind)
    images_path = os.path.join(path,
                               <span class="hljs-string">&apos;%s-images-idx3-ubyte&apos;</span>
                               % kind)

    <span class="hljs-keyword">with</span> open(labels_path, <span class="hljs-string">&apos;rb&apos;</span>) <span class="hljs-keyword">as</span> lbpath:
        magic, n = struct.unpack(<span class="hljs-string">&apos;&gt;II&apos;</span>,
                                 lbpath.read(<span class="hljs-number">8</span>))
        labels = np.fromfile(lbpath,
                             dtype=np.uint8)

    <span class="hljs-keyword">with</span> open(images_path, <span class="hljs-string">&apos;rb&apos;</span>) <span class="hljs-keyword">as</span> imgpath:
        magic, num, rows, cols = struct.unpack(<span class="hljs-string">&quot;&gt;IIII&quot;</span>,
                                               imgpath.read(<span class="hljs-number">16</span>))
        images = np.fromfile(imgpath,
                             dtype=np.uint8).reshape(len(labels), <span class="hljs-number">784</span>)

    <span class="hljs-keyword">return</span> images, labels
</code></pre>
<pre><code class="lang-python">X_train, y_train = load_mnist(<span class="hljs-string">&apos;data/mnist&apos;</span>, kind=<span class="hljs-string">&apos;train&apos;</span>)
print(<span class="hljs-string">&apos;Rows: %d, columns: %d&apos;</span> % (X_train.shape[<span class="hljs-number">0</span>], X_train.shape[<span class="hljs-number">1</span>]))
</code></pre>
<pre><code>Rows: 60000, columns: 784
</code></pre><pre><code class="lang-python">X_test, y_test = load_mnist(<span class="hljs-string">&apos;data/mnist&apos;</span>, kind=<span class="hljs-string">&apos;t10k&apos;</span>)
print(<span class="hljs-string">&apos;Rows: %d, columns: %d&apos;</span> % (X_test.shape[<span class="hljs-number">0</span>], X_test.shape[<span class="hljs-number">1</span>]))
</code></pre>
<pre><code>Rows: 10000, columns: 784
</code></pre><p>Visualize the first digit of each class:</p>
<pre><code class="lang-python"><span class="hljs-comment"># import pandas as pd</span>
<span class="hljs-comment"># train = pd.read_csv(&apos;data/digit_recognizer/train.csv&apos;)</span>
<span class="hljs-comment"># train.shape</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

fig, ax = plt.subplots(nrows=<span class="hljs-number">2</span>, ncols=<span class="hljs-number">5</span>, sharex=<span class="hljs-keyword">True</span>, sharey=<span class="hljs-keyword">True</span>,)
ax = ax.flatten()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
    img = X_train[y_train == i][<span class="hljs-number">0</span>].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
    ax[i].imshow(img, cmap=<span class="hljs-string">&apos;Greys&apos;</span>, interpolation=<span class="hljs-string">&apos;nearest&apos;</span>)

ax[<span class="hljs-number">0</span>].set_xticks([])
ax[<span class="hljs-number">0</span>].set_yticks([])
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/mnist_all.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_45_0.png" alt="png"></p>
<p>Visualize 25 different versions of &quot;7&quot;:</p>
<pre><code class="lang-python">fig, ax = plt.subplots(nrows=<span class="hljs-number">5</span>, ncols=<span class="hljs-number">5</span>, sharex=<span class="hljs-keyword">True</span>, sharey=<span class="hljs-keyword">True</span>,)
ax = ax.flatten()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">25</span>):
    img = X_train[y_train == <span class="hljs-number">7</span>][i].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
    ax[i].imshow(img, cmap=<span class="hljs-string">&apos;Greys&apos;</span>, interpolation=<span class="hljs-string">&apos;nearest&apos;</span>)

ax[<span class="hljs-number">0</span>].set_xticks([])
ax[<span class="hljs-number">0</span>].set_yticks([])
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/mnist_7.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_47_0.png" alt="png"></p>
<p>Uncomment the following lines to optionally save the data in CSV format.
However, note that those CSV files will take up a substantial amount of storage space:</p>
<ul>
<li>train_img.csv 1.1 GB (gigabytes)</li>
<li>train_labels.csv 1.4 MB (megabytes)</li>
<li>test_img.csv 187.0 MB</li>
<li>test_labels 144 KB (kilobytes)</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># np.savetxt(&apos;train_img.csv&apos;, X_train, fmt=&apos;%i&apos;, delimiter=&apos;,&apos;)</span>
<span class="hljs-comment"># np.savetxt(&apos;train_labels.csv&apos;, y_train, fmt=&apos;%i&apos;, delimiter=&apos;,&apos;)</span>
<span class="hljs-comment"># X_train = np.genfromtxt(&apos;train_img.csv&apos;, dtype=int, delimiter=&apos;,&apos;)</span>
<span class="hljs-comment"># y_train = np.genfromtxt(&apos;train_labels.csv&apos;, dtype=int, delimiter=&apos;,&apos;)</span>

<span class="hljs-comment"># np.savetxt(&apos;test_img.csv&apos;, X_test, fmt=&apos;%i&apos;, delimiter=&apos;,&apos;)</span>
<span class="hljs-comment"># np.savetxt(&apos;test_labels.csv&apos;, y_test, fmt=&apos;%i&apos;, delimiter=&apos;,&apos;)</span>
<span class="hljs-comment"># X_test = np.genfromtxt(&apos;test_img.csv&apos;, dtype=int, delimiter=&apos;,&apos;)</span>
<span class="hljs-comment"># y_test = np.genfromtxt(&apos;test_labels.csv&apos;, dtype=int, delimiter=&apos;,&apos;)</span>
</code></pre>
<p><br>
<br></p>
<h2 id="implementing-a-multi-layer-perceptron"><a name="implementing-a-multi-layer-perceptron" class="plugin-anchor" href="#implementing-a-multi-layer-perceptron"><i class="fa fa-link" aria-hidden="true"></i></a>Implementing a multi-layer perceptron</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> expit
<span class="hljs-keyword">import</span> sys


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNetMLP</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; Feedforward neural network / Multi-layer perceptron classifier.

    Parameters
    ------------
    n_output : int
      Number of output units, should be equal to the
      number of unique class labels.

    n_features : int
      Number of features (dimensions) in the target dataset.
      Should be equal to the number of columns in the X array.

    n_hidden : int (default: 30)
      Number of hidden units.

    l1 : float (default: 0.0)
      Lambda value for L1-regularization.
      No regularization if l1=0.0 (default)

    l2 : float (default: 0.0)
      Lambda value for L2-regularization.
      No regularization if l2=0.0 (default)

    epochs : int (default: 500)
      Number of passes over the training set.

    eta : float (default: 0.001)
      Learning rate.

    alpha : float (default: 0.0)
      Momentum constant. Factor multiplied with the
      gradient of the previous epoch t-1 to improve
      learning speed
      w(t) := w(t) - (grad(t) + alpha*grad(t-1))

    decrease_const : float (default: 0.0)
      Decrease constant. Shrinks the learning rate
      after each epoch via eta / (1 + epoch*decrease_const)

    shuffle : bool (default: False)
      Shuffles training data every epoch if True to prevent circles.

    minibatches : int (default: 1)
      Divides training data into k minibatches for efficiency.
      Normal gradient descent learning if k=1 (default).

    random_state : int (default: None)
      Set random state for shuffling and initializing the weights.

    Attributes
    -----------
    cost_ : list
      Sum of squared errors after each epoch.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, n_output, n_features, n_hidden=<span class="hljs-number">30</span>,
                 l1=<span class="hljs-number">0.0</span>, l2=<span class="hljs-number">0.0</span>, epochs=<span class="hljs-number">500</span>, eta=<span class="hljs-number">0.001</span>,
                 alpha=<span class="hljs-number">0.0</span>, decrease_const=<span class="hljs-number">0.0</span>, shuffle=True,
                 minibatches=<span class="hljs-number">1</span>, random_state=None)</span>:</span>

        np.random.seed(random_state)
        self.n_output = n_output
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.w1, self.w2 = self._initialize_weights()
        self.l1 = l1
        self.l2 = l2
        self.epochs = epochs
        self.eta = eta
        self.alpha = alpha
        self.decrease_const = decrease_const
        self.shuffle = shuffle
        self.minibatches = minibatches

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_encode_labels</span><span class="hljs-params">(self, y, k)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Encode labels into one-hot representation

        Parameters
        ------------
        y : array, shape = [n_samples]
            Target values.

        Returns
        -----------
        onehot : array, shape = (n_labels, n_samples)

        &quot;&quot;&quot;</span>
        onehot = np.zeros((k, y.shape[<span class="hljs-number">0</span>]))
        <span class="hljs-keyword">for</span> idx, val <span class="hljs-keyword">in</span> enumerate(y):
            onehot[val, idx] = <span class="hljs-number">1.0</span>
        <span class="hljs-keyword">return</span> onehot

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_initialize_weights</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Initialize weights with small random numbers.&quot;&quot;&quot;</span>
        w1 = np.random.uniform(<span class="hljs-number">-1.0</span>, <span class="hljs-number">1.0</span>, size=self.n_hidden*(self.n_features + <span class="hljs-number">1</span>))
        w1 = w1.reshape(self.n_hidden, self.n_features + <span class="hljs-number">1</span>)
        w2 = np.random.uniform(<span class="hljs-number">-1.0</span>, <span class="hljs-number">1.0</span>, size=self.n_output*(self.n_hidden + <span class="hljs-number">1</span>))
        w2 = w2.reshape(self.n_output, self.n_hidden + <span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> w1, w2

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_sigmoid</span><span class="hljs-params">(self, z)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute logistic function (sigmoid)

        Uses scipy.special.expit to avoid overflow
        error for very small input values z.

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># return 1.0 / (1.0 + np.exp(-z))</span>
        <span class="hljs-keyword">return</span> expit(z)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_sigmoid_gradient</span><span class="hljs-params">(self, z)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute gradient of the logistic function&quot;&quot;&quot;</span>
        sg = self._sigmoid(z)
        <span class="hljs-keyword">return</span> sg * (<span class="hljs-number">1</span> - sg)  <span class="hljs-comment"># sigmoid &#x51FD;&#x6570;&#x7684;&#x5BFC;&#x6570;&#x6BD4;&#x8F83;&#x7B80;&#x5355;&#xFF0C;&#x5C31;&#x662F; sg * (1-sg)</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_add_bias_unit</span><span class="hljs-params">(self, X, how=<span class="hljs-string">&apos;column&apos;</span>)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Add bias unit (column or row of 1s) to array at index 0&quot;&quot;&quot;</span>
        <span class="hljs-keyword">if</span> how == <span class="hljs-string">&apos;column&apos;</span>:
            X_new = np.ones((X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">1</span>))
            X_new[:, <span class="hljs-number">1</span>:] = X
        <span class="hljs-keyword">elif</span> how == <span class="hljs-string">&apos;row&apos;</span>:
            X_new = np.ones((X.shape[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))
            X_new[<span class="hljs-number">1</span>:, :] = X
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> AttributeError(<span class="hljs-string">&apos;`how` must be `column` or `row`&apos;</span>)
        <span class="hljs-keyword">return</span> X_new

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_feedforward</span><span class="hljs-params">(self, X, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute feedforward step

        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
          Input layer with original features.

        w1 : array, shape = [n_hidden_units, n_features]
          Weight matrix for input layer -&gt; hidden layer.

        w2 : array, shape = [n_output_units, n_hidden_units]
          Weight matrix for hidden layer -&gt; output layer.

        Returns
        ----------
        a1 : array, shape = [n_samples, n_features+1]
          Input values with bias unit.

        z2 : array, shape = [n_hidden, n_samples]
          Net input of hidden layer.

        a2 : array, shape = [n_hidden+1, n_samples]
          Activation of hidden layer.

        z3 : array, shape = [n_output_units, n_samples]
          Net input of output layer.

        a3 : array, shape = [n_output_units, n_samples]
          Activation of output layer.

        &quot;&quot;&quot;</span>
        a1 = self._add_bias_unit(X, how=<span class="hljs-string">&apos;column&apos;</span>)
        z2 = w1.dot(a1.T)
        a2 = self._sigmoid(z2)
        a2 = self._add_bias_unit(a2, how=<span class="hljs-string">&apos;row&apos;</span>)
        z3 = w2.dot(a2)
        a3 = self._sigmoid(z3)
        <span class="hljs-keyword">return</span> a1, z2, a2, z3, a3

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_L2_reg</span><span class="hljs-params">(self, lambda_, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute L2-regularization cost&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> (lambda_/<span class="hljs-number">2.0</span>) * (np.sum(w1[:, <span class="hljs-number">1</span>:] ** <span class="hljs-number">2</span>) + np.sum(w2[:, <span class="hljs-number">1</span>:] ** <span class="hljs-number">2</span>))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_L1_reg</span><span class="hljs-params">(self, lambda_, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute L1-regularization cost&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> (lambda_/<span class="hljs-number">2.0</span>) * (np.abs(w1[:, <span class="hljs-number">1</span>:]).sum() + np.abs(w2[:, <span class="hljs-number">1</span>:]).sum())

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_cost</span><span class="hljs-params">(self, y_enc, output, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute cost function.

        y_enc : array, shape = (n_labels, n_samples)
          one-hot encoded class labels.

        output : array, shape = [n_output_units, n_samples]
          Activation of the output layer (feedforward)

        w1 : array, shape = [n_hidden_units, n_features]
          Weight matrix for input layer -&gt; hidden layer.

        w2 : array, shape = [n_output_units, n_hidden_units]
          Weight matrix for hidden layer -&gt; output layer.

        Returns
        ---------
        cost : float
          Regularized cost.

        &quot;&quot;&quot;</span>
        term1 = -y_enc * (np.log(output))
        term2 = (<span class="hljs-number">1</span> - y_enc) * np.log(<span class="hljs-number">1</span> - output)
        cost = np.sum(term1 - term2)  <span class="hljs-comment"># &#x4EA4;&#x4E92;&#x71B5;</span>
        L1_term = self._L1_reg(self.l1, w1, w2)
        L2_term = self._L2_reg(self.l2, w1, w2)
        cost = cost + L1_term + L2_term
        <span class="hljs-keyword">return</span> cost

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_gradient</span><span class="hljs-params">(self, a1, a2, a3, z2, y_enc, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Compute gradient step using backpropagation.

        Parameters
        ------------
        a1 : array, shape = [n_samples, n_features+1]
          Input values with bias unit.

        a2 : array, shape = [n_hidden+1, n_samples]
          Activation of hidden layer.

        a3 : array, shape = [n_output_units, n_samples]
          Activation of output layer.

        z2 : array, shape = [n_hidden, n_samples]
          Net input of hidden layer.

        y_enc : array, shape = (n_labels, n_samples)
          one-hot encoded class labels.

        w1 : array, shape = [n_hidden_units, n_features]
          Weight matrix for input layer -&gt; hidden layer.

        w2 : array, shape = [n_output_units, n_hidden_units]
          Weight matrix for hidden layer -&gt; output layer.

        Returns
        ---------

        grad1 : array, shape = [n_hidden_units, n_features]
          Gradient of the weight matrix w1.

        grad2 : array, shape = [n_output_units, n_hidden_units]
            Gradient of the weight matrix w2.

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># backpropagation</span>
        sigma3 = a3 - y_enc
        z2 = self._add_bias_unit(z2, how=<span class="hljs-string">&apos;row&apos;</span>)
        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)
        sigma2 = sigma2[<span class="hljs-number">1</span>:, :]
        grad1 = sigma2.dot(a1)
        grad2 = sigma3.dot(a2.T)

        <span class="hljs-comment"># regularize</span>
        grad1[:, <span class="hljs-number">1</span>:] += (w1[:, <span class="hljs-number">1</span>:] * (self.l1 + self.l2))
        grad2[:, <span class="hljs-number">1</span>:] += (w2[:, <span class="hljs-number">1</span>:] * (self.l1 + self.l2))

        <span class="hljs-keyword">return</span> grad1, grad2

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Predict class labels

        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
          Input layer with original features.

        Returns:
        ----------
        y_pred : array, shape = [n_samples]
          Predicted class labels.

        &quot;&quot;&quot;</span>
        <span class="hljs-keyword">if</span> len(X.shape) != <span class="hljs-number">2</span>:
            <span class="hljs-keyword">raise</span> AttributeError(<span class="hljs-string">&apos;X must be a [n_samples, n_features] array.\n&apos;</span>
                                 <span class="hljs-string">&apos;Use X[:,None] for 1-feature classification,&apos;</span>
                                 <span class="hljs-string">&apos;\nor X[[i]] for 1-sample classification&apos;</span>)

        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)
        y_pred = np.argmax(z3, axis=<span class="hljs-number">0</span>)
        <span class="hljs-keyword">return</span> y_pred

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y, print_progress=False)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Learn weights from training data.

        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
          Input layer with original features.

        y : array, shape = [n_samples]
          Target class labels.

        print_progress : bool (default: False)
          Prints progress as the number of epochs
          to stderr.

        Returns:
        ----------
        self

        &quot;&quot;&quot;</span>
        self.cost_ = []
        X_data, y_data = X.copy(), y.copy()
        y_enc = self._encode_labels(y, self.n_output)

        delta_w1_prev = np.zeros(self.w1.shape)
        delta_w2_prev = np.zeros(self.w2.shape)

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.epochs):

            <span class="hljs-comment"># adaptive learning rate</span>
            self.eta /= (<span class="hljs-number">1</span> + self.decrease_const*i)

            <span class="hljs-keyword">if</span> print_progress:
                sys.stderr.write(<span class="hljs-string">&apos;\rEpoch: %d/%d&apos;</span> % (i+<span class="hljs-number">1</span>, self.epochs))
                sys.stderr.flush()

            <span class="hljs-keyword">if</span> self.shuffle:
                idx = np.random.permutation(y_data.shape[<span class="hljs-number">0</span>])
                X_data, y_data = X_data[idx], y_data[idx]

            mini = np.array_split(range(y_data.shape[<span class="hljs-number">0</span>]), self.minibatches)
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mini:

                <span class="hljs-comment"># feedforward</span>
                a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2)
                cost = self._get_cost(y_enc=y_enc[:, idx],
                                      output=a3,
                                      w1=self.w1,
                                      w2=self.w2)
                self.cost_.append(cost)

                <span class="hljs-comment"># compute gradient via backpropagation</span>
                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,
                                                  a3=a3, z2=z2,
                                                  y_enc=y_enc[:, idx],
                                                  w1=self.w1,
                                                  w2=self.w2)

                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2
                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))
                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))
                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2

        <span class="hljs-keyword">return</span> self
</code></pre>
<p><br>
<br></p>
<h2 id="training-an-artificial-neural-network"><a name="training-an-artificial-neural-network" class="plugin-anchor" href="#training-an-artificial-neural-network"><i class="fa fa-link" aria-hidden="true"></i></a>Training an artificial neural network</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python">nn = NeuralNetMLP(n_output=<span class="hljs-number">10</span>,
                  n_features=X_train.shape[<span class="hljs-number">1</span>],
                  n_hidden=<span class="hljs-number">50</span>,
                  l2=<span class="hljs-number">0.1</span>,
                  l1=<span class="hljs-number">0.0</span>,
                  epochs=<span class="hljs-number">1000</span>,
                  eta=<span class="hljs-number">0.001</span>,
                  alpha=<span class="hljs-number">0.001</span>,
                  decrease_const=<span class="hljs-number">0.00001</span>,
                  minibatches=<span class="hljs-number">50</span>,
                  random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python">nn.fit(X_train, y_train, print_progress=<span class="hljs-keyword">True</span>)
</code></pre>
<pre><code>Epoch: 1000/1000




&lt;__main__.NeuralNetMLP at 0x11854e7d0&gt;
</code></pre><pre><code class="lang-python">plt.plot(range(len(nn.cost_)), nn.cost_)
plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">2000</span>])
plt.ylabel(<span class="hljs-string">&apos;Cost&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Epochs * 50&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/cost.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_59_0.png" alt="png"></p>
<pre><code class="lang-python">batches = np.array_split(range(len(nn.cost_)), <span class="hljs-number">1000</span>)
cost_ary = np.array(nn.cost_)
cost_avgs = [np.mean(cost_ary[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> batches]
</code></pre>
<pre><code class="lang-python">plt.plot(range(len(cost_avgs)), cost_avgs, color=<span class="hljs-string">&apos;red&apos;</span>)
plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">2000</span>])
plt.ylabel(<span class="hljs-string">&apos;Cost&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
plt.tight_layout();
<span class="hljs-comment">#plt.savefig(&apos;./figures/cost2.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_61_0.png" alt="png"></p>
<pre><code class="lang-python">y_train_pred = nn.predict(X_train)
acc = np.sum(y_train == y_train_pred, axis=<span class="hljs-number">0</span>) / X_train.shape[<span class="hljs-number">0</span>]
print(<span class="hljs-string">&apos;Training accuracy: %.2f%%&apos;</span> % (acc * <span class="hljs-number">100</span>))
</code></pre>
<pre><code>Training accuracy: 0.00%
</code></pre><pre><code class="lang-python">y_test_pred = nn.predict(X_test)
acc = np.sum(y_test == y_test_pred, axis=<span class="hljs-number">0</span>) / X_test.shape[<span class="hljs-number">0</span>]
print(<span class="hljs-string">&apos;Training accuracy: %.2f%%&apos;</span> % (acc * <span class="hljs-number">100</span>))
</code></pre>
<pre><code>Training accuracy: 0.00%
</code></pre><pre><code class="lang-python">miscl_img = X_test[y_test != y_test_pred][:<span class="hljs-number">25</span>]
correct_lab = y_test[y_test != y_test_pred][:<span class="hljs-number">25</span>]
miscl_lab= y_test_pred[y_test != y_test_pred][:<span class="hljs-number">25</span>]

fig, ax = plt.subplots(nrows=<span class="hljs-number">5</span>, ncols=<span class="hljs-number">5</span>, sharex=<span class="hljs-keyword">True</span>, sharey=<span class="hljs-keyword">True</span>,)
ax = ax.flatten()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">25</span>):
    img = miscl_img[i].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)
    ax[i].imshow(img, cmap=<span class="hljs-string">&apos;Greys&apos;</span>, interpolation=<span class="hljs-string">&apos;nearest&apos;</span>)
    ax[i].set_title(<span class="hljs-string">&apos;%d) t: %d p: %d&apos;</span> % (i+<span class="hljs-number">1</span>, correct_lab[i], miscl_lab[i]))

ax[<span class="hljs-number">0</span>].set_xticks([])
ax[<span class="hljs-number">0</span>].set_yticks([])
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/mnist_miscl.png&apos;, dpi=300)</span>
plt.show()
</code></pre>
<p><img src="output_64_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="debugging-neural-networks-with-gradient-checking"><a name="debugging-neural-networks-with-gradient-checking" class="plugin-anchor" href="#debugging-neural-networks-with-gradient-checking"><i class="fa fa-link" aria-hidden="true"></i></a>Debugging neural networks with gradient checking</h1>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> scipy.special <span class="hljs-keyword">import</span> expit
<span class="hljs-keyword">import</span> sys


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MLPGradientCheck</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; Feedforward neural network / Multi-layer perceptron classifier.

    Parameters
    ------------
    n_output : int
        Number of output units, should be equal to the
        number of unique class labels.
    n_features : int
        Number of features (dimensions) in the target dataset.
        Should be equal to the number of columns in the X array.
    n_hidden : int (default: 30)
        Number of hidden units.
    l1 : float (default: 0.0)
        Lambda value for L1-regularization.
        No regularization if l1=0.0 (default)
    l2 : float (default: 0.0)
        Lambda value for L2-regularization.
        No regularization if l2=0.0 (default)
    epochs : int (default: 500)
        Number of passes over the training set.
    eta : float (default: 0.001)
        Learning rate.
    alpha : float (default: 0.0)
        Momentum constant. Factor multiplied with the
        gradient of the previous epoch t-1 to improve
        learning speed
        w(t) := w(t) - (grad(t) + alpha*grad(t-1))
    decrease_const : float (default: 0.0)
        Decrease constant. Shrinks the learning rate
        after each epoch via eta / (1 + epoch*decrease_const)
    shuffle : bool (default: False)
        Shuffles training data every epoch if True to prevent circles.
    minibatches : int (default: 1)
        Divides training data into k minibatches for efficiency.
        Normal gradient descent learning if k=1 (default).
    random_state : int (default: None)
        Set random state for shuffling and initializing the weights.

    Attributes
    -----------
    cost_ : list
        Sum of squared errors after each epoch.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, n_output, n_features, n_hidden=<span class="hljs-number">30</span>,
                 l1=<span class="hljs-number">0.0</span>, l2=<span class="hljs-number">0.0</span>, epochs=<span class="hljs-number">500</span>, eta=<span class="hljs-number">0.001</span>,
                 alpha=<span class="hljs-number">0.0</span>, decrease_const=<span class="hljs-number">0.0</span>, shuffle=True,
                 minibatches=<span class="hljs-number">1</span>, random_state=None)</span>:</span>

        np.random.seed(random_state)
        self.n_output = n_output
        self.n_features = n_features
        self.n_hidden = n_hidden
        self.w1, self.w2 = self._initialize_weights()
        self.l1 = l1
        self.l2 = l2
        self.epochs = epochs
        self.eta = eta
        self.alpha = alpha
        self.decrease_const = decrease_const
        self.shuffle = shuffle
        self.minibatches = minibatches

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_encode_labels</span><span class="hljs-params">(self, y, k)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Encode labels into one-hot representation

        Parameters
        ------------
        y : array, shape = [n_samples]
            Target values.

        Returns
        -----------
        onehot : array, shape = (n_labels, n_samples)

        &quot;&quot;&quot;</span>
        onehot = np.zeros((k, y.shape[<span class="hljs-number">0</span>]))
        <span class="hljs-keyword">for</span> idx, val <span class="hljs-keyword">in</span> enumerate(y):
            onehot[val, idx] = <span class="hljs-number">1.0</span>
        <span class="hljs-keyword">return</span> onehot

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_initialize_weights</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Initialize weights with small random numbers.&quot;&quot;&quot;</span>
        w1 = np.random.uniform(<span class="hljs-number">-1.0</span>, <span class="hljs-number">1.0</span>,
                               size=self.n_hidden*(self.n_features + <span class="hljs-number">1</span>))
        w1 = w1.reshape(self.n_hidden, self.n_features + <span class="hljs-number">1</span>)
        w2 = np.random.uniform(<span class="hljs-number">-1.0</span>, <span class="hljs-number">1.0</span>,
                               size=self.n_output*(self.n_hidden + <span class="hljs-number">1</span>))
        w2 = w2.reshape(self.n_output, self.n_hidden + <span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> w1, w2

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_sigmoid</span><span class="hljs-params">(self, z)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute logistic function (sigmoid)

        Uses scipy.special.expit to avoid overflow
        error for very small input values z.

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># return 1.0 / (1.0 + np.exp(-z))</span>
        <span class="hljs-keyword">return</span> expit(z)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_sigmoid_gradient</span><span class="hljs-params">(self, z)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute gradient of the logistic function&quot;&quot;&quot;</span>
        sg = self._sigmoid(z)
        <span class="hljs-keyword">return</span> sg * (<span class="hljs-number">1.0</span> - sg)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_add_bias_unit</span><span class="hljs-params">(self, X, how=<span class="hljs-string">&apos;column&apos;</span>)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Add bias unit (column or row of 1s) to array at index 0&quot;&quot;&quot;</span>
        <span class="hljs-keyword">if</span> how == <span class="hljs-string">&apos;column&apos;</span>:
            X_new = np.ones((X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>] + <span class="hljs-number">1</span>))
            X_new[:, <span class="hljs-number">1</span>:] = X
        <span class="hljs-keyword">elif</span> how == <span class="hljs-string">&apos;row&apos;</span>:
            X_new = np.ones((X.shape[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">1</span>]))
            X_new[<span class="hljs-number">1</span>:, :] = X
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">raise</span> AttributeError(<span class="hljs-string">&apos;`how` must be `column` or `row`&apos;</span>)
        <span class="hljs-keyword">return</span> X_new

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_feedforward</span><span class="hljs-params">(self, X, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute feedforward step

        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
            Input layer with original features.
        w1 : array, shape = [n_hidden_units, n_features]
            Weight matrix for input layer -&gt; hidden layer.
        w2 : array, shape = [n_output_units, n_hidden_units]
            Weight matrix for hidden layer -&gt; output layer.

        Returns
        ----------
        a1 : array, shape = [n_samples, n_features+1]
            Input values with bias unit.
        z2 : array, shape = [n_hidden, n_samples]
            Net input of hidden layer.
        a2 : array, shape = [n_hidden+1, n_samples]
            Activation of hidden layer.
        z3 : array, shape = [n_output_units, n_samples]
            Net input of output layer.
        a3 : array, shape = [n_output_units, n_samples]
            Activation of output layer.

        &quot;&quot;&quot;</span>
        a1 = self._add_bias_unit(X, how=<span class="hljs-string">&apos;column&apos;</span>)
        z2 = w1.dot(a1.T)
        a2 = self._sigmoid(z2)
        a2 = self._add_bias_unit(a2, how=<span class="hljs-string">&apos;row&apos;</span>)
        z3 = w2.dot(a2)
        a3 = self._sigmoid(z3)
        <span class="hljs-keyword">return</span> a1, z2, a2, z3, a3

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_L2_reg</span><span class="hljs-params">(self, lambda_, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute L2-regularization cost&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> (lambda_/<span class="hljs-number">2.0</span>) * (np.sum(w1[:, <span class="hljs-number">1</span>:] ** <span class="hljs-number">2</span>) +
                                np.sum(w2[:, <span class="hljs-number">1</span>:] ** <span class="hljs-number">2</span>))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_L1_reg</span><span class="hljs-params">(self, lambda_, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute L1-regularization cost&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> (lambda_/<span class="hljs-number">2.0</span>) * (np.abs(w1[:, <span class="hljs-number">1</span>:]).sum() +
                                np.abs(w2[:, <span class="hljs-number">1</span>:]).sum())

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_cost</span><span class="hljs-params">(self, y_enc, output, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute cost function.

        Parameters
        ----------
        y_enc : array, shape = (n_labels, n_samples)
            one-hot encoded class labels.
        output : array, shape = [n_output_units, n_samples]
            Activation of the output layer (feedforward)
        w1 : array, shape = [n_hidden_units, n_features]
            Weight matrix for input layer -&gt; hidden layer.
        w2 : array, shape = [n_output_units, n_hidden_units]
            Weight matrix for hidden layer -&gt; output layer.

        Returns
        ---------
        cost : float
            Regularized cost.

        &quot;&quot;&quot;</span>
        term1 = -y_enc * (np.log(output))
        term2 = (<span class="hljs-number">1.0</span> - y_enc) * np.log(<span class="hljs-number">1.0</span> - output)
        cost = np.sum(term1 - term2)
        L1_term = self._L1_reg(self.l1, w1, w2)
        L2_term = self._L2_reg(self.l2, w1, w2)
        cost = cost + L1_term + L2_term
        <span class="hljs-keyword">return</span> cost

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_gradient</span><span class="hljs-params">(self, a1, a2, a3, z2, y_enc, w1, w2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Compute gradient step using backpropagation.

        Parameters
        ------------
        a1 : array, shape = [n_samples, n_features+1]
            Input values with bias unit.
        a2 : array, shape = [n_hidden+1, n_samples]
            Activation of hidden layer.
        a3 : array, shape = [n_output_units, n_samples]
            Activation of output layer.
        z2 : array, shape = [n_hidden, n_samples]
            Net input of hidden layer.
        y_enc : array, shape = (n_labels, n_samples)
            one-hot encoded class labels.
        w1 : array, shape = [n_hidden_units, n_features]
            Weight matrix for input layer -&gt; hidden layer.
        w2 : array, shape = [n_output_units, n_hidden_units]
            Weight matrix for hidden layer -&gt; output layer.

        Returns
        ---------
        grad1 : array, shape = [n_hidden_units, n_features]
            Gradient of the weight matrix w1.
        grad2 : array, shape = [n_output_units, n_hidden_units]
            Gradient of the weight matrix w2.

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># backpropagation</span>
        sigma3 = a3 - y_enc
        z2 = self._add_bias_unit(z2, how=<span class="hljs-string">&apos;row&apos;</span>)
        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)
        sigma2 = sigma2[<span class="hljs-number">1</span>:, :]
        grad1 = sigma2.dot(a1)
        grad2 = sigma3.dot(a2.T)

        <span class="hljs-comment"># regularize</span>
        grad1[:, <span class="hljs-number">1</span>:] += (w1[:, <span class="hljs-number">1</span>:] * (self.l1 + self.l2))
        grad2[:, <span class="hljs-number">1</span>:] += (w2[:, <span class="hljs-number">1</span>:] * (self.l1 + self.l2))

        <span class="hljs-keyword">return</span> grad1, grad2

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_gradient_checking</span><span class="hljs-params">(self, X, y_enc, w1, w2, epsilon, grad1, grad2)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Apply gradient checking (for debugging only)

        Returns
        ---------
        relative_error : float
          Relative error between the numerically
          approximated gradients and the backpropagated gradients.

        &quot;&quot;&quot;</span>
        num_grad1 = np.zeros(np.shape(w1))
        epsilon_ary1 = np.zeros(np.shape(w1))
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(w1.shape[<span class="hljs-number">0</span>]):
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(w1.shape[<span class="hljs-number">1</span>]):
                epsilon_ary1[i, j] = epsilon
                a1, z2, a2, z3, a3 = self._feedforward(X,
                                                       w1 - epsilon_ary1, w2)
                cost1 = self._get_cost(y_enc, a3, w1-epsilon_ary1, w2)
                a1, z2, a2, z3, a3 = self._feedforward(X,
                                                       w1 + epsilon_ary1, w2)
                cost2 = self._get_cost(y_enc, a3, w1 + epsilon_ary1, w2)
                num_grad1[i, j] = (cost2 - cost1) / (<span class="hljs-number">2.0</span> * epsilon)
                epsilon_ary1[i, j] = <span class="hljs-number">0</span>

        num_grad2 = np.zeros(np.shape(w2))
        epsilon_ary2 = np.zeros(np.shape(w2))
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(w2.shape[<span class="hljs-number">0</span>]):
            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(w2.shape[<span class="hljs-number">1</span>]):
                epsilon_ary2[i, j] = epsilon
                a1, z2, a2, z3, a3 = self._feedforward(X, w1,
                                                       w2 - epsilon_ary2)
                cost1 = self._get_cost(y_enc, a3, w1, w2 - epsilon_ary2)
                a1, z2, a2, z3, a3 = self._feedforward(X, w1,
                                                       w2 + epsilon_ary2)
                cost2 = self._get_cost(y_enc, a3, w1, w2 + epsilon_ary2)
                num_grad2[i, j] = (cost2 - cost1) / (<span class="hljs-number">2.0</span> * epsilon)
                epsilon_ary2[i, j] = <span class="hljs-number">0</span>

        num_grad = np.hstack((num_grad1.flatten(), num_grad2.flatten()))
        grad = np.hstack((grad1.flatten(), grad2.flatten()))
        norm1 = np.linalg.norm(num_grad - grad)
        norm2 = np.linalg.norm(num_grad)
        norm3 = np.linalg.norm(grad)
        relative_error = norm1 / (norm2 + norm3)
        <span class="hljs-keyword">return</span> relative_error

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Predict class labels

        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
            Input layer with original features.

        Returns:
        ----------
        y_pred : array, shape = [n_samples]
            Predicted class labels.

        &quot;&quot;&quot;</span>
        <span class="hljs-keyword">if</span> len(X.shape) != <span class="hljs-number">2</span>:
            <span class="hljs-keyword">raise</span> AttributeError(<span class="hljs-string">&apos;X must be a [n_samples, n_features] array.\n&apos;</span>
                                 <span class="hljs-string">&apos;Use X[:,None] for 1-feature classification,&apos;</span>
                                 <span class="hljs-string">&apos;\nor X[[i]] for 1-sample classification&apos;</span>)

        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)
        y_pred = np.argmax(z3, axis=<span class="hljs-number">0</span>)
        <span class="hljs-keyword">return</span> y_pred

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y, print_progress=False)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Learn weights from training data.

        Parameters
        -----------
        X : array, shape = [n_samples, n_features]
            Input layer with original features.
        y : array, shape = [n_samples]
            Target class labels.
        print_progress : bool (default: False)
            Prints progress as the number of epochs
            to stderr.

        Returns:
        ----------
        self

        &quot;&quot;&quot;</span>
        self.cost_ = []
        X_data, y_data = X.copy(), y.copy()
        y_enc = self._encode_labels(y, self.n_output)

        delta_w1_prev = np.zeros(self.w1.shape)
        delta_w2_prev = np.zeros(self.w2.shape)

        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.epochs):

            <span class="hljs-comment"># adaptive learning rate</span>
            self.eta /= (<span class="hljs-number">1</span> + self.decrease_const*i)

            <span class="hljs-keyword">if</span> print_progress:
                sys.stderr.write(<span class="hljs-string">&apos;\rEpoch: %d/%d&apos;</span> % (i+<span class="hljs-number">1</span>, self.epochs))
                sys.stderr.flush()

            <span class="hljs-keyword">if</span> self.shuffle:
                idx = np.random.permutation(y_data.shape[<span class="hljs-number">0</span>])
                X_data, y_enc = X_data[idx], y_enc[idx]

            mini = np.array_split(range(y_data.shape[<span class="hljs-number">0</span>]), self.minibatches)
            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> mini:

                <span class="hljs-comment"># feedforward</span>
                a1, z2, a2, z3, a3 = self._feedforward(X[idx],
                                                       self.w1,
                                                       self.w2)
                cost = self._get_cost(y_enc=y_enc[:, idx],
                                      output=a3,
                                      w1=self.w1,
                                      w2=self.w2)
                self.cost_.append(cost)

                <span class="hljs-comment"># compute gradient via backpropagation</span>
                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,
                                                  a3=a3, z2=z2,
                                                  y_enc=y_enc[:, idx],
                                                  w1=self.w1,
                                                  w2=self.w2)

                <span class="hljs-comment"># start gradient checking</span>
                grad_diff = self._gradient_checking(X=X_data[idx],
                                                    y_enc=y_enc[:, idx],
                                                    w1=self.w1,
                                                    w2=self.w2,
                                                    epsilon=<span class="hljs-number">1e-5</span>,
                                                    grad1=grad1,
                                                    grad2=grad2)


                <span class="hljs-keyword">if</span> grad_diff &lt;= <span class="hljs-number">1e-7</span>:
                    print(<span class="hljs-string">&apos;Ok: %s&apos;</span> % grad_diff)
                <span class="hljs-keyword">elif</span> grad_diff &lt;= <span class="hljs-number">1e-4</span>:
                    print(<span class="hljs-string">&apos;Warning: %s&apos;</span> % grad_diff)
                <span class="hljs-keyword">else</span>:
                    print(<span class="hljs-string">&apos;PROBLEM: %s&apos;</span> % grad_diff)

                <span class="hljs-comment"># update weights; [alpha * delta_w_prev] for momentum learning</span>
                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2
                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))
                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))
                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2

        <span class="hljs-keyword">return</span> self
</code></pre>
<pre><code class="lang-python">nn_check = MLPGradientCheck(n_output=<span class="hljs-number">10</span>,
                            n_features=X_train.shape[<span class="hljs-number">1</span>],
                            n_hidden=<span class="hljs-number">10</span>,
                            l2=<span class="hljs-number">0.0</span>,
                            l1=<span class="hljs-number">0.0</span>,
                            epochs=<span class="hljs-number">10</span>,
                            eta=<span class="hljs-number">0.001</span>,
                            alpha=<span class="hljs-number">0.0</span>,
                            decrease_const=<span class="hljs-number">0.0</span>,
                            minibatches=<span class="hljs-number">1</span>,
                            shuffle=<span class="hljs-keyword">False</span>,
                            random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python">nn_check.fit(X_train[:<span class="hljs-number">5</span>], y_train[:<span class="hljs-number">5</span>], print_progress=<span class="hljs-keyword">False</span>)
</code></pre>
<pre><code>Ok: 2.59699590792e-10
Ok: 2.9553528175e-10
Ok: 2.38060754028e-10
Ok: 3.07760791451e-10
Ok: 3.38742154283e-10
Ok: 3.57890531092e-10
Ok: 2.17697902147e-10
Ok: 2.36171066791e-10
Ok: 3.42158139292e-10
Ok: 2.10657747496e-10





&lt;__main__.MLPGradientCheck at 0x1288634d0&gt;
</code></pre><p><br>
<br></p>
<h1 id="other-neural-network-architectures"><a name="other-neural-network-architectures" class="plugin-anchor" href="#other-neural-network-architectures"><i class="fa fa-link" aria-hidden="true"></i></a>Other neural network architectures</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="convolutional-neural-networks"><a name="convolutional-neural-networks" class="plugin-anchor" href="#convolutional-neural-networks"><i class="fa fa-link" aria-hidden="true"></i></a>Convolutional Neural Networks</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/convolutional.png" style="width:400px"></p>
<p><img src="../figures/cnn.png" style="width:700px"></p>
<p><br>
<br></p>
<h2 id="recurrent-neural-networks"><a name="recurrent-neural-networks" class="plugin-anchor" href="#recurrent-neural-networks"><i class="fa fa-link" aria-hidden="true"></i></a>Recurrent Neural Networks</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/rnn.png" style="width:400px"></p>
<script type="text/javascript">var className='atoc';</script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../w7-text-mining/7w.html" class="navigation navigation-prev " aria-label="Previous page: 文本挖掘">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../w9-deep-learning/9w.html" class="navigation navigation-next " aria-label="Next page: 深度学习">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"神经网络","level":"1.10","depth":1,"next":{"title":"深度学习","level":"1.11","depth":1,"path":"w9-deep-learning/9w.md","ref":"w9-deep-learning/9w.md","articles":[]},"previous":{"title":"文本挖掘","level":"1.9","depth":1,"path":"w7-text-mining/7w.md","ref":"w7-text-mining/7w.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","anchors","github","splitter","sharing","atoc","comment"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/lyltj2010/DataMining"},"atoc":{"addClass":true,"className":"atoc"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":true,"instapaper":false,"twitter":true,"vk":false,"weibo":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"comment":{"highlightCommented":true}},"theme":"default","author":"wizardforcel","pdf":{"pageNumbers":true,"fontSize":16,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"数据挖掘开源书","language":"zh","links":{"sidebar":{"数据挖掘开源书":"https://www.gitbook.com/book/wizardforcel/data-mining-book"},"gitbook":true},"gitbook":"*","description":"数据挖掘开源书"},"file":{"path":"w8-neural-network/8w.md","mtime":"2016-12-24T21:25:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-04-29T11:04:29.751Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-atoc/atoc.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-comment/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

