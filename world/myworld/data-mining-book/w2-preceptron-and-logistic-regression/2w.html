
<!DOCTYPE HTML>
<html lang="zh" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>感知机和逻辑回归 · 数据挖掘开源书</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        <meta name="author" content="wizardforcel">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-atoc/atoc.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-comment/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../w3-decision-tree-and-ensemble-learning/3w.html" />
    
    
    <link rel="prev" href="../w1-regression/1w.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="輸入並搜尋" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://www.gitbook.com/book/wizardforcel/data-mining-book" target="_blank" class="custom-link">数据挖掘开源书</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../w0-introduction/0w.html">
            
                <a href="../w0-introduction/0w.html">
            
                    
                    数据挖掘导论和信贷模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../w1-regression/1w.html">
            
                <a href="../w1-regression/1w.html">
            
                    
                    回归模型和房价预测
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4" data-path="2w.html">
            
                <a href="2w.html">
            
                    
                    感知机和逻辑回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                    
                    决策树和集成学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../w4-feature-engineering/4w.html">
            
                <a href="../w4-feature-engineering/4w.html">
            
                    
                    特征工程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../w5-tuning-parameter/5w.html">
            
                <a href="../w5-tuning-parameter/5w.html">
            
                    
                    参数调优
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../w6-unsupervised-learning/6w.html">
            
                <a href="../w6-unsupervised-learning/6w.html">
            
                    
                    无监督学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../w7-text-mining/7w.html">
            
                <a href="../w7-text-mining/7w.html">
            
                    
                    文本挖掘
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../w8-neural-network/8w.html">
            
                <a href="../w8-neural-network/8w.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../w9-deep-learning/9w.html">
            
                <a href="../w9-deep-learning/9w.html">
            
                    
                    深度学习
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本書使用 GitBook 釋出
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >感知机和逻辑回归</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="sections"><a name="sections" class="plugin-anchor" href="#sections"><i class="fa fa-link" aria-hidden="true"></i></a>Sections</h2>
<ul>
<li><a href="#implementing-a-perceptron-learning-algorithm-in-python">Implementing a perceptron learning algorithm in Python</a><ul>
<li><a href="#training-a-perceptron-model-on-the-iris-dataset">Training a perceptron model on the Iris dataset</a></li>
</ul>
</li>
<li><a href="#adaptive-linear-neurons-and-the-convergence-of-learning-adaline">Adaptive linear neurons and the convergence of learning</a><ul>
<li><a href="#implementing-an-adaptive-linear-neuron-in-python">Implementing an adaptive linear neuron in Python</a></li>
</ul>
</li>
<li><a href="#implementing-logistic-regression-in-python">Implementing logistic regression in Python</a></li>
<li><a href="#classification-with-scikit-learn">Classification with scikit-learn</a><ul>
<li><a href="#loading-and-preprocessing-the-data">Loading and preprocessing the data</a></li>
<li><a href="#other-available-data">Other Available Data</a></li>
<li><a href="#training-a-perceptron-via-scikit-learn">Training a perceptron via scikit-learn</a></li>
<li><a href="#modeling-class-probabilities-via-logistic-regression">Modeling class probabilities via logistic regression</a></li>
<li><a href="#maximum-margin-classification-with-support-vector-machines">Maximum margin classification with support vector machines</a></li>
<li><a href="#solving-non-linear-problems-using-a-kernel-svm">Solving non-linear problems using a kernel SVM</a></li>
<li><a href="#k-nearest-neighbors---a-lazy-learning-algorithm">K-nearest neighbors - a lazy learning algorithm</a></li>
</ul>
</li>
<li><a href="#scoring-metrics-for-classification">Scoring metrics for classification</a><ul>
<li><a href="#classification-metrics-in-scikit-learn">Classification metrics in Scikit-learn</a></li>
<li><a href="#reading-a-confusion-matrix">Reading a confusion matrix</a></li>
<li><a href="#precision-recall-and-f-measures">Precision, recall and F-measures</a></li>
<li><a href="#roc-and-auc">ROC and AUC</a></li>
<li><a href="#hinge-loss">Hinge loss</a></li>
<li><a href="#log-loss">Log loss</a></li>
</ul>
</li>
</ul>
<p><br>
<br></p>
<h2 id="&#x4EC0;&#x4E48;&#x662F;&#x611F;&#x77E5;&#x673A;&#x5206;&#x7C7B;"><a name="&#x4EC0;&#x4E48;&#x662F;&#x611F;&#x77E5;&#x673A;&#x5206;&#x7C7B;" class="plugin-anchor" href="#&#x4EC0;&#x4E48;&#x662F;&#x611F;&#x77E5;&#x673A;&#x5206;&#x7C7B;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x4EC0;&#x4E48;&#x662F;&#x611F;&#x77E5;&#x673A;&#x5206;&#x7C7B;</h2>
<p>&#x6700;&#x7B80;&#x5355;&#x5F62;&#x5F0F;&#x7684;&#x524D;&#x9988;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#xFF0C;&#x662F;&#x4E00;&#x79CD;&#x4E8C;&#x5143;&#x7EBF;&#x6027;&#x5206;&#x7C7B;&#x5668;, &#x628A;&#x77E9;&#x9635;&#x4E0A;&#x7684;&#x8F93;&#x5165; <script type="math/tex; ">\displaystyle x</script> &#xFF08;&#x5B9E;&#x6570;&#x503C;&#x5411;&#x91CF;&#xFF09;&#x6620;&#x5C04;&#x5230;&#x8F93;&#x51FA;&#x503C; <script type="math/tex; ">\displaystyle f(x)</script> &#x4E0A;&#xFF08;&#x4E00;&#x4E2A;&#x4E8C;&#x5143;&#x7684;&#x503C;&#xFF09;&#x3002;</p>
<p><script type="math/tex; ">\displaystyle f(x)={\begin{cases}+1&{\text{if }}w\cdot x+b>0\\-1&{\text{else}}\end{cases}}</script></p>
<h3 id="&#x5B66;&#x4E60;&#x7B97;&#x6CD5;"><a name="&#x5B66;&#x4E60;&#x7B97;&#x6CD5;" class="plugin-anchor" href="#&#x5B66;&#x4E60;&#x7B97;&#x6CD5;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x5B66;&#x4E60;&#x7B97;&#x6CD5;</h3>
<p>&#x6211;&#x4EEC;&#x9996;&#x5148;&#x5B9A;&#x4E49;&#x4E00;&#x4E9B;&#x53D8;&#x91CF;&#xFF1A;</p>
<ul>
<li><script type="math/tex; ">\displaystyle x(j)</script> &#x8868;&#x793A;n&#x7EF4;&#x8F93;&#x5165;&#x5411;&#x91CF;&#x4E2D;&#x7684;&#x7B2C;j&#x9879;</li>
<li><script type="math/tex; ">\displaystyle w(j)</script> &#x8868;&#x793A;&#x6743;&#x91CD;&#x5411;&#x91CF;&#x7684;&#x7B2C;j&#x9879;</li>
<li><script type="math/tex; ">\displaystyle f(x)</script> &#x8868;&#x793A;&#x795E;&#x7ECF;&#x5143;&#x63A5;&#x53D7;&#x8F93;&#x5165; <script type="math/tex; ">\displaystyle x</script> &#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;</li>
<li><script type="math/tex; ">\displaystyle \alpha </script> &#x662F;&#x4E00;&#x4E2A;&#x5E38;&#x6570;&#xFF0C;&#x7B26;&#x5408; <script type="math/tex; ">\displaystyle 0<\alpha \leq 1</script> &#xFF08;&#x63A5;&#x53D7;&#x7387;&#xFF09;</li>
<li>&#x66F4;&#x8FDB;&#x4E00;&#x6B65;&#xFF0C;&#x4E3A;&#x4E86;&#x7B80;&#x4FBF;&#x6211;&#x4EEC;&#x5047;&#x5B9A;&#x504F;&#x7F6E;&#x91CF; <script type="math/tex; ">\displaystyle b</script> &#x7B49;&#x4E8E;0&#x3002;&#x56E0;&#x4E3A;&#x4E00;&#x4E2A;&#x989D;&#x5916;&#x7684;&#x7EF4;&#x5EA6; <script type="math/tex; ">\displaystyle n+1</script> &#x7EF4;&#xFF0C;&#x53EF;&#x4EE5;&#x7528; <script type="math/tex; ">\displaystyle x(n+1)=1</script> &#x7684;&#x5F62;&#x5F0F;&#x52A0;&#x5230;&#x8F93;&#x5165;&#x5411;&#x91CF;&#xFF0C;&#x8FD9;&#x6837;&#x6211;&#x4EEC;&#x5C31;&#x53EF;&#x4EE5;&#x7528; <script type="math/tex; ">\displaystyle w(n+1)</script> &#x4EE3;&#x66FF;&#x504F;&#x7F6E;&#x91CF;&#x3002;</li>
</ul>
<p>&#x611F;&#x77E5;&#x5668;&#x7684;&#x5B66;&#x4E60;&#x901A;&#x8FC7;&#x5BF9;<strong>&#x6240;&#x6709;&#x8BAD;&#x7EC3;&#x5B9E;&#x4F8B;</strong>&#x8FDB;&#x884C;<strong>&#x591A;&#x6B21;&#x7684;&#x8FED;&#x4EE3;&#x8FDB;&#x884C;&#x66F4;&#x65B0;</strong>&#x7684;&#x65B9;&#x5F0F;&#x6765;&#x5EFA;&#x6A21;&#x3002;</p>
<p>&#x4EE4; <script type="math/tex; ">\displaystyle D_{m}=\{(x_{1},y_{1}),\dots ,(x_{m},y_{m})\}</script> &#x8868;&#x793A;&#x4E00;&#x4E2A;&#x6709; <script type="math/tex; ">\displaystyle m</script> &#x4E2A;&#x8BAD;&#x7EC3;&#x5B9E;&#x4F8B;&#x7684;&#x8BAD;&#x7EC3;&#x96C6;&#x3002;</p>
<p>&#x6BCF;&#x6B21;&#x8FED;&#x4EE3;&#x6743;&#x91CD;&#x5411;&#x91CF;&#x4EE5;&#x5982;&#x4E0B;&#x65B9;&#x5F0F;&#x66F4;&#x65B0;&#xFF1A;
&#x5BF9;&#x4E8E;&#x6BCF;&#x4E2A; <script type="math/tex; ">\displaystyle D_{m}=\{(x_{1},y_{1}),\dots ,(x_{m},y_{m})\}</script> &#x4E2D;&#x7684;&#x6BCF;&#x4E2A; <script type="math/tex; ">\displaystyle (x,y)</script> &#x5BF9;&#xFF0C;
<script type="math/tex; ">\displaystyle w(j):=w(j)+{\alpha (y-f(x))}{x(j)}\quad (j=1,\ldots ,n)</script></p>
<p>&#x6CE8;&#x610F;&#x8FD9;&#x610F;&#x5473;&#x7740;&#xFF0C;&#x4EC5;&#x5F53;&#x9488;&#x5BF9;&#x7ED9;&#x5B9A;&#x8BAD;&#x7EC3;&#x5B9E;&#x4F8B; <script type="math/tex; ">\displaystyle (x,y)</script> &#x4EA7;&#x751F;&#x7684;&#x8F93;&#x51FA;&#x503C; <script type="math/tex; ">\displaystyle f(x)</script> &#x4E0E;&#x9884;&#x671F;&#x7684;&#x8F93;&#x51FA;&#x503C; <script type="math/tex; ">\displaystyle y</script> &#x4E0D;&#x540C;&#x65F6;&#xFF0C;&#x6743;&#x91CD;&#x5411;&#x91CF;&#x624D;&#x4F1A;&#x53D1;&#x751F;&#x6539;&#x53D8;&#x3002;</p>
<p>&#x5982;&#x679C;&#x5B58;&#x5728;&#x4E00;&#x4E2A;&#x6B63;&#x7684;&#x5E38;&#x6570; <script type="math/tex; ">\displaystyle \gamma </script> &#x548C;&#x6743;&#x91CD;&#x5411;&#x91CF; <script type="math/tex; ">\displaystyle w</script> &#xFF0C;&#x5BF9;&#x6240;&#x6709;&#x7684; <script type="math/tex; ">\displaystyle i</script> &#x6EE1;&#x8DB3; <script type="math/tex; ">\displaystyle y_{i}\cdot \left(\langle w,x_{i}\rangle +b\right)>\gamma </script> &#xFF0C;&#x8BAD;&#x7EC3;&#x96C6; <script type="math/tex; ">\displaystyle D_{m}</script> &#x5C31;&#x88AB;&#x53EB;&#x505A;&#x7EBF;&#x6027;&#x5206;&#x9694;&#x3002;
&#x7136;&#x800C;&#xFF0C;&#x5982;&#x679C;&#x8BAD;&#x7EC3;&#x96C6;&#x4E0D;&#x662F;&#x7EBF;&#x6027;&#x5206;&#x9694;&#x7684;&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E2A;&#x7B97;&#x6CD5;&#x5219;&#x4E0D;&#x80FD;&#x786E;&#x4FDD;&#x4F1A;&#x6536;&#x655B;&#x3002;</p>
<h1 id="implementing-a-perceptron-learning-algorithm-in-python"><a name="implementing-a-perceptron-learning-algorithm-in-python" class="plugin-anchor" href="#implementing-a-perceptron-learning-algorithm-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implementing a perceptron learning algorithm in Python</h1>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Perceptron</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Perceptron classifier.

    Parameters
    ------------
    eta : float
        Learning rate (between 0.0 and 1.0)
    n_iter : int
        Passes over the training dataset.

    Attributes
    -----------
    w_ : 1d-array
        Weights after fitting.
    errors_ : list
        Number of misclassifications in every epoch.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, eta=<span class="hljs-number">0.01</span>, n_iter=<span class="hljs-number">10</span>)</span>:</span>
        self.eta = eta
        self.n_iter = n_iter  <span class="hljs-comment"># the number of epochs</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y : array-like, shape = [n_samples]
            Target values.

        Returns
        -------
        self : object

        &quot;&quot;&quot;</span>
        self.w_ = np.zeros(<span class="hljs-number">1</span> + X.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># weights, &#x521D;&#x59CB;&#x503C;0</span>
        self.errors_ = []

        <span class="hljs-comment"># &#x5BF9;&#x6BCF;&#x4E2A; sample &#x5FAA;&#x73AF;&#x66F4;&#x65B0;</span>
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(self.n_iter):
            errors = <span class="hljs-number">0</span>
            <span class="hljs-keyword">for</span> xi, target <span class="hljs-keyword">in</span> zip(X, y):
                update = self.eta * (target - self.predict(xi))  <span class="hljs-comment">#learning rate*error</span>
                self.w_[<span class="hljs-number">1</span>:] += update * xi
                self.w_[<span class="hljs-number">0</span>] += update
                errors += int(update != <span class="hljs-number">0.0</span>)
            self.errors_.append(errors)  <span class="hljs-comment"># &#x9519;&#x8BEF;&#x7684;&#x5206;&#x7C7B;&#x7ED3;&#x679C;</span>
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net_input</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Calculate net input w*x&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.dot(X, self.w_[<span class="hljs-number">1</span>:]) + self.w_[<span class="hljs-number">0</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.where(self.net_input(X) &gt;= <span class="hljs-number">0.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
</code></pre>
<p><br>
<br></p>
<h2 id="training-a-perceptron-model-on-the-iris-dataset"><a name="training-a-perceptron-model-on-the-iris-dataset" class="plugin-anchor" href="#training-a-perceptron-model-on-the-iris-dataset"><i class="fa fa-link" aria-hidden="true"></i></a>Training a perceptron model on the Iris dataset</h2>
<p>&#x8FD9;&#x91CC;&#x53EA;&#x8003;&#x8651;&#x4E24;&#x79CD;&#x82B1; Setosa &#x548C; Versicolor , &#x4EE5;&#x53CA;&#x4E24;&#x79CD;&#x7279;&#x5F81; sepal length &#x548C; petal length.</p>
<p>&#x4F46;&#x662F; Perceptron Model &#x53EF;&#x4EE5;&#x89E3;&#x51B3;&#x591A;&#x7C7B;&#x522B;&#x5206;&#x7C7B;&#x95EE;&#x9898;, &#x53C2;&#x8003; <a href="https://en.wikipedia.org/wiki/Multiclass_classification" target="_blank">one-vs-all</a></p>
<p>[<a href="#sections">back to top</a>]</p>
<h3 id="reading-in-the-iris-data"><a name="reading-in-the-iris-data" class="plugin-anchor" href="#reading-in-the-iris-data"><i class="fa fa-link" aria-hidden="true"></i></a>Reading-in the Iris data</h3>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

df = pd.read_csv(<span class="hljs-string">&apos;data/iris.csv&apos;</span>, header=<span class="hljs-keyword">None</span>)
df.tail()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>146</th>
      <td>6.7</td>
      <td>3</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5</td>
      <td>1.9</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.5</td>
      <td>3</td>
      <td>5.2</td>
      <td>2</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>virginica</td>
    </tr>
    <tr>
      <th>150</th>
      <td>5.9</td>
      <td>3</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>virginica</td>
    </tr>
  </tbody>
</table>
</div>



<p><br>
<br></p>
<h3 id="plotting-the-iris-data"><a name="plotting-the-iris-data" class="plugin-anchor" href="#plotting-the-iris-data"><i class="fa fa-link" aria-hidden="true"></i></a>Plotting the Iris data</h3>
<pre><code class="lang-python"><span class="hljs-comment"># &#x5C06;&#x4E24;&#x4E2A;&#x5206;&#x7C7B;&#x5148;&#x53EF;&#x89C6;&#x5316;</span>
%matplotlib inline
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-comment"># select setosa and versicolor</span>
<span class="hljs-comment"># &#x4E24;&#x79CD;&#x5404;&#x9009;&#x62E9;50&#x4E2A;, &#x628A;&#x7C7B;&#x522B;&#x6539;&#x4E3A; -1 &#x548C; 1, &#x65B9;&#x4FBF;&#x753B;&#x56FE;</span>
y = df.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">100</span>, <span class="hljs-number">4</span>].values
y = np.where(y == <span class="hljs-string">&apos;Iris-setosa&apos;</span>, <span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)

<span class="hljs-comment"># extract sepal length and petal length</span>
X = df.iloc[<span class="hljs-number">0</span>:<span class="hljs-number">100</span>, [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]].values

<span class="hljs-comment"># plot data</span>
plt.scatter(X[:<span class="hljs-number">50</span>, <span class="hljs-number">0</span>], X[:<span class="hljs-number">50</span>, <span class="hljs-number">1</span>],
            color=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, label=<span class="hljs-string">&apos;setosa&apos;</span>)
plt.scatter(X[<span class="hljs-number">50</span>:<span class="hljs-number">100</span>, <span class="hljs-number">0</span>], X[<span class="hljs-number">50</span>:<span class="hljs-number">100</span>, <span class="hljs-number">1</span>],
            color=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;x&apos;</span>, label=<span class="hljs-string">&apos;versicolor&apos;</span>)

plt.xlabel(<span class="hljs-string">&apos;petal length [cm]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;sepal length [cm]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./iris_1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_13_0.png" alt="png"></p>
<p><br>
<br></p>
<h3 id="training-the-perceptron-model"><a name="training-the-perceptron-model" class="plugin-anchor" href="#training-the-perceptron-model"><i class="fa fa-link" aria-hidden="true"></i></a>Training the perceptron model</h3>
<pre><code class="lang-python">ppn = Perceptron(eta=<span class="hljs-number">0.1</span>, n_iter=<span class="hljs-number">10</span>)
ppn.fit(X, y)
ppn.errors_
</code></pre>
<pre><code>[2, 2, 3, 2, 1, 0, 0, 0, 0, 0]
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># error &#x753B;&#x56FE;, &#x68C0;&#x67E5;&#x662F;&#x5426; error &#x8D8B;&#x8FD1;&#x4E8E;0 &#x5728;&#x591A;&#x6B21; loop &#x66F4;&#x65B0;&#x540E;</span>
plt.plot(range(<span class="hljs-number">1</span>, len(ppn.errors_) + <span class="hljs-number">1</span>), ppn.errors_, marker=<span class="hljs-string">&apos;o&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Number of misclassifications&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./perceptron_1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_17_0.png" alt="png"></p>
<p>&#x7ED3;&#x679C; error &#x7684;&#x786E;&#x6700;&#x540E;&#x4E3A; 0, &#x8BC1;&#x660E;&#x662F; convergent &#x7684;, &#x4E14;&#x5206;&#x7C7B;&#x6548;&#x679C;&#x5E94;&#x8BE5;&#x8BF4;&#x662F;&#x975E;&#x5E38;&#x51C6;&#x786E;&#x4E86;</p>
<p><br>
<br></p>
<h3 id="a-function-for-plotting-decision-regions"><a name="a-function-for-plotting-decision-regions" class="plugin-anchor" href="#a-function-for-plotting-decision-regions"><i class="fa fa-link" aria-hidden="true"></i></a>A function for plotting decision regions</h3>
<p>&#x8FD9;&#x4E2A;&#x51FD;&#x6570;&#x7528;&#x4E8E;&#x753B;&#x51B3;&#x7B56;&#x8FB9;&#x754C;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap
<span class="hljs-comment"># Colormap object generated from a list of colors.</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_decision_regions</span><span class="hljs-params">(X, y, classifier, resolution=<span class="hljs-number">0.02</span>)</span>:</span>

    <span class="hljs-comment"># setup marker generator and color map</span>
    markers = (<span class="hljs-string">&apos;s&apos;</span>, <span class="hljs-string">&apos;x&apos;</span>, <span class="hljs-string">&apos;o&apos;</span>, <span class="hljs-string">&apos;^&apos;</span>, <span class="hljs-string">&apos;v&apos;</span>)
    colors = (<span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;lightgreen&apos;</span>, <span class="hljs-string">&apos;gray&apos;</span>, <span class="hljs-string">&apos;cyan&apos;</span>)
    cmap = ListedColormap(colors[:len(np.unique(y))])

    <span class="hljs-comment"># plot the decision surface &#x786E;&#x5B9A;&#x6A2A;&#x7EB5;&#x8F74;&#x8FB9;&#x754C;</span>
    x1_min, x1_max = X[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>  <span class="hljs-comment"># &#x6700;&#x5C0F;-1, &#x6700;&#x5927;+1</span>
    x2_min, x2_max = X[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>

    <span class="hljs-comment"># create a pair of grid arrays</span>
    <span class="hljs-comment"># flatten the grid arrays then predict</span>
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                         np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)

    <span class="hljs-comment"># maps the different decision regions to different colors</span>
    plt.contourf(xx1, xx2, Z, alpha=<span class="hljs-number">0.4</span>, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    <span class="hljs-comment"># plot class samples</span>
    <span class="hljs-keyword">for</span> idx, cl <span class="hljs-keyword">in</span> enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, <span class="hljs-number">0</span>], y=X[y == cl, <span class="hljs-number">1</span>],
                    alpha=<span class="hljs-number">0.8</span>, c=cmap(idx),
                    marker=markers[idx], label=cl)
</code></pre>
<pre><code class="lang-python">plot_decision_regions(X, y, classifier=ppn)
plt.xlabel(<span class="hljs-string">&apos;sepal length [cm]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal length [cm]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./perceptron_2.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_22_0.png" alt="png"></p>
<p>&#x867D;&#x7136; Perceptron Model &#x5728;&#x4E0A;&#x9762; Iris &#x4F8B;&#x5B50;&#x91CC;&#x8868;&#x73B0;&#x5F97;&#x5F88;&#x597D;&#xFF0C;&#x4F46;&#x5728;&#x5176;&#x4ED6;&#x95EE;&#x9898;&#x4E0A;&#x5374;&#x4E0D;&#x4E00;&#x5B9A;&#x8868;&#x73B0;&#x5F97;&#x597D;&#x3002;
Frank Rosenblatt &#x4ECE;&#x6570;&#x5B66;&#x4E0A;&#x8BC1;&#x660E;&#x4E86;&#xFF0C;&#x5728;&#x7EBF;&#x6027;&#x53EF;&#x5206;&#x7684;&#x6570;&#x636E;&#x91CC;&#xFF0C;Perceptron &#x7684;&#x5B66;&#x4E60;&#x89C4;&#x5219;&#x4F1A; converge&#xFF0C;&#x4F46;&#x5728;&#x7EBF;&#x6027;&#x4E0D;&#x53EF;&#x5206;&#x7684;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5374;&#x65E0;&#x6CD5; converge</p>
<p><br>
<br></p>
<h1 id="adaptive-linear-neurons-and-the-convergence-of-learning-adaline"><a name="adaptive-linear-neurons-and-the-convergence-of-learning-adaline" class="plugin-anchor" href="#adaptive-linear-neurons-and-the-convergence-of-learning-adaline"><i class="fa fa-link" aria-hidden="true"></i></a>Adaptive linear neurons and the convergence of learning (Adaline)</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="implementing-an-adaptive-linear-neuron-in-python"><a name="implementing-an-adaptive-linear-neuron-in-python" class="plugin-anchor" href="#implementing-an-adaptive-linear-neuron-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implementing an adaptive linear neuron in Python</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>ADAptive LInear NEuron classifier &#x4E5F;&#x662F;&#x4E00;&#x4E2A;&#x5355;&#x5C42;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;. &#x5B83;&#x7684;&#x91CD;&#x70B9;&#x5C31;&#x662F;&#x5B9A;&#x4E49;&#x53CA;&#x6700;&#x4F18;&#x5316; cost function, &#x5BF9;&#x4E8E;&#x7406;&#x89E3;&#x66F4;&#x9AD8;&#x5C42;&#x6B21;&#x66F4;&#x96BE;&#x7684;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5206;&#x7C7B;&#x6A21;&#x578B;&#x662F;&#x975E;&#x5E38;&#x597D;&#x7684;&#x5165;&#x95E8;.</p>
<p>&#x5B83;&#x4E0E; Perceptron &#x4E0D;&#x540C;&#x7684;&#x5730;&#x65B9;&#x5728;&#x4E8E;&#x66F4;&#x65B0; weights &#x65F6;&#x662F;&#x7528;&#x7684; linear activation function, &#x800C;&#x4E0D;&#x662F;unit step function.</p>
<p>Adaline &#x4E2D;&#x8FD9;&#x4E2A; linear activation function &#x8F93;&#x51FA;&#x7B49;&#x4E8E;&#x8F93;&#x5165;, <script type="math/tex; "> φ(w^T x)=w^T x</script>.</p>
<p>&#x7136;&#x540E; activation &#x540E;&#x4F1A;&#x6709;&#x4E00;&#x4E2A; quantizer &#x7528;&#x6765;&#x5B66;&#x4E60;&#x66F4;&#x65B0; weights</p>
<p>&#x5B9A;&#x4E49; cost function &#x4E3A; SSE: Sum of Squared Errors
<script type="math/tex; "> \displaystyle J(w)= \frac 1 2 \sum_i(y^{(i)} −\phi(z^{(i)}))^2</script></p>
<p>&#x8FD9;&#x4E2A; function &#x662F;&#x53EF;&#x5BFC;&#x7684;, &#x5E76;&#x4E14;&#x662F; convex &#x7684;, &#x53EF;&#x4EE5;&#x8FDB;&#x884C;&#x6700;&#x4F18;&#x5316;, &#x4F7F;&#x7528; gradient descent &#x7B97;&#x6CD5;.</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AdalineGD</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;ADAptive LInear NEuron classifier.

    Parameters
    ------------
    eta : float
        Learning rate (between 0.0 and 1.0)
    n_iter : int
        Passes over the training dataset.

    Attributes
    -----------
    w_ : 1d-array
        Weights after fitting.
    errors_ : list
        Number of misclassifications in every epoch.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, eta=<span class="hljs-number">0.01</span>, n_iter=<span class="hljs-number">50</span>)</span>:</span>
        self.eta = eta
        self.n_iter = n_iter

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y : array-like, shape = [n_samples]
            Target values.

        Returns
        -------
        self : object

        &quot;&quot;&quot;</span>
        self.w_ = np.zeros(<span class="hljs-number">1</span> + X.shape[<span class="hljs-number">1</span>])
        self.cost_ = []

        <span class="hljs-comment"># gradient descent</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.n_iter):
            output = self.net_input(X)
            errors = (y - output)
            self.w_[<span class="hljs-number">1</span>:] += self.eta * X.T.dot(errors)
            self.w_[<span class="hljs-number">0</span>] += self.eta * errors.sum()
            cost = (errors**<span class="hljs-number">2</span>).sum() / <span class="hljs-number">2.0</span>
            self.cost_.append(cost) <span class="hljs-comment"># cost list, to check algorithm convergence</span>
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net_input</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.dot(X, self.w_[<span class="hljs-number">1</span>:]) + self.w_[<span class="hljs-number">0</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activation</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> self.net_input(X)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.where(self.activation(X) &gt;= <span class="hljs-number">0.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x6D4B;&#x8BD5;&#x4E24;&#x79CD; learning rate, 0.01 &#x548C; 0.0001</span>
fig, ax = plt.subplots(nrows=<span class="hljs-number">1</span>, ncols=<span class="hljs-number">2</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>))

ada1 = AdalineGD(n_iter=<span class="hljs-number">10</span>, eta=<span class="hljs-number">0.01</span>).fit(X, y)
ax[<span class="hljs-number">0</span>].plot(range(<span class="hljs-number">1</span>, len(ada1.cost_) + <span class="hljs-number">1</span>), np.log10(ada1.cost_), marker=<span class="hljs-string">&apos;o&apos;</span>)
ax[<span class="hljs-number">0</span>].set_xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
ax[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;log(Sum-squared-error)&apos;</span>)
ax[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">&apos;Adaline - Learning rate 0.01&apos;</span>)

ada2 = AdalineGD(n_iter=<span class="hljs-number">10</span>, eta=<span class="hljs-number">0.0001</span>).fit(X, y)
ax[<span class="hljs-number">1</span>].plot(range(<span class="hljs-number">1</span>, len(ada2.cost_) + <span class="hljs-number">1</span>), ada2.cost_, marker=<span class="hljs-string">&apos;o&apos;</span>)
ax[<span class="hljs-number">1</span>].set_xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
ax[<span class="hljs-number">1</span>].set_ylabel(<span class="hljs-string">&apos;Sum-squared-error&apos;</span>)
ax[<span class="hljs-number">1</span>].set_title(<span class="hljs-string">&apos;Adaline - Learning rate 0.0001&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./adaline_1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_32_0.png" alt="png"></p>
<p>&#x5DE6;&#x56FE;&#x663E;&#x793A; learning rate &#x592A;&#x5927;, error &#x6CA1;&#x6709;&#x53D8;&#x5C0F;, &#x53CD;&#x800C;&#x53D8;&#x5927;&#x4E86;.</p>
<p>&#x53F3;&#x56FE;&#x663E;&#x793A; learning rate &#x592A;&#x5C0F;, error &#x53D8;&#x5316;&#x901F;&#x5EA6;&#x592A;&#x5C0F;</p>
<p><br>
<br></p>
<h4 id="standardizing-features-and-re-training-adaline"><a name="standardizing-features-and-re-training-adaline" class="plugin-anchor" href="#standardizing-features-and-re-training-adaline"><i class="fa fa-link" aria-hidden="true"></i></a>Standardizing features and re-training adaline</h4>
<p><script type="math/tex; "> x^\prime_j = \frac {x_j - \mu_j} {\sigma_j} </script></p>
<pre><code class="lang-python"><span class="hljs-comment"># standardize features</span>
X_std = np.copy(X)
X_std[:,<span class="hljs-number">0</span>] = (X[:,<span class="hljs-number">0</span>] - X[:,<span class="hljs-number">0</span>].mean()) / X[:,<span class="hljs-number">0</span>].std()
X_std[:,<span class="hljs-number">1</span>] = (X[:,<span class="hljs-number">1</span>] - X[:,<span class="hljs-number">1</span>].mean()) / X[:,<span class="hljs-number">1</span>].std()
</code></pre>
<pre><code class="lang-python">ada = AdalineGD(n_iter=<span class="hljs-number">15</span>, eta=<span class="hljs-number">0.01</span>)
ada.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada)
plt.title(<span class="hljs-string">&apos;Adaline - Gradient Descent&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;sepal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./adaline_2.png&apos;, dpi=300)</span>
plt.show()

plt.plot(range(<span class="hljs-number">1</span>, len(ada.cost_) + <span class="hljs-number">1</span>), ada.cost_, marker=<span class="hljs-string">&apos;o&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Sum-squared-error&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./adaline_3.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_37_0.png" alt="png"></p>
<p><img src="output_37_1.png" alt="png"></p>
<p>&#x5206;&#x7C7B;&#x6548;&#x679C;&#x4E0D;&#x9519;, error &#x6700;&#x7EC8;&#x63A5;&#x8FD1;&#x4E8E;0
&#x503C;&#x5F97;&#x6CE8;&#x610F;&#x7684;&#x662F;&#xFF0C;&#x867D;&#x7136;&#x6211;&#x4EEC;&#x7684;&#x5206;&#x7C7B;&#x5168;&#x90E8;&#x6B63;&#x786E;&#xFF0C;&#x4F46; error &#x4E5F;&#x4E0D;&#x7B49;&#x4E8E;0</p>
<pre><code class="lang-python">ada.w_ <span class="hljs-comment">#weights</span>
</code></pre>
<pre><code>array([  1.36557432e-16,  -1.26256159e-01,   1.10479201e+00])
</code></pre><p><br>
<br></p>
<h3 id="large-scale-machine-learning-and-stochastic-gradient-descent"><a name="large-scale-machine-learning-and-stochastic-gradient-descent" class="plugin-anchor" href="#large-scale-machine-learning-and-stochastic-gradient-descent"><i class="fa fa-link" aria-hidden="true"></i></a>Large scale machine learning and stochastic gradient descent</h3>
<p><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">Stochastic gradient descent &#x968F;&#x673A;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;</a> &#x6BD4;&#x4E00;&#x822C;&#x7684;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;&#x66F4;&#x6709;&#x4F18;&#x52BF;, &#x56E0;&#x4E3A;&#x6BCF;&#x4E00;&#x6B65;&#x8BA1;&#x7B97;&#x7684; cost &#x66F4;&#x5C0F;, &#x6BCF;&#x4E00;&#x6B65;&#x66F4;&#x65B0;&#x90FD;&#x662F;&#x968F;&#x673A;&#x53D6;&#x5176;&#x4E2D;&#x4E00;&#x5C0F;&#x6B65;&#x66F4;&#x65B0;&#x5C31;&#x53EF;.</p>
<p>batch gradient descent &#x4E00;&#x6B21;&#x66F4;&#x65B0;&#x9700;&#x8981;&#x8BA1;&#x7B97;&#x4E00;&#x904D;&#x6574;&#x4E2A;&#x6570;&#x636E;&#x96C6;
<script type="math/tex; "> \Delta w = \eta \sum_i (y^{(i)} - \phi(z^{(i)}))x^{(i)} </script></p>
<p>stochastic gradient descent &#x4E00;&#x6B21;&#x66F4;&#x65B0;&#x53EA;&#x9700;&#x8BA1;&#x7B97;&#x4E00;&#x4E2A;&#x6570;&#x636E;&#x70B9;
<script type="math/tex; "> \Delta w = \eta(y^{(i)} - \phi(z^{(i)}))x^{(i)} </script></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AdalineSGD</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;ADAptive LInear NEuron classifier.

    Parameters
    ------------
    eta : float
        Learning rate (between 0.0 and 1.0)
    n_iter : int
        Passes over the training dataset.

    Attributes
    -----------
    w_ : 1d-array
        Weights after fitting.
    errors_ : list
        Number of misclassifications in every epoch.
    shuffle : bool (default: True)
        Shuffles training data every epoch if True to prevent cycles.
    random_state : int (default: None)
        Set random state for shuffling and initializing the weights.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, eta=<span class="hljs-number">0.01</span>, n_iter=<span class="hljs-number">10</span>, shuffle=True, random_state=None)</span>:</span>
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = <span class="hljs-keyword">False</span>
        self.shuffle = shuffle
        <span class="hljs-keyword">if</span> random_state:  <span class="hljs-comment"># allow the specication of a random seed for consistency</span>
            np.random.seed(random_state)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y : array-like, shape = [n_samples]
            Target values.

        Returns
        -------
        self : object

        &quot;&quot;&quot;</span>
        self._initialize_weights(X.shape[<span class="hljs-number">1</span>])
        self.cost_ = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.n_iter):
            <span class="hljs-keyword">if</span> self.shuffle:
                X, y = self._shuffle(X, y)
            cost = []
            <span class="hljs-keyword">for</span> xi, target <span class="hljs-keyword">in</span> zip(X, y):
                cost.append(self._update_weights(xi, target))
            avg_cost = sum(cost) / len(y)
            self.cost_.append(avg_cost)
        <span class="hljs-keyword">return</span> self

    <span class="hljs-comment"># &#x5728;&#x6BCF;&#x4E2A; epoch &#x524D;&#x662F;&#x5426; shuffle data</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_shuffle</span><span class="hljs-params">(self, X, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Shuffle training data&quot;&quot;&quot;</span>
        r = np.random.permutation(len(y))
        <span class="hljs-keyword">return</span> X[r], y[r]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_initialize_weights</span><span class="hljs-params">(self, m)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Initialize weights to zeros&quot;&quot;&quot;</span>
        self.w_ = np.zeros(<span class="hljs-number">1</span> + m)
        self.w_initialized = <span class="hljs-keyword">True</span>

    <span class="hljs-comment">#stochastic gradient descent</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_update_weights</span><span class="hljs-params">(self, xi, target)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Apply Adaline learning rule to update the weights&quot;&quot;&quot;</span>
        output = self.net_input(xi)
        error = (target - output)
        self.w_[<span class="hljs-number">1</span>:] += self.eta * xi.dot(error) <span class="hljs-comment"># &#x4EC5;&#x4E00;&#x4E2A; error &#x76F8;&#x4E58;</span>
        self.w_[<span class="hljs-number">0</span>] += self.eta * error <span class="hljs-comment"># &#x4EC5;&#x4EC5;&#x662F;&#x4E00;&#x4E2A; error, &#x800C;&#x975E; sum</span>
        cost = <span class="hljs-number">0.5</span> * error**<span class="hljs-number">2</span>
        <span class="hljs-keyword">return</span> cost

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net_input</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.dot(X, self.w_[<span class="hljs-number">1</span>:]) + self.w_[<span class="hljs-number">0</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activation</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> self.net_input(X)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.where(self.activation(X) &gt;= <span class="hljs-number">0.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># plot result</span>
ada = AdalineSGD(n_iter=<span class="hljs-number">15</span>, eta=<span class="hljs-number">0.01</span>, random_state=<span class="hljs-number">1</span>)
ada.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada)
plt.title(<span class="hljs-string">&apos;Adaline - Stochastic Gradient Descent&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;sepal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)

plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./adaline_4.png&apos;, dpi=300)</span>
plt.show()

plt.plot(range(<span class="hljs-number">1</span>, len(ada.cost_) + <span class="hljs-number">1</span>), ada.cost_, marker=<span class="hljs-string">&apos;o&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Average Cost&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./adaline_5.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_43_0.png" alt="png"></p>
<p><img src="output_43_1.png" alt="png"></p>
<p>&#x5BF9;&#x6BD4;&#x53EF;&#x4EE5;&#x53D1;&#x73B0;&#xFF0C;Stochastic gradient descent &#x7684;&#x5B66;&#x4E60;&#x901F;&#x7387;&#x6BD4; Batch gradient descnent &#x7684;&#x9AD8;&#x5F88;&#x591A;</p>
<p><br>
<br></p>
<h1 id="implementing-logistic-regression-in-python"><a name="implementing-logistic-regression-in-python" class="plugin-anchor" href="#implementing-logistic-regression-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implementing logistic regression in Python</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>logistic regression:  powerful algorithm for linear and binary classication problems</p>
<p>odds ratio: the odds in favor of a particular event.<script type="math/tex; ">\displaystyle \frac{p}{(1-p)}</script>, if p stands for the probability of the positive event, we want to predict.<script type="math/tex; ">\displaystyle logit(p) = log\frac{p}{1-p}</script></p>
<p>and <script type="math/tex; ">\displaystyle p(y=1|x)</script> is conditional probability that a particular sample belongs to
class 1 given its features x. Therefore, the logit is <script type="math/tex; ">\displaystyle logit(p(y=1|x)) = w_0x_0 + w_1x_1 + ... + w_mx_m = \sum_{i=0}^nw_ix_i</script></p>
<p>we want to know the probability, which is inverse of logit function, we call it logistic funcition, or <strong>sigmoid function</strong>.
<script type="math/tex; ">\displaystyle \phi (z) = \frac{1}{1+e^{-z}}</script></p>
<p>output of sigmoid function is as the probability of particular sample belonging to class 1</p>
<h3 id="plot-sigmoid-function"><a name="plot-sigmoid-function" class="plugin-anchor" href="#plot-sigmoid-function"><i class="fa fa-link" aria-hidden="true"></i></a>Plot sigmoid function:</h3>
<pre><code class="lang-python">%matplotlib inline
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + np.exp(-z))

z = np.arange(<span class="hljs-number">-7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0.1</span>)
phi_z = sigmoid(z)

plt.plot(z, phi_z)
plt.axvline(<span class="hljs-number">0.0</span>, color=<span class="hljs-string">&apos;k&apos;</span>)
plt.ylim(<span class="hljs-number">-0.1</span>, <span class="hljs-number">1.1</span>)
plt.xlabel(<span class="hljs-string">&apos;z&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;$\phi (z)$&apos;</span>)

<span class="hljs-comment"># y axis ticks and gridline</span>
plt.yticks([<span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">1.0</span>])
ax = plt.gca()
ax.yaxis.grid(<span class="hljs-keyword">True</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/sigmoid.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_50_0.png" alt="png"></p>
<p>when <script type="math/tex; ">\phi(z)</script> approached 1 if z&#x2192;<script type="math/tex; ">\infty</script>, goes to 1 if z&#x2192;<script type="math/tex; ">-\infty</script></p>
<h3 id="plot-cost-function"><a name="plot-cost-function" class="plugin-anchor" href="#plot-cost-function"><i class="fa fa-link" aria-hidden="true"></i></a>Plot cost function:</h3>
<p>use log-likelihood function to redefine cost function</p>
<p>then <script type="math/tex; ">\displaystyle J(\phi(z),y;w)={\begin{cases}-log(\phi(z))&{\text{if }}y=1\\-log(1-\phi(z))&{\text{if }}y=0\end{cases}}</script></p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost_1</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> - np.log(sigmoid(z))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cost_0</span><span class="hljs-params">(z)</span>:</span>
    <span class="hljs-keyword">return</span> - np.log(<span class="hljs-number">1</span> - sigmoid(z))

z = np.arange(<span class="hljs-number">-10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0.1</span>)
phi_z = sigmoid(z)

c1 = [cost_1(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> z]
plt.plot(phi_z, c1, label=<span class="hljs-string">&apos;J(w) if y=1&apos;</span>)

c0 = [cost_0(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> z]
plt.plot(phi_z, c0, linestyle=<span class="hljs-string">&apos;--&apos;</span>, label=<span class="hljs-string">&apos;J(w) if y=0&apos;</span>)

plt.ylim(<span class="hljs-number">0.0</span>, <span class="hljs-number">5.1</span>)
plt.xlim([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
plt.xlabel(<span class="hljs-string">&apos;$\phi$(z)&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;J(w)&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;best&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/log_cost.png&apos;, dpi=300)</span>

<span class="hljs-comment"># this illustrates the cost for the classification of a single-sample instance for diff values of phi(z)</span>
</code></pre>
<p><img src="output_53_0.png" alt="png"></p>
<p>cost &#x8D8B;&#x8FD1;&#x4E8E;0 &#x5982;&#x679C;&#x6B63;&#x786E;&#x9884;&#x6D4B; class.</p>
<h3 id="implement-in-python"><a name="implement-in-python" class="plugin-anchor" href="#implement-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implement in Python</h3>
<p>The following implementation is similar to the Adaline implementation except that we replace the sum of squared errors cost function with the logistic cost function</p>
<p><script type="math/tex; ">J(\mathbf{w}) = \sum_{i=1}^{m} - y^{(i)} log \bigg( \phi\big(z^{(i)}\big) \bigg) - \big(1 - y^{(i)}\big) log\bigg(1-\phi\big(z^{(i)}\big)\bigg).</script></p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LogisticRegression</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;LogisticRegression classifier.

    Parameters
    ------------
    eta : float
        Learning rate (between 0.0 and 1.0)
    n_iter : int
        Passes over the training dataset.

    Attributes
    -----------
    w_ : 1d-array
        Weights after fitting.
    cost_ : list
        Cost in every epoch.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, eta=<span class="hljs-number">0.01</span>, n_iter=<span class="hljs-number">50</span>)</span>:</span>
        self.eta = eta
        self.n_iter = n_iter

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y : array-like, shape = [n_samples]
            Target values.

        Returns
        -------
        self : object

        &quot;&quot;&quot;</span>
        self.w_ = np.zeros(<span class="hljs-number">1</span> + X.shape[<span class="hljs-number">1</span>])
        self.cost_ = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.n_iter):
            y_val = self.activation(X)
            errors = (y - y_val)
            neg_grad = X.T.dot(errors)
            self.w_[<span class="hljs-number">1</span>:] += self.eta * neg_grad
            self.w_[<span class="hljs-number">0</span>] += self.eta * errors.sum()
            self.cost_.append(self._logit_cost(y, self.activation(X)))
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_logit_cost</span><span class="hljs-params">(self, y, y_val)</span>:</span>
        logit = -y.dot(np.log(y_val)) - ((<span class="hljs-number">1</span> - y).dot(np.log(<span class="hljs-number">1</span> - y_val)))
        <span class="hljs-keyword">return</span> logit

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_sigmoid</span><span class="hljs-params">(self, z)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + np.exp(-z))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net_input</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.dot(X, self.w_[<span class="hljs-number">1</span>:]) + self.w_[<span class="hljs-number">0</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activation</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Activate the logistic neuron&quot;&quot;&quot;</span>
        z = self.net_input(X)
        <span class="hljs-keyword">return</span> self._sigmoid(z)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_proba</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        Predict class probabilities for X.

        Parameters
        ----------
        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.

        Returns
        ----------
          Class 1 probability : float

        &quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> activation(X)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        Predict class labels for X.

        Parameters
        ----------
        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.

        Returns
        ----------
        class : int
            Predicted class label.

        &quot;&quot;&quot;</span>
        <span class="hljs-comment"># equivalent to np.where(self.activation(X) &gt;= 0.5, 1, 0)</span>
        <span class="hljs-keyword">return</span> np.where(self.net_input(X) &gt;= <span class="hljs-number">0.0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)
</code></pre>
<pre><code class="lang-python">y[y == <span class="hljs-number">-1</span>] = <span class="hljs-number">0</span>  <span class="hljs-comment"># &#x5C06;&#x8D1F;&#x6027;&#x6807;&#x7B7E;&#x7F16;&#x7801;&#x4E3A; 0</span>
y
</code></pre>
<pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1])
</code></pre><pre><code class="lang-python">lr = LogisticRegression(n_iter=<span class="hljs-number">500</span>, eta=<span class="hljs-number">0.02</span>).fit(X_std, y)
plt.plot(range(<span class="hljs-number">1</span>, len(lr.cost_) + <span class="hljs-number">1</span>), np.log10(lr.cost_))
plt.xlabel(<span class="hljs-string">&apos;Epochs&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Cost&apos;</span>)
plt.title(<span class="hljs-string">&apos;Logistic Regression - Learning rate 0.02&apos;</span>)

plt.tight_layout()
</code></pre>
<p><img src="output_59_0.png" alt="png"></p>
<pre><code class="lang-python">plot_decision_regions(X_std, y, classifier=lr)
plt.title(<span class="hljs-string">&apos;Logistic Regression - Gradient Descent&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;sepal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
</code></pre>
<p><img src="output_60_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="classification-with-scikit-learn"><a name="classification-with-scikit-learn" class="plugin-anchor" href="#classification-with-scikit-learn"><i class="fa fa-link" aria-hidden="true"></i></a>Classification with scikit-learn</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="loading-and-preprocessing-the-data"><a name="loading-and-preprocessing-the-data" class="plugin-anchor" href="#loading-and-preprocessing-the-data"><i class="fa fa-link" aria-hidden="true"></i></a>Loading and preprocessing the data</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>Loading the <strong>Iris dataset</strong> from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

iris = datasets.load_iris()
X = iris.data[:, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]
y = iris.target

print(<span class="hljs-string">&apos;Class labels:&apos;</span>, np.unique(y))
print(iris.target_names)
</code></pre>
<pre><code>(&apos;Class labels:&apos;, array([0, 1, 2]))
[&apos;setosa&apos; &apos;versicolor&apos; &apos;virginica&apos;]
</code></pre><p>Splitting data into 70% training and 30% test data:</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split

X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)
</code></pre>
<p>Standardizing the features:</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train) <span class="hljs-comment"># standardize by mean &amp; std</span>
X_test_std = sc.transform(X_test)
</code></pre>
<p><br>
<br></p>
<h2 id="other-available-data"><a name="other-available-data" class="plugin-anchor" href="#other-available-data"><i class="fa fa-link" aria-hidden="true"></i></a>Other Available Data</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><a href="http://scikit-learn.org/stable/datasets/#dataset-loading-utilities" target="_blank">Scikit-learn makes available a host of datasets for testing learning algorithms</a>.
They come in three flavors:</p>
<ul>
<li><strong>Packaged Data:</strong> these small datasets are packaged with the scikit-learn installation,
and can be downloaded using the tools in <code>sklearn.datasets.load_*</code></li>
<li><strong>Downloadable Data:</strong> these larger datasets are available for download, and scikit-learn
includes tools which streamline this process.  These tools can be found in
<code>sklearn.datasets.fetch_*</code></li>
<li><strong>Generated Data:</strong> there are several datasets which are generated from models based on a
random seed.  These are available in the <code>sklearn.datasets.make_*</code></li>
</ul>
<p>You can explore the available dataset loaders, fetchers, and generators using IPython&apos;s
tab-completion functionality.  After importing the <code>datasets</code> submodule from <code>sklearn</code>,
type</p>
<pre><code>datasets.load_&lt;TAB&gt;
</code></pre><p>or</p>
<pre><code>datasets.fetch_&lt;TAB&gt;
</code></pre><p>or</p>
<pre><code>datasets.make_&lt;TAB&gt;
</code></pre><p>to see a list of available functions.</p>
<p>The data downloaded using the <code>fetch_</code> scripts are stored locally,
within a subdirectory of your home directory.
You can use the following to determine where it is:</p>
<pre><code>from sklearn.datasets import get_data_home
get_data_home()
</code></pre><p><br>
<br></p>
<h2 id="training-a-perceptron-via-scikit-learn"><a name="training-a-perceptron-via-scikit-learn" class="plugin-anchor" href="#training-a-perceptron-via-scikit-learn"><i class="fa fa-link" aria-hidden="true"></i></a>Training a perceptron via scikit-learn</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> Perceptron
<span class="hljs-comment"># sklearn &#x4E2D;&#x6709;&#x5C01;&#x88C5;&#x597D;&#x7684; Perceptron &#x51FD;&#x6570;</span>
ppn = Perceptron(n_iter=<span class="hljs-number">40</span>, eta0=<span class="hljs-number">0.1</span>, random_state=<span class="hljs-number">0</span>)
ppn.fit(X_train_std, y_train)
</code></pre>
<pre><code>Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True,
      n_iter=40, n_jobs=1, penalty=None, random_state=0, shuffle=True,
      verbose=0, warm_start=False)
</code></pre><pre><code class="lang-python">y_test.shape
</code></pre>
<pre><code>(45,)
</code></pre><pre><code class="lang-python">y_pred = ppn.predict(X_test_std) <span class="hljs-comment"># predict</span>
print(<span class="hljs-string">&apos;Misclassified samples: %d&apos;</span> % (y_test != y_pred).sum()) <span class="hljs-comment"># &#x9519;&#x8BEF;&#x4E2A;&#x6570;</span>
</code></pre>
<pre><code>Misclassified samples: 4
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score

print(<span class="hljs-string">&apos;Accuracy: %.2f&apos;</span> % accuracy_score(y_test, y_pred)) <span class="hljs-comment"># 91% &#x7684;&#x51C6;&#x786E;&#x7387;</span>
</code></pre>
<pre><code>Accuracy: 0.91
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

<span class="hljs-comment"># &#x91CD;&#x65B0;&#x5B9A;&#x4E49;&#x753B;&#x51B3;&#x7B56;&#x8FB9;&#x754C;&#x51FD;&#x6570;, &#x4F7F;&#x5F97;&#x80FD;&#x533A;&#x5206;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x548C;&#x6D4B;&#x8BD5;&#x6570;&#x636E;</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_decision_regions</span><span class="hljs-params">(X, y, classifier, test_idx=None, resolution=<span class="hljs-number">0.02</span>)</span>:</span>

    <span class="hljs-comment"># setup marker generator and color map</span>
    markers = (<span class="hljs-string">&apos;s&apos;</span>, <span class="hljs-string">&apos;x&apos;</span>, <span class="hljs-string">&apos;o&apos;</span>, <span class="hljs-string">&apos;^&apos;</span>, <span class="hljs-string">&apos;v&apos;</span>)
    colors = (<span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;lightgreen&apos;</span>, <span class="hljs-string">&apos;gray&apos;</span>, <span class="hljs-string">&apos;cyan&apos;</span>)
    cmap = ListedColormap(colors[:len(np.unique(y))])

    <span class="hljs-comment"># plot the decision surface</span>
    x1_min, x1_max = X[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
    x2_min, x2_max = X[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                         np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=<span class="hljs-number">0.4</span>, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    <span class="hljs-comment"># plot all samples</span>
    <span class="hljs-keyword">for</span> idx, cl <span class="hljs-keyword">in</span> enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, <span class="hljs-number">0</span>], y=X[y == cl, <span class="hljs-number">1</span>],
                    alpha=<span class="hljs-number">0.8</span>, c=cmap(idx),
                    marker=markers[idx], label=cl)

    <span class="hljs-comment"># highlight test samples</span>
    <span class="hljs-keyword">if</span> test_idx:
        X_test, y_test = X[test_idx, :], y[test_idx]
        plt.scatter(X_test[:, <span class="hljs-number">0</span>], X_test[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;&apos;</span>,
                alpha=<span class="hljs-number">1.0</span>, linewidth=<span class="hljs-number">1</span>, marker=<span class="hljs-string">&apos;o&apos;</span>,
                s=<span class="hljs-number">55</span>, label=<span class="hljs-string">&apos;test set&apos;</span>)
</code></pre>
<p>Training a perceptron model using the standardized training data:</p>
<pre><code class="lang-python">X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

plot_decision_regions(X=X_combined_std, y=y_combined,
                      classifier=ppn, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))
plt.xlabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/iris_perceptron_scikit.png&apos;, dpi=300)</span>
<span class="hljs-comment"># &#x8FD9;&#x6B21;&#x662F;3&#x4E2A;&#x5206;&#x7C7B;&#x4E00;&#x8D77;</span>
</code></pre>
<p><img src="output_85_0.png" alt="png"></p>
<p>Peceptron &#x6A21;&#x578B;&#x5BF9;&#x4E8E;&#x5E76;&#x4E0D;&#x662F;&#x5B8C;&#x5168;&#x7EBF;&#x6027;&#x9694;&#x79BB;&#x7684; dataset &#x4E0D;&#x80FD; converge, &#x6240;&#x4EE5;&#x5B9E;&#x9645;&#x5E94;&#x7528;&#x4E2D;&#x5E76;&#x4E0D;&#x591A;&#x7528;.
<br>
<br></p>
<p><br>
<br></p>
<h2 id="modeling-class-probabilities-via-logistic-regression"><a name="modeling-class-probabilities-via-logistic-regression" class="plugin-anchor" href="#modeling-class-probabilities-via-logistic-regression"><i class="fa fa-link" aria-hidden="true"></i></a>Modeling class probabilities via logistic regression</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># use Logistic Regression</span>
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-comment"># C parameter &#x662F;&#x4EC0;&#x4E48;&#x5462;?</span>
lr = LogisticRegression(C=<span class="hljs-number">1000.0</span>, random_state=<span class="hljs-number">0</span>)
lr.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=lr, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))
plt.xlabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/logistic_regression.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_90_0.png" alt="png"></p>
<pre><code class="lang-python">lr.predict_proba(X_test_std[<span class="hljs-number">0</span>,:].reshape(<span class="hljs-number">1</span>,<span class="hljs-number">-1</span>)) <span class="hljs-comment"># predict probability</span>
</code></pre>
<pre><code>array([[  2.05743774e-11,   6.31620264e-02,   9.36837974e-01]])
</code></pre><h3 id="regularization-path"><a name="regularization-path" class="plugin-anchor" href="#regularization-path"><i class="fa fa-link" aria-hidden="true"></i></a>Regularization path</h3>
<p>&#x89E3;&#x51B3; overfitting: &#x6A21;&#x578B;&#x62DF;&#x5408;&#x7684;&#x8FC7;&#x597D;, &#x4EE5;&#x81F4;&#x4E8E;&#x6CA1;&#x6709;&#x4E00;&#x822C;&#x6027;, &#x9884;&#x6D4B;&#x65B0;&#x7684;&#x6837;&#x672C;&#x7684;&#x7ED3;&#x679C;&#x5C31;&#x4F1A;&#x5F88;&#x5DEE;</p>
<p>&#x4E00;&#x822C;&#x8FC7;&#x62DF;&#x5408;&#x7684;&#x6A21;&#x578B;&#x4F1A;&#x6709; high variance</p>
<p>&#x6700;&#x5E38;&#x7528;&#x7684;&#x89E3;&#x51B3;&#x65B9;&#x6CD5;&#x5C31;&#x53EB;&#x505A; L2 regulatization<script type="math/tex; ">\displaystyle \frac{\lambda}{2} \lVert w \rVert^2 = \frac{\lambda}{2}\sum_{j=1}^m w_j^2</script></p>
<p>&#x5176;&#x4E2D;<script type="math/tex; ">\lambda</script> &#x5C31;&#x662F; regularization parameter, &#x53EF;&#x4EE5;&#x7528;&#x6765;&#x63A7;&#x5236;&#x62DF;&#x5408;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x7684;&#x597D;&#x574F;, &#x800C; <script type="math/tex; ">C = \frac{1}{\lambda}</script> &#x5C31;&#x662F;&#x524D;&#x9762;&#x63D0;&#x5230;&#x8FC7;&#x7684; parameter</p>
<pre><code class="lang-python">weights, params = [], []
<span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">-5</span>, <span class="hljs-number">5</span>):
    lr = LogisticRegression(C=<span class="hljs-number">10</span>**c, random_state=<span class="hljs-number">0</span>)
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[<span class="hljs-number">1</span>])
    params.append(<span class="hljs-number">10</span>**c)

weights = np.array(weights)
plt.plot(params, weights[:, <span class="hljs-number">0</span>],
         label=<span class="hljs-string">&apos;petal length&apos;</span>)
plt.plot(params, weights[:, <span class="hljs-number">1</span>], linestyle=<span class="hljs-string">&apos;--&apos;</span>,
         label=<span class="hljs-string">&apos;petal width&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;weight coefficient&apos;</span>)
plt.xlabel(<span class="hljs-string">&apos;C&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.xscale(<span class="hljs-string">&apos;log&apos;</span>)
<span class="hljs-comment"># plt.savefig(&apos;./figures/regression_path.png&apos;, dpi=300)</span>

<span class="hljs-comment"># C &#x51CF;&#x5C0F;&#x7684;&#x8BDD;, &#x5C31;&#x662F;&#x589E;&#x52A0; regularization</span>
</code></pre>
<p><img src="output_93_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="logistic-regression-with-regularization"><a name="logistic-regression-with-regularization" class="plugin-anchor" href="#logistic-regression-with-regularization"><i class="fa fa-link" aria-hidden="true"></i></a>Logistic regression with regularization</h2>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LogitGD</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Logistic Regression classifier.

    Parameters
    ------------
    eta : float
        Learning rate (between 0.0 and 1.0)
    n_iter : int
        Passes over the training dataset.

    Attributes
    -----------
    w_ : 1d-array
        Weights after fitting.
    errors_ : list
        Number of misclassifications in every epoch.

    &quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, eta=<span class="hljs-number">0.01</span>, lamb = <span class="hljs-number">0.01</span>, n_iter=<span class="hljs-number">50</span>)</span>:</span>
        self.eta = eta
        self.n_iter = n_iter
        self.lamb = lamb

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span><span class="hljs-params">(self, X, y)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
            Training vectors, where n_samples is the number of samples and
            n_features is the number of features.
        y : array-like, shape = [n_samples]
            Target values.

        Returns
        -------
        self : object

        &quot;&quot;&quot;</span>
        self.w_ = np.zeros(<span class="hljs-number">1</span> + X.shape[<span class="hljs-number">1</span>])
        self.cost_ = []


        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.n_iter):
            output = self.net_input(X)
            errors = (y - output)
            self.w_[<span class="hljs-number">1</span>:] += self.eta * X.T.dot(errors) - self.lamb* self.w_[<span class="hljs-number">1</span>:]
            self.w_[<span class="hljs-number">0</span>] += self.eta * errors.sum()
            cost = (errors**<span class="hljs-number">2</span>).sum() / <span class="hljs-number">2.0</span> + self.lamb* np.sum(self.w_[<span class="hljs-number">1</span>:]**<span class="hljs-number">2</span>)
            self.cost_.append(cost)
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">net_input</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.dot(X, self.w_[<span class="hljs-number">1</span>:]) + self.w_[<span class="hljs-number">0</span>]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sigmoid</span><span class="hljs-params">(z)</span>:</span>
        <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + np.exp(-z))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">activation</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> sigmoid(self.net_input(X))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self, X)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span>
        <span class="hljs-keyword">return</span> np.where(self.activation(X) &gt;= <span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
</code></pre>
<pre><code class="lang-python">

</code></pre>
<h2 id="&#x5176;&#x5B83;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x7B80;&#x4ECB;"><a name="&#x5176;&#x5B83;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x7B80;&#x4ECB;" class="plugin-anchor" href="#&#x5176;&#x5B83;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x7B80;&#x4ECB;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x5176;&#x5B83;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x7B80;&#x4ECB;</h2>
<h1 id="maximum-margin-classification-with-support-vector-machines"><a name="maximum-margin-classification-with-support-vector-machines" class="plugin-anchor" href="#maximum-margin-classification-with-support-vector-machines"><i class="fa fa-link" aria-hidden="true"></i></a>Maximum margin classification with support vector machines</h1>
<p> &#x76EE;&#x7684;&#x662F; maximize the <strong>margin</strong>, margin &#x662F;&#x5206;&#x79BB;&#x51B3;&#x7B56;&#x8FB9;&#x754C;&#x4E0E;&#x79BB;&#x4E4B;&#x6700;&#x8FD1;&#x7684;&#x8BAD;&#x7EC3;&#x6837;&#x672C;&#x4E4B;&#x95F4;&#x7684;&#x8DDD;&#x79BB;.  </p>
<p><img src="https://www.safaribooksonline.com/library/view/python-real-world-data/9781786465160/graphics/3547_03_07.jpg" width="90%" height="90%"></p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># train SVC</span>
<span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC

svm = SVC(kernel=<span class="hljs-string">&apos;linear&apos;</span>, C=<span class="hljs-number">1.0</span>, random_state=<span class="hljs-number">0</span>)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=svm, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))
plt.xlabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/support_vector_machine_linear.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_102_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="solving-non-linear-problems-using-a-kernel-svm"><a name="solving-non-linear-problems-using-a-kernel-svm" class="plugin-anchor" href="#solving-non-linear-problems-using-a-kernel-svm"><i class="fa fa-link" aria-hidden="true"></i></a>Solving non-linear problems using a kernel SVM</h2>
<p>SVM &#x53EF;&#x4EE5;&#x89E3;&#x51B3;&#x975E;&#x7EBF;&#x6027;&#x95EE;&#x9898;</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># create a simple dataset</span>
np.random.seed(<span class="hljs-number">0</span>)
X_xor = np.random.randn(<span class="hljs-number">200</span>, <span class="hljs-number">2</span>)
y_xor = np.logical_xor(X_xor[:, <span class="hljs-number">0</span>] &gt; <span class="hljs-number">0</span>, X_xor[:, <span class="hljs-number">1</span>] &gt; <span class="hljs-number">0</span>) <span class="hljs-comment"># 100&#x4E2A; with label 1, 100 withlabel 0</span>
y_xor = np.where(y_xor, <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)

plt.scatter(X_xor[y_xor==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], X_xor[y_xor==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;b&apos;</span>, marker=<span class="hljs-string">&apos;x&apos;</span>, label=<span class="hljs-string">&apos;1&apos;</span>)
plt.scatter(X_xor[y_xor==<span class="hljs-number">-1</span>, <span class="hljs-number">0</span>], X_xor[y_xor==<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;r&apos;</span>, marker=<span class="hljs-string">&apos;s&apos;</span>, label=<span class="hljs-string">&apos;-1&apos;</span>)

plt.xlim([<span class="hljs-number">-3</span>, <span class="hljs-number">3</span>])
plt.ylim([<span class="hljs-number">-3</span>, <span class="hljs-number">3</span>])
plt.legend(loc=<span class="hljs-string">&apos;best&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/xor.png&apos;, dpi=300)</span>

<span class="hljs-comment"># &#x4F7F;&#x7528;&#x666E;&#x901A;&#x7684;  linear logistic Regression &#x4E0D;&#x80FD;&#x5F88;&#x597D;&#x5C06;&#x6837;&#x672C;&#x5206;&#x4E3A;+ve &#x548C;-ve</span>
</code></pre>
<p><img src="output_106_0.png" alt="png"></p>
<p>rbf &#x662F;&#x6307; radial basis function kernel &#x6216;&#x8005; Gaussian kernel</p>
<p><script type="math/tex; ">\displaystyle k(x^{(i)}, x^{(j)}) = exp (- \frac{\lVert x^{(i)}-x^{(j)} \rVert^2}{2 \sigma^2})</script></p>
<p>simplified to <script type="math/tex; ">\displaystyle exp(-\gamma \lVert x^{(i)}-x^{(j)} \rVert^2)</script>
with <script type="math/tex; ">\gamma = \frac{1}{2\sigma^2}</script></p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x4F7F;&#x7528; svm kernel &#x65B9;&#x6CD5;, &#x6295;&#x5C04;&#x5230;&#x9AD8;&#x7EAC;&#x5EA6;&#x4E2D;, &#x4F7F;&#x4E4B;&#x6210;&#x4E3A;&#x7EBF;&#x6027;&#x53EF;&#x5206;&#x79BB;&#x7684;</span>
svm = SVC(kernel=<span class="hljs-string">&apos;rbf&apos;</span>, random_state=<span class="hljs-number">0</span>, gamma=<span class="hljs-number">0.10</span>, C=<span class="hljs-number">10.0</span>)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor,
                      classifier=svm)

plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/support_vector_machine_rbf_xor.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_108_0.png" alt="png"></p>
<p>&#x5176;&#x4E2D;<script type="math/tex; ">\gamma</script> parameter &#x53EF;&#x4EE5;&#x88AB;&#x7406;&#x89E3;&#x4E3A; cut-off parameter for Gaussian sphere</p>
<p>&#x5F53;<script type="math/tex; ">\gamma</script> &#x589E;&#x52A0;, &#x4E5F;&#x5C31;&#x589E;&#x52A0;&#x4E86;&#x8BAD;&#x7EC3;&#x6837;&#x672C;&#x7684;&#x5F71;&#x54CD;, &#x4E5F;&#x5C31;&#x4F1A;&#x4F7F;&#x51B3;&#x7B56;&#x8FB9;&#x754C;&#x53D8;&#x5F97;&#x6A21;&#x7CCA;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC
<span class="hljs-comment">## gamma &#x8F83;&#x5C0F;</span>
svm = SVC(kernel=<span class="hljs-string">&apos;rbf&apos;</span>, random_state=<span class="hljs-number">0</span>, gamma=<span class="hljs-number">0.2</span>, C=<span class="hljs-number">1.0</span>)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=svm, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))
plt.xlabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/support_vector_machine_rbf_iris_1.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_110_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># gamma &#x5F88;&#x5927;, &#x8FB9;&#x754C; tight</span>
svm = SVC(kernel=<span class="hljs-string">&apos;rbf&apos;</span>, random_state=<span class="hljs-number">0</span>, gamma=<span class="hljs-number">100.0</span>, C=<span class="hljs-number">1.0</span>)
svm.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=svm, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))
plt.xlabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/support_vector_machine_rbf_iris_2.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_111_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="k-nearest-neighbors---a-lazy-learning-algorithm"><a name="k-nearest-neighbors---a-lazy-learning-algorithm" class="plugin-anchor" href="#k-nearest-neighbors---a-lazy-learning-algorithm"><i class="fa fa-link" aria-hidden="true"></i></a>K-nearest neighbors - a lazy learning algorithm</h2>
<p>it doesn&apos;t learn a discriminative function from the training data but memorizes the training dataset instead.</p>
<ol>
<li>Choose the number of k and a distance metric.</li>
<li>Find the k nearest neighbors of the sample that we want to classify.</li>
<li>Assign the class label by majority vote.</li>
</ol>
<p>&#x8FD9;&#x79CD;&#x65B9;&#x6CD5;&#x597D;&#x5904;&#x5728;&#x4E8E;&#x65B0;&#x6570;&#x636E;&#x8FDB;&#x6765;, &#x5206;&#x7C7B;&#x5668;&#x53EF;&#x4EE5;&#x9A6C;&#x4E0A;&#x5B66;&#x4E60;&#x5E76;&#x9002;&#x5E94;, &#x4F46;&#x662F;&#x8BA1;&#x7B97;&#x6210;&#x672C;&#x4E5F;&#x662F;&#x7EBF;&#x6027;&#x589E;&#x957F;, &#x5B58;&#x50A8;&#x4E5F;&#x662F;&#x95EE;&#x9898;.</p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier
<span class="hljs-comment"># &#x5BFB;&#x627E;5&#x4E2A;&#x90BB;&#x5C45;</span>
knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number">5</span>, p=<span class="hljs-number">2</span>, metric=<span class="hljs-string">&apos;minkowski&apos;</span>)
knn.fit(X_train_std, y_train)

plot_decision_regions(X_combined_std, y_combined,
                      classifier=knn, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))

plt.xlabel(<span class="hljs-string">&apos;petal length [standardized]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [standardized]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/k_nearest_neighbors.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_115_0.png" alt="png"></p>
<p>&#x5982;&#x4F55;&#x9009;&#x62E9; k &#x662F;&#x4E00;&#x4E2A;&#x91CD;&#x70B9;, &#x5E76;&#x4E14;&#x9700;&#x8981;&#x6807;&#x51C6;&#x5316;&#x6570;&#x636E;. &#x4F8B;&#x5B50;&#x4E2D;&#x7528;&#x5230;&#x7684;&apos;minkowski&apos; distance &#x662F;&#x666E;&#x901A;&#x7684; Euclidean &#x548C; Manhattan distance &#x7684;&#x6269;&#x5C55;.</p>
<p><script type="math/tex; ">\displaystyle d(x^{(i)}, x^{(j)}) = \sqrt[p]{\sum_k \left|x_k^{(i)}x_k^{(j)}\right|^p} </script></p>
<p><br>
<br></p>
<h1 id="scoring-metrics-for-classification"><a name="scoring-metrics-for-classification" class="plugin-anchor" href="#scoring-metrics-for-classification"><i class="fa fa-link" aria-hidden="true"></i></a>Scoring metrics for classification</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="classification-metrics-in-scikit-learn"><a name="classification-metrics-in-scikit-learn" class="plugin-anchor" href="#classification-metrics-in-scikit-learn"><i class="fa fa-link" aria-hidden="true"></i></a>Classification metrics in Scikit-learn</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p></p><p>The <a class="reference internal" href="http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics" title="sklearn.metrics" target="_blank"><code class="xref py py-mod docutils literal">sklearn.metrics</code></a> module implements several loss, score, and utility
functions to measure classification performance.
Some metrics might require probability estimates of the positive class,
confidence values, or binary decisions values.
Most implementations allow each sample to provide a weighted contribution
to the overall score, through the <code class="docutils literal">sample_weight</code> parameter.</p><p></p>
<p></p><p>Some of these are restricted to the binary classification case:</p><p></p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%">
<col width="90%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef" target="_blank"><code class="xref py py-obj docutils literal">matthews_corrcoef</code></a>(y_true,&#xA0;y_pred)</td>
<td>Compute the Matthews correlation coefficient (MCC) for binary classes</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve" target="_blank"><code class="xref py py-obj docutils literal">precision_recall_curve</code></a>(y_true,&#xA0;probas_pred)</td>
<td>Compute precision-recall pairs for different probability thresholds</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve" target="_blank"><code class="xref py py-obj docutils literal">roc_curve</code></a>(y_true,&#xA0;y_score[,&#xA0;pos_label,&#xA0;...])</td>
<td>Compute Receiver operating characteristic (ROC)</td>
</tr>
</tbody>
</table>
<p>Others also work in the multiclass case:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%">
<col width="90%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix" target="_blank"><code class="xref py py-obj docutils literal">confusion_matrix</code></a>(y_true,&#xA0;y_pred[,&#xA0;labels])</td>
<td>Compute confusion matrix to evaluate the accuracy of a classification</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss" target="_blank"><code class="xref py py-obj docutils literal">hinge_loss</code></a>(y_true,&#xA0;pred_decision[,&#xA0;labels,&#xA0;...])</td>
<td>Average hinge loss (non-regularized)</td>
</tr>
</tbody>
</table>
<p>Some also work in the multilabel case:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%">
<col width="90%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score" target="_blank"><code class="xref py py-obj docutils literal">accuracy_score</code></a>(y_true,&#xA0;y_pred[,&#xA0;normalize,&#xA0;...])</td>
<td>Accuracy classification score.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report" target="_blank"><code class="xref py py-obj docutils literal">classification_report</code></a>(y_true,&#xA0;y_pred[,&#xA0;...])</td>
<td>Build a text report showing the main classification metrics</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score" target="_blank"><code class="xref py py-obj docutils literal">f1_score</code></a>(y_true,&#xA0;y_pred[,&#xA0;labels,&#xA0;...])</td>
<td>Compute the F1 score, also known as balanced F-score or F-measure</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score" target="_blank"><code class="xref py py-obj docutils literal">fbeta_score</code></a>(y_true,&#xA0;y_pred,&#xA0;beta[,&#xA0;labels,&#xA0;...])</td>
<td>Compute the F-beta score</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss" target="_blank"><code class="xref py py-obj docutils literal">hamming_loss</code></a>(y_true,&#xA0;y_pred[,&#xA0;classes])</td>
<td>Compute the average Hamming loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score" target="_blank"><code class="xref py py-obj docutils literal">jaccard_similarity_score</code></a>(y_true,&#xA0;y_pred[,&#xA0;...])</td>
<td>Jaccard similarity coefficient score</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss" target="_blank"><code class="xref py py-obj docutils literal">log_loss</code></a>(y_true,&#xA0;y_pred[,&#xA0;eps,&#xA0;normalize,&#xA0;...])</td>
<td>Log loss, aka logistic loss or cross-entropy loss.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support" target="_blank"><code class="xref py py-obj docutils literal">precision_recall_fscore_support</code></a>(y_true,&#xA0;y_pred)</td>
<td>Compute precision, recall, F-measure and support for each class</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score" target="_blank"><code class="xref py py-obj docutils literal">precision_score</code></a>(y_true,&#xA0;y_pred[,&#xA0;labels,&#xA0;...])</td>
<td>Compute the precision</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score" target="_blank"><code class="xref py py-obj docutils literal">recall_score</code></a>(y_true,&#xA0;y_pred[,&#xA0;labels,&#xA0;...])</td>
<td>Compute the recall</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss" target="_blank"><code class="xref py py-obj docutils literal">zero_one_loss</code></a>(y_true,&#xA0;y_pred[,&#xA0;normalize,&#xA0;...])</td>
<td>Zero-one classification loss.</td>
</tr>
</tbody>
</table>
<p>And some work with binary and multilabel (but not multiclass) problems:</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%">
<col width="90%">
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score" target="_blank"><code class="xref py py-obj docutils literal">average_precision_score</code></a>(y_true,&#xA0;y_score[,&#xA0;...])</td>
<td>Compute average precision (AP) from prediction scores</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score" target="_blank"><code class="xref py py-obj docutils literal">roc_auc_score</code></a>(y_true,&#xA0;y_score[,&#xA0;average,&#xA0;...])</td>
<td>Compute Area Under the Curve (AUC) from prediction scores</td>
</tr>
</tbody>
</table>


<pre><code class="lang-python"><span class="hljs-comment"># &#x6784;&#x5EFA;&#x6570;&#x636E;</span>
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.svm <span class="hljs-keyword">import</span> SVC

X, y = datasets.make_classification(n_classes=<span class="hljs-number">2</span>, random_state=<span class="hljs-number">0</span>)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)

sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train) <span class="hljs-comment"># standardize by mean &amp; std</span>
X_test_std = sc.transform(X_test)

model = SVC(probability=<span class="hljs-keyword">True</span>, random_state=<span class="hljs-number">0</span>)
model.fit(X_train_std, y_train);
</code></pre>
<p>default score for classification in sklearn is accuracy (&#x6807;&#x7B7E;&#x9884;&#x6D4B;&#x6B63;&#x786E;&#x7684;&#x6BD4;&#x4F8B;)</p>
<p><script type="math/tex; "> accuracy(y, \hat y) = \frac 1 n \sum^{n - 1}_{i=0} 1 (\hat y_i = y_i)</script>
where <script type="math/tex; ">1(x)</script> is the <a href="http://en.wikipedia.org/wiki/Indicator_function" target="_blank">indicator function</a></p>
<pre><code class="lang-python">model.score(X_test_std, y_test)
</code></pre>
<pre><code>0.83333333333333337
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score
y_pred = model.predict(X_test_std)
accuracy_score(y_test, y_pred)
</code></pre>
<pre><code>0.83333333333333337
</code></pre><p><br>
<br></p>
<h2 id="reading-a-confusion-matrix"><a name="reading-a-confusion-matrix" class="plugin-anchor" href="#reading-a-confusion-matrix"><i class="fa fa-link" aria-hidden="true"></i></a>Reading a confusion matrix</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/confusion_matrix.png" alt="Confusion Matrix"></p>
<p>For multi-class problems, it is often interesting to know which of the classes are hard to predict, and which are easy, or which classes get confused. One way to get more information about misclassifications is <code>the confusion_matrix</code>, which shows for each true class, how frequent a given predicted outcome is.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> confusion_matrix

y_test_pred = model.predict(X_test_std)

confmat = confusion_matrix(y_test, y_test_pred)
print(confmat)
</code></pre>
<pre><code>[[15  3]
 [ 2 10]]
</code></pre><pre><code class="lang-python">plt.matshow(confusion_matrix(y_test, y_test_pred), cmap=plt.cm.Blues)
plt.colorbar()
plt.xlabel(<span class="hljs-string">&quot;Predicted label&quot;</span>)
plt.ylabel(<span class="hljs-string">&quot;True label&quot;</span>);
</code></pre>
<p><img src="output_133_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="precision-recall-and-f-measures"><a name="precision-recall-and-f-measures" class="plugin-anchor" href="#precision-recall-and-f-measures"><i class="fa fa-link" aria-hidden="true"></i></a>Precision, recall and F-measures</h2>
<p>[<a href="#sections">back to top</a>]</p>
<ul>
<li><strong>Precision</strong> is how many of the predictions for a class are actually that class.</li>
<li><strong>Recall</strong> is how many of the true positives were recovered:</li>
<li><strong>f1-score</strong> is the geometric average of precision and recall:</li>
</ul>
<p>With TP, FP, TN, FN, FPR, TPR standing for &quot;true positive&quot;, &quot;false positive&quot;, &quot;true negative&quot; and &quot;false negative&quot;, &quot;false positive rate&quot;, &quot;true positive rate&quot; repectively:</p>
<p>\begin{align}
&amp;PRE = \frac{TP}{TP+FP} \
&amp;REC = TPR = \frac{TP}{FN+TP} \
&amp;F1 = 2 \frac{PRE \times REC}{PRE+REC} \
&amp;F_\beta = (1+\beta^2)\frac{PRE \times REC}{\beta^2 PRE+REC} \
&amp;FPR = \frac{FP}{FP+TN} \
&amp;TPR = \frac{TP}{FN+TP}
\end{align}</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> precision_score, recall_score, f1_score, fbeta_score

print(<span class="hljs-string">&apos;Precision: %.3f&apos;</span> % precision_score(y_true=y_test, y_pred=y_test_pred))
print(<span class="hljs-string">&apos;Recall: %.3f&apos;</span> % recall_score(y_true=y_test, y_pred=y_test_pred))
print(<span class="hljs-string">&apos;F1: %.3f&apos;</span> % f1_score(y_true=y_test, y_pred=y_test_pred))
print(<span class="hljs-string">&apos;F_beta2: %.3f&apos;</span> % fbeta_score(y_true=y_test, y_pred=y_test_pred, beta=<span class="hljs-number">2</span>))
</code></pre>
<pre><code>Precision: 0.769
Recall: 0.833
F1: 0.800
F_beta2: 0.820
</code></pre><p>Another useful function is the classification_report which provides precision, recall, fscore and support for all classes.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report
print(classification_report(y_test, y_test_pred))
</code></pre>
<pre><code>             precision    recall  f1-score   support

          0       0.88      0.83      0.86        18
          1       0.77      0.83      0.80        12

avg / total       0.84      0.83      0.83        30
</code></pre><p>These metrics are helpful in two particular cases that come up often in practice:</p>
<ol>
<li>Imbalanced classes, that is one class might be much more frequent than the other.</li>
<li>Asymmetric costs, that is one kind of error is much more &quot;costly&quot; than the other.</li>
</ol>
<p><br>
<br></p>
<h2 id="roc-and-auc"><a name="roc-and-auc" class="plugin-anchor" href="#roc-and-auc"><i class="fa fa-link" aria-hidden="true"></i></a>ROC and AUC</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>A <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank">receiver operating characteristic curve, or ROC curve</a>, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.</p>
<p>&#x5982;&#x679C;&#x5206;&#x7C7B;&#x5668;&#x6548;&#x679C;&#x5F88;&#x597D;, &#x90A3;&#x4E48;&#x56FE;&#x5E94;&#x8BE5;&#x4F1A;&#x5728;&#x5DE6;&#x4E0A;&#x89D2;.</p>
<p>&#x5728; ROC curve &#x7684;&#x57FA;&#x7840;&#x4E0A;, &#x53EF;&#x4EE5;&#x8BA1;&#x7B97; AUC -- area under the curve.</p>
<h3 id="area-under-curve"><a name="area-under-curve" class="plugin-anchor" href="#area-under-curve"><i class="fa fa-link" aria-hidden="true"></i></a>Area Under Curve</h3>
<p>The AUC is a common evaluation metric for binary classification problems.
Consider a plot of the true positive rate vs the false positive rate as the threshold value for classifying an item as 0 or is increased from 0 to 1:
if the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1.
If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5.</p>
<p>One characteristic of the AUC is that it is independent of the fraction of the test population which is class 0 or class 1: this makes the AUC useful for evaluating the performance of classifiers on unbalanced data sets.</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">roc_curve</span><span class="hljs-params">(true_labels, predicted_probs, n_points=<span class="hljs-number">100</span>, pos_class=<span class="hljs-number">1</span>)</span>:</span>
    thr = np.linspace(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,n_points)
    tpr = np.zeros(n_points)
    fpr = np.zeros(n_points)

    pos = true_labels == pos_class
    neg = np.logical_not(pos)
    n_pos = np.count_nonzero(pos)
    n_neg = np.count_nonzero(neg)

    <span class="hljs-keyword">for</span> i,t <span class="hljs-keyword">in</span> enumerate(thr):
        tpr[i] = np.count_nonzero(np.logical_and(predicted_probs &gt;= t, pos)) / float(n_pos)
        fpr[i] = np.count_nonzero(np.logical_and(predicted_probs &gt;= t, neg)) / float(n_neg)

    <span class="hljs-keyword">return</span> fpr, tpr, thr
</code></pre>
<pre><code class="lang-python">df_imputed = pd.read_csv(<span class="hljs-string">&apos;df_imputed&apos;</span>)
</code></pre>
<pre><code class="lang-python">features = [<span class="hljs-string">&apos;revolving_utilization_of_unsecured_lines&apos;</span>,
 <span class="hljs-string">&apos;age&apos;</span>,
 <span class="hljs-string">&apos;number_of_time30-59_days_past_due_not_worse&apos;</span>,
 <span class="hljs-string">&apos;debt_ratio&apos;</span>,
 <span class="hljs-string">&apos;monthly_income&apos;</span>,
 <span class="hljs-string">&apos;number_of_open_credit_lines_and_loans&apos;</span>,
 <span class="hljs-string">&apos;number_of_times90_days_late&apos;</span>,
 <span class="hljs-string">&apos;number_real_estate_loans_or_lines&apos;</span>,
 <span class="hljs-string">&apos;number_of_time60-89_days_past_due_not_worse&apos;</span>,
 <span class="hljs-string">&apos;number_of_dependents&apos;</span>,
 <span class="hljs-string">&apos;income_bins&apos;</span>,
 <span class="hljs-string">&apos;age_bin&apos;</span>,
 <span class="hljs-string">&apos;monthly_income_scaled&apos;</span>]
y = df_imputed.serious_dlqin2yrs
X = pd.get_dummies(df_imputed[features], columns = [<span class="hljs-string">&apos;income_bins&apos;</span>, <span class="hljs-string">&apos;age_bin&apos;</span>])
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split
train_X, test_X, train_y, test_y = train_test_split(X, y ,train_size=<span class="hljs-number">0.7</span>,random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Randomly generated predictions should give us a diagonal ROC curve</span>
preds = np.random.rand(len(test_y))
fpr, tpr, thr = roc_curve(test_y, preds)
plt.plot(fpr, tpr);
</code></pre>
<p><img src="output_152_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
clf = LogisticRegression()
clf.fit(train_X,train_y)
preds = clf.predict_proba(test_X)[:,<span class="hljs-number">1</span>]
fpr, tpr, thr = roc_curve(test_y, preds)
plt.plot(fpr, tpr);
</code></pre>
<p><img src="output_153_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="log-loss"><a name="log-loss" class="plugin-anchor" href="#log-loss"><i class="fa fa-link" aria-hidden="true"></i></a>Log loss</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates.
It is commonly used in (multinomial) logistic regression and neural networks,
as well as in some variants of expectation-maximization,
and can be used to evaluate the probability outputs (<code>predict_proba</code>)
of a classifier instead of its discrete predictions.</p>
<p>For binary classification with a true label <script type="math/tex; ">y \in \{0,1\}</script> and a probability estimate <script type="math/tex; ">p = \operatorname{Pr}(y = 1)</script>,
the log loss per sample is the negative log-likelihood of the classifier given the true label:</p>
<p><script type="math/tex; "> L_{log}(y, p) = -log Pr(y|p) = -(y log(p) + (1-y) log(1-p)) </script></p>
<p>This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix <script type="math/tex; ">Y</script>,
i.e., <script type="math/tex; ">y_{i,k} = 1</script> if sample <script type="math/tex; ">i</script> has label <script type="math/tex; ">k</script> taken from a set of <script type="math/tex; ">K</script> labels.
Let <script type="math/tex; ">P</script> be a matrix of probability estimates, with <script type="math/tex; ">p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)</script>.
Then the log loss of the whole set is</p>
<p><script type="math/tex; ">L_{log}(Y, P) = -logPr(Y|P) = - \frac 1 N \sum^{N-1}_{i=0} \sum^{K-1}_{k=0} y_{i,k}logp_{i,k} </script></p>
<p>To see how this generalizes the binary log loss given above, note that in the binary case,
we have <script type="math/tex; ">p_{i,0} = 1 - p_{i,1}</script> and <script type="math/tex; ">y_{i,0} = 1 - y_{i,1}</script>,
so expanding the inner sum over <script type="math/tex; ">y_{i,k} \in \{0,1\}</script> gives the binary log loss.</p>
<p>The <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" target="_blank">log_loss</a> function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator&#x2019;s predict_proba method.</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> log_loss
y_true = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
y_pred = [[<span class="hljs-number">.9</span>, <span class="hljs-number">.1</span>], [<span class="hljs-number">.8</span>, <span class="hljs-number">.2</span>], [<span class="hljs-number">.3</span>, <span class="hljs-number">.7</span>], [<span class="hljs-number">.01</span>, <span class="hljs-number">.99</span>]]
log_loss(y_true, y_pred)
</code></pre>
<pre><code>0.17380733669106749
</code></pre><h2 id="hinge-loss"><a name="hinge-loss" class="plugin-anchor" href="#hinge-loss"><i class="fa fa-link" aria-hidden="true"></i></a>Hinge loss</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>The <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" target="_blank">hinge_loss</a> function computes the average distance between
the model and the data using <a href="http://en.wikipedia.org/wiki/Hinge_loss" target="_blank">hinge loss</a>, a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.)</p>
<p>If the labels are encoded with +1 and -1,  <script type="math/tex; ">y</script>:
is the true value, and $w$ is the predicted decisions as output by decision_function,
then the hinge loss is defined as:</p>
<p><script type="math/tex; "> L_{Hinge}(y, w) = max\{1-wy, 0\} = |1-wy|_+ </script></p>
<p>If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer &amp; Singer.
If <script type="math/tex; ">y_w</script> is the predicted decision for true label and <script type="math/tex; ">y_t</script> is the maximum of the predicted decisions for all other labels,
where predicted decisions are output by decision function, then multiclass hinge loss is defined by:</p>
<p><script type="math/tex; "> L_{Hinge}(y_w, y_t) = max\{1+y_t-y_w,0\}</script></p>
<pre><code class="lang-python"><span class="hljs-comment"># Here a small example demonstrating the use of the hinge_loss function</span>
<span class="hljs-comment"># with a svm classifier in a binary class problem:</span>
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> svm
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> hinge_loss
X = [[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>]]
y = [<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>]
est = svm.LinearSVC(random_state=<span class="hljs-number">0</span>)
est.fit(X, y)
pred_decision = est.decision_function([[<span class="hljs-number">-2</span>], [<span class="hljs-number">3</span>], [<span class="hljs-number">0.5</span>]])
print(pred_decision)
print(hinge_loss([<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], pred_decision))
</code></pre>
<pre><code>[-2.18173682  2.36360149  0.09093234]
0.303022554204
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># Here is an example demonstrating the use of the hinge_loss function</span>
<span class="hljs-comment"># with a svm classifier in a multiclass problem:</span>
X = np.array([[<span class="hljs-number">0</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>]])
Y = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
labels = np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
est = svm.LinearSVC()
est.fit(X, Y)
pred_decision = est.decision_function([[<span class="hljs-number">-1</span>], [<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>]])
y_true = [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]
hinge_loss(y_true, pred_decision, labels)
</code></pre>
<pre><code>0.56412359941917456
</code></pre><h2 id="&#x7EC3;&#x4E60;&#xFF1A;&#x5C1D;&#x8BD5;&#x5728;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#xFF0C;-&#x753B;&#x51FA;&#x7CFB;&#x6570;&#x7684;&#x53D8;&#x5316;&#xFF0C;&#x4EE5;&#x53CA;&#x6700;&#x7EC8;&#x7684;&#x9884;&#x6D4B;&#x6548;&#x679C;"><a name="&#x7EC3;&#x4E60;&#xFF1A;&#x5C1D;&#x8BD5;&#x5728;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#xFF0C;-&#x753B;&#x51FA;&#x7CFB;&#x6570;&#x7684;&#x53D8;&#x5316;&#xFF0C;&#x4EE5;&#x53CA;&#x6700;&#x7EC8;&#x7684;&#x9884;&#x6D4B;&#x6548;&#x679C;" class="plugin-anchor" href="#&#x7EC3;&#x4E60;&#xFF1A;&#x5C1D;&#x8BD5;&#x5728;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#xFF0C;-&#x753B;&#x51FA;&#x7CFB;&#x6570;&#x7684;&#x53D8;&#x5316;&#xFF0C;&#x4EE5;&#x53CA;&#x6700;&#x7EC8;&#x7684;&#x9884;&#x6D4B;&#x6548;&#x679C;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x7EC3;&#x4E60;&#xFF1A;&#x5C1D;&#x8BD5;&#x5728;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x4E2D;&#x4F7F;&#x7528;&#x6B63;&#x5219;&#x5316;&#x65B9;&#x6CD5;&#xFF0C; &#x753B;&#x51FA;&#x7CFB;&#x6570;&#x7684;&#x53D8;&#x5316;&#xFF0C;&#x4EE5;&#x53CA;&#x6700;&#x7EC8;&#x7684;&#x9884;&#x6D4B;&#x6548;&#x679C;</h2>
<pre><code class="lang-python">

</code></pre>
<script type="text/javascript">var className='atoc';</script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../w1-regression/1w.html" class="navigation navigation-prev " aria-label="Previous page: 回归模型和房价预测">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html" class="navigation navigation-next " aria-label="Next page: 决策树和集成学习">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"感知机和逻辑回归","level":"1.4","depth":1,"next":{"title":"决策树和集成学习","level":"1.5","depth":1,"path":"w3-decision-tree-and-ensemble-learning/3w.md","ref":"w3-decision-tree-and-ensemble-learning/3w.md","articles":[]},"previous":{"title":"回归模型和房价预测","level":"1.3","depth":1,"path":"w1-regression/1w.md","ref":"w1-regression/1w.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","anchors","github","splitter","sharing","atoc","comment"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/lyltj2010/DataMining"},"atoc":{"addClass":true,"className":"atoc"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":true,"instapaper":false,"twitter":true,"vk":false,"weibo":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"comment":{"highlightCommented":true}},"theme":"default","author":"wizardforcel","pdf":{"pageNumbers":true,"fontSize":16,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"数据挖掘开源书","language":"zh","links":{"sidebar":{"数据挖掘开源书":"https://www.gitbook.com/book/wizardforcel/data-mining-book"},"gitbook":true},"gitbook":"*","description":"数据挖掘开源书"},"file":{"path":"w2-preceptron-and-logistic-regression/2w.md","mtime":"2016-12-24T21:25:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-04-29T11:04:29.751Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-atoc/atoc.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-comment/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

