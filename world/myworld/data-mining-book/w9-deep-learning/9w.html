
<!DOCTYPE HTML>
<html lang="zh" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>深度学习 · 数据挖掘开源书</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        <meta name="author" content="wizardforcel">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-atoc/atoc.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-comment/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    
    <link rel="prev" href="../w8-neural-network/8w.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="輸入並搜尋" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://www.gitbook.com/book/wizardforcel/data-mining-book" target="_blank" class="custom-link">数据挖掘开源书</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../w0-introduction/0w.html">
            
                <a href="../w0-introduction/0w.html">
            
                    
                    数据挖掘导论和信贷模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../w1-regression/1w.html">
            
                <a href="../w1-regression/1w.html">
            
                    
                    回归模型和房价预测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../w2-preceptron-and-logistic-regression/2w.html">
            
                <a href="../w2-preceptron-and-logistic-regression/2w.html">
            
                    
                    感知机和逻辑回归
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                <a href="../w3-decision-tree-and-ensemble-learning/3w.html">
            
                    
                    决策树和集成学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../w4-feature-engineering/4w.html">
            
                <a href="../w4-feature-engineering/4w.html">
            
                    
                    特征工程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../w5-tuning-parameter/5w.html">
            
                <a href="../w5-tuning-parameter/5w.html">
            
                    
                    参数调优
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../w6-unsupervised-learning/6w.html">
            
                <a href="../w6-unsupervised-learning/6w.html">
            
                    
                    无监督学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../w7-text-mining/7w.html">
            
                <a href="../w7-text-mining/7w.html">
            
                    
                    文本挖掘
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../w8-neural-network/8w.html">
            
                <a href="../w8-neural-network/8w.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.11" data-path="9w.html">
            
                <a href="9w.html">
            
                    
                    深度学习
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本書使用 GitBook 釋出
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >深度学习</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="sections"><a name="sections" class="plugin-anchor" href="#sections"><i class="fa fa-link" aria-hidden="true"></i></a>Sections</h2>
<ul>
<li><a href="#introduction-to-deep-learning-with-tensorflow">Introduction to Deep Learning with TensorFlow</a><ul>
<li><a href="#what-is-deep-learning">What is Deep Learning?</a></li>
<li><a href="#What-is-TensorFlow">What is TensorFlow</a></li>
<li><a href="#what-is-a-data-flow-graph">What is a Data Flow Graph</a></li>
</ul>
</li>
<li><a href="#tensorflow-examples">TensorFlow examples</a><ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#hello-world">Hello world</a></li>
<li><a href="#basic-operations">Basic Operations</a></li>
</ul>
</li>
<li><a href="#basic-models">Basic Models</a><ul>
<li><a href="#nearest-neighbor">Nearest Neighbor</a></li>
<li><a href="#linear-regression">Linear Regression</a></li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
</ul>
</li>
<li><a href="#neural-networks">Neural Networks</a><ul>
<li><a href="#multilayer-perceptron">Multilayer Perceptron</a></li>
<li><a href="#convolutional-neural-network">Convolutional Neural Network</a></li>
<li><a href="#recurrent-neural-network-lstm">Recurrent Neural Network (LSTM)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br>
<br></p>
<h1 id="introduction-to-deep-learning-with-tensorflow"><a name="introduction-to-deep-learning-with-tensorflow" class="plugin-anchor" href="#introduction-to-deep-learning-with-tensorflow"><i class="fa fa-link" aria-hidden="true"></i></a>Introduction to Deep Learning with TensorFlow</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="what-is-deep-learning"><a name="what-is-deep-learning" class="plugin-anchor" href="#what-is-deep-learning"><i class="fa fa-link" aria-hidden="true"></i></a>What is Deep Learning?</h2>
<p>[<a href="#sections">back to top</a>]</p>
<blockquote>
<p><i>Deep learning is a particular kind of machine learning that achieves great power and &#xFB02;exibility by learning to represent the world as a nested hierarchy of concepts, with each concept de&#xFB01;ned in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.</i></p>
</blockquote>
<div align="right">
  I. Goodfellow, Y. Bengio, and A. Courville, &quot;Deep Learning.&quot; Book in preparation for MIT Press, 2016. <br>
  http://www.deeplearningbook.org/
</div>

<h4 id="representation-learning"><a name="representation-learning" class="plugin-anchor" href="#representation-learning"><i class="fa fa-link" aria-hidden="true"></i></a>Representation Learning</h4>
<blockquote>
<p><i>Use machine learning to discover not only the mapping from representation to output but also the representation itself. </i></p>
</blockquote>
<div align="right">
  I. Goodfellow, Y. Bengio, and A. Courville, &quot;Deep Learning.&quot; Book in preparation for MIT Press, 2016. <br>
  http://www.deeplearningbook.org/
</div>

<p><br>
<br></p>
<h2 id="what-is-tensorflow"><a name="what-is-tensorflow" class="plugin-anchor" href="#what-is-tensorflow"><i class="fa fa-link" aria-hidden="true"></i></a>What is TensorFlow</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p><img src="../figures/tf-logo.png" alt=""></p>
<blockquote>
<p><i>TensorFlow&#x2122; is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google&apos;s Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.</i></p>
</blockquote>
<ul>
<li>A TensorFlow computation is described by a directed <strong>graph</strong>, which is composed of a set on <strong>nodes</strong></li>
<li>Each node represents the instantiation of an <strong>operation</strong></li>
<li>An <strong>operation</strong> represents an abstract computation (e.g., &quot;matrix multiply&quot;, or &quot;add&quot;)</li>
<li>Clients programs interact with the TensorFlow system by creating a <strong>Session</strong></li>
<li>Computations represented as a dataflow graph where <strong>tensors</strong> flow along the graph edges</li>
</ul>
<p><br>
<br></p>
<h2 id="what-is-a-data-flow-graph"><a name="what-is-a-data-flow-graph" class="plugin-anchor" href="#what-is-a-data-flow-graph"><i class="fa fa-link" aria-hidden="true"></i></a>What is a Data Flow Graph</h2>
<p>[<a href="#sections">back to top</a>]</p>
<blockquote>
<p><i> Data flow graphs describe mathematical computation with a directed graph of nodes &amp; edges. Nodes typically implement mathematical operations, but can also represent endpoints to feed in data, push out results, or read/write persistent variables. Edges describe the input/output relationships between nodes. These data edges carry dynamically-sized multidimensional data arrays, or tensors. The flow of tensors through the graph is where TensorFlow gets its name. Nodes are assigned to computational devices and execute asynchronously and in parallel once all the tensors on their incoming edges becomes available.</i></p>
</blockquote>
<p><br>
<br></p>
<h1 id="tensorflow-examples"><a name="tensorflow-examples" class="plugin-anchor" href="#tensorflow-examples"><i class="fa fa-link" aria-hidden="true"></i></a>TensorFlow examples</h1>
<p><img src="../figures/tf_in_one_slide.png" alt=""></p>
<p>[<a href="#sections">back to top</a>]</p>
<h2 id="introduction"><a name="introduction" class="plugin-anchor" href="#introduction"><i class="fa fa-link" aria-hidden="true"></i></a>Introduction</h2>
<p>[<a href="#sections">back to top</a>]</p>
<h3 id="hello-world"><a name="hello-world" class="plugin-anchor" href="#hello-world"><i class="fa fa-link" aria-hidden="true"></i></a>Hello world</h3>
<p>[<a href="#sections">back to top</a>]</p>
<ul>
<li><a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#launching-the-graph-in-a-session" target="_blank">Session</a>: TensorFlow &#x662F;&#x5728; session &#x4E2D;&#x8FD0;&#x884C; computation graph</li>
<li><a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#fetches" target="_blank">Fetches</a>: &#x5728; session &#x4E2D;&#x6267;&#x884C; run()&#xFF0C; &#x53EF;&#x4EE5; fetch &#x5F97;&#x5230; operation &#x7684;&#x7ED3;&#x679C;</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># Simple hello world using TensorFlow</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf


<span class="hljs-comment"># Create a Constant op</span>
<span class="hljs-comment"># The op is added as a node to the default graph.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># The value returned by the constructor represents the output</span>
<span class="hljs-comment"># of the Constant op.</span>

hello = tf.constant(<span class="hljs-string">&apos;Hello, TensorFlow!&apos;</span>)

<span class="hljs-comment"># Start tf session</span>
sess = tf.Session()

<span class="hljs-comment"># Run graph</span>
<span class="hljs-keyword">print</span> sess.run(hello)
</code></pre>
<pre><code>Hello, TensorFlow!
</code></pre><p><br>
<br></p>
<h3 id="basic-operations"><a name="basic-operations" class="plugin-anchor" href="#basic-operations"><i class="fa fa-link" aria-hidden="true"></i></a>Basic Operations</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># Basic constant operations</span>
<span class="hljs-comment"># The value returned by the constructor represents the output</span>
<span class="hljs-comment"># of the Constant op.</span>
a = tf.constant(<span class="hljs-number">2</span>)
b = tf.constant(<span class="hljs-number">3</span>)

<span class="hljs-comment"># Launch the default graph.</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;a=2, b=3&quot;</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Addition with constants: %i&quot;</span> % sess.run(a+b)
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Multiplication with constants: %i&quot;</span> % sess.run(a*b)
</code></pre>
<pre><code>a=2, b=3
Addition with constants: 5
Multiplication with constants: 6
</code></pre><ul>
<li><a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#feeds" target="_blank">Feed</a>: &#x5728; sess.run() &#x4E2D;&#x4F20;&#x5165; feed_dict &#x53C2;&#x6570;&#x53EF;&#x4EE5;&#x5411;&#x5BF9;&#x5E94;&#x7684;&#x8282;&#x70B9;&#x5582;&#x5165;&#x6570;&#x636E;</li>
<li>placeholder &#x4F5C;&#x4E3A;&#x4E00;&#x4E2A;&#x5360;&#x4F4D;&#x7B26;&#xFF0C;&#x662F;&#x6570;&#x636E;&#x8F93;&#x5165;&#x7684;&#x7AEF;&#x70B9;&#xFF0C;&#x5FC5;&#x987B;&#x8981;&#x5728; run() &#x4E2D;&#x5582;&#x5165;&#x6570;&#x636E;</li>
</ul>
<pre><code class="lang-python"><span class="hljs-comment"># Basic Operations with variable as graph input</span>
<span class="hljs-comment"># The value returned by the constructor represents the output</span>
<span class="hljs-comment"># of the Variable op. (define as input when running session)</span>
<span class="hljs-comment"># tf Graph input</span>
a = tf.placeholder(tf.int16)
b = tf.placeholder(tf.int16)
<span class="hljs-comment"># placeholder &#x662F;&#x4FE1;&#x606F;&#x8F93;&#x5165;&#x7684;&#x7AEF;&#x70B9;</span>
<span class="hljs-comment"># &#x5728; sess &#x4E2D;&#x901A;&#x8FC7; feed_dict &#x53C2;&#x6570;&#x6765;&#x5582;&#x5165;&#x6570;&#x636E;</span>

<span class="hljs-comment"># Define some operations</span>
add = tf.add(a, b)
mul = tf.mul(a, b)
<span class="hljs-comment"># &#x8FD9;&#x91CC;&#x5B9A;&#x4E49;&#x7684;&#x64CD;&#x4F5C;&#x662F;&#x8C61;&#x5F81;&#x6027;&#x7684;&#xFF0C;sess.run &#x4E4B;&#x540E;&#x624D;&#x4F1A;&#x771F;&#x6B63;&#x5728;&#x5185;&#x5B58;&#x4E2D;&#x8DD1;&#x8D77;&#x6765;</span>

<span class="hljs-comment"># Launch the default graph.</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    <span class="hljs-comment"># Run every operation with variable input</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Addition with variables: %i&quot;</span> % sess.run(add, feed_dict={a: <span class="hljs-number">2</span>, b: <span class="hljs-number">3</span>})
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Multiplication with variables: %i&quot;</span> % sess.run(mul, feed_dict={a: <span class="hljs-number">2</span>, b: <span class="hljs-number">3</span>})
</code></pre>
<pre><code>Addition with variables: 5
Multiplication with variables: 6
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># ----------------</span>
<span class="hljs-comment"># More in details:</span>
<span class="hljs-comment"># Matrix Multiplication from TensorFlow official tutorial</span>

<span class="hljs-comment"># Create a Constant op that produces a 1x2 matrix.  The op is</span>
<span class="hljs-comment"># added as a node to the default graph.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># The value returned by the constructor represents the output</span>
<span class="hljs-comment"># of the Constant op.</span>
matrix1 = tf.constant([[<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>]])

<span class="hljs-comment"># Create another Constant that produces a 2x1 matrix.</span>
matrix2 = tf.constant([[<span class="hljs-number">2.</span>],[<span class="hljs-number">2.</span>]])

<span class="hljs-comment"># Create a Matmul op that takes &apos;matrix1&apos; and &apos;matrix2&apos; as inputs.</span>
<span class="hljs-comment"># The returned value, &apos;product&apos;, represents the result of the matrix</span>
<span class="hljs-comment"># multiplication.</span>
product = tf.matmul(matrix1, matrix2)

<span class="hljs-comment"># To run the matmul op we call the session &apos;run()&apos; method, passing &apos;product&apos;</span>
<span class="hljs-comment"># which represents the output of the matmul op.  This indicates to the call</span>
<span class="hljs-comment"># that we want to get the output of the matmul op back.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># All inputs needed by the op are run automatically by the session.  They</span>
<span class="hljs-comment"># typically are run in parallel.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># The call &apos;run(product)&apos; thus causes the execution of threes ops in the</span>
<span class="hljs-comment"># graph: the two constants and matmul.</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># The output of the op is returned in &apos;result&apos; as a numpy `ndarray` object.</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    result = sess.run(product)
    <span class="hljs-keyword">print</span> result
</code></pre>
<pre><code>[[ 12.]]
</code></pre><p><br>
<br></p>
<h2 id="basic-models"><a name="basic-models" class="plugin-anchor" href="#basic-models"><i class="fa fa-link" aria-hidden="true"></i></a>Basic Models</h2>
<h3 id="linear-regression"><a name="linear-regression" class="plugin-anchor" href="#linear-regression"><i class="fa fa-link" aria-hidden="true"></i></a>Linear Regression</h3>
<p>[<a href="#sections">back to top</a>]</p>
<p><a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage.html#variables" target="_blank">Variable</a>:</p>
<ul>
<li>variable &#x662F;&#x5728;&#x8BA1;&#x7B97;&#x56FE;&#x4E2D;&#x53EF;&#x8BAD;&#x7EC3;&#x7684;&#x91CF;</li>
<li>&#x5728; session &#x4E2D;&#x5FC5;&#x987B;&#x5148;&#x8981;&#x521D;&#x59CB;&#x5316;</li>
<li>name &#x53C2;&#x6570;&#x53EF;&#x5B9A;&#x4E49; variable &#x5728; graph &#x4E2D;&#x7684;&#x540D;&#x79F0;</li>
</ul>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Parameters</span>
learning_rate = <span class="hljs-number">0.01</span>
training_epochs = <span class="hljs-number">1000</span>
display_step = <span class="hljs-number">100</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Training Data</span>
train_X = np.asarray([<span class="hljs-number">3.3</span>,<span class="hljs-number">4.4</span>,<span class="hljs-number">5.5</span>,<span class="hljs-number">6.71</span>,<span class="hljs-number">6.93</span>,<span class="hljs-number">4.168</span>,<span class="hljs-number">9.779</span>,<span class="hljs-number">6.182</span>,<span class="hljs-number">7.59</span>,<span class="hljs-number">2.167</span>,
                         <span class="hljs-number">7.042</span>,<span class="hljs-number">10.791</span>,<span class="hljs-number">5.313</span>,<span class="hljs-number">7.997</span>,<span class="hljs-number">5.654</span>,<span class="hljs-number">9.27</span>,<span class="hljs-number">3.1</span>])
train_Y = np.asarray([<span class="hljs-number">1.7</span>,<span class="hljs-number">2.76</span>,<span class="hljs-number">2.09</span>,<span class="hljs-number">3.19</span>,<span class="hljs-number">1.694</span>,<span class="hljs-number">1.573</span>,<span class="hljs-number">3.366</span>,<span class="hljs-number">2.596</span>,<span class="hljs-number">2.53</span>,<span class="hljs-number">1.221</span>,
                         <span class="hljs-number">2.827</span>,<span class="hljs-number">3.465</span>,<span class="hljs-number">1.65</span>,<span class="hljs-number">2.904</span>,<span class="hljs-number">2.42</span>,<span class="hljs-number">2.94</span>,<span class="hljs-number">1.3</span>])
n_samples = train_X.shape[<span class="hljs-number">0</span>]
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># tf Graph Input</span>
X = tf.placeholder(<span class="hljs-string">&quot;float&quot;</span>)
Y = tf.placeholder(<span class="hljs-string">&quot;float&quot;</span>)

<span class="hljs-comment"># Set model weights</span>
W = tf.Variable(np.random.randn(), name=<span class="hljs-string">&quot;weight&quot;</span>)
b = tf.Variable(np.random.randn(), name=<span class="hljs-string">&quot;bias&quot;</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Construct a linear model</span>
pred = tf.add(tf.mul(X, W), b)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Mean squared error</span>
cost = tf.reduce_sum(tf.pow(pred-Y, <span class="hljs-number">2</span>))/(<span class="hljs-number">2</span>*n_samples)
<span class="hljs-comment"># Gradient descent</span>
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Initializing the variables</span>
init = tf.initialize_all_variables() <span class="hljs-comment"># &#x5B9A;&#x4E49;&#x521D;&#x59CB;&#x5316;&#x64CD;&#x4F5C;</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Launch the graph</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    sess.run(init)  <span class="hljs-comment"># variable &#x5728; session &#x4E2D;&#x5FC5;&#x987B;&#x5148;&#x521D;&#x59CB;&#x5316;</span>

    <span class="hljs-comment"># Fit all training data</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(training_epochs):
        <span class="hljs-keyword">for</span> (x, y) <span class="hljs-keyword">in</span> zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})  <span class="hljs-comment"># &#x6BCF; run &#x4E00;&#x6B21; optimizer&#xFF0C; &#x8FDB;&#x884C;&#x4E00;&#x6B21;&#x68AF;&#x5EA6;&#x4E0B;&#x964D;</span>

        <span class="hljs-comment">#Display logs per epoch step</span>
        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % display_step == <span class="hljs-number">0</span>:
            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})
            <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Epoch:&quot;</span>, <span class="hljs-string">&apos;%04d&apos;</span> % (epoch+<span class="hljs-number">1</span>), <span class="hljs-string">&quot;cost=&quot;</span>, <span class="hljs-string">&quot;{:.9f}&quot;</span>.format(c), \
                <span class="hljs-string">&quot;W=&quot;</span>, sess.run(W), <span class="hljs-string">&quot;b=&quot;</span>, sess.run(b)

    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Optimization Finished!&quot;</span>
    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Training cost=&quot;</span>, training_cost, <span class="hljs-string">&quot;W=&quot;</span>, sess.run(W), <span class="hljs-string">&quot;b=&quot;</span>, sess.run(b), <span class="hljs-string">&apos;\n&apos;</span>

    <span class="hljs-comment">#Graphic display</span>
    plt.plot(train_X, train_Y, <span class="hljs-string">&apos;ro&apos;</span>, label=<span class="hljs-string">&apos;Original data&apos;</span>)
    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label=<span class="hljs-string">&apos;Fitted line&apos;</span>)
    plt.legend()
    plt.show()
</code></pre>
<pre><code>Epoch: 0050 cost= 0.104400784 W= 0.157368 b= 1.46493
Epoch: 0100 cost= 0.101245113 W= 0.162853 b= 1.42547
Epoch: 0150 cost= 0.098453194 W= 0.168012 b= 1.38836
Epoch: 0200 cost= 0.095982887 W= 0.172864 b= 1.35345
Epoch: 0250 cost= 0.093797326 W= 0.177427 b= 1.32062
Epoch: 0300 cost= 0.091863610 W= 0.18172 b= 1.28975
Epoch: 0350 cost= 0.090152740 W= 0.185757 b= 1.26071
Epoch: 0400 cost= 0.088638850 W= 0.189554 b= 1.23339
Epoch: 0450 cost= 0.087299488 W= 0.193125 b= 1.2077
Epoch: 0500 cost= 0.086114518 W= 0.196483 b= 1.18354
Epoch: 0550 cost= 0.085066028 W= 0.199641 b= 1.16082
Epoch: 0600 cost= 0.084138311 W= 0.202612 b= 1.13945
Epoch: 0650 cost= 0.083317406 W= 0.205406 b= 1.11935
Epoch: 0700 cost= 0.082590967 W= 0.208034 b= 1.10044
Epoch: 0750 cost= 0.081948124 W= 0.210505 b= 1.08266
Epoch: 0800 cost= 0.081379279 W= 0.21283 b= 1.06594
Epoch: 0850 cost= 0.080875866 W= 0.215017 b= 1.05021
Epoch: 0900 cost= 0.080430366 W= 0.217073 b= 1.03542
Epoch: 0950 cost= 0.080036096 W= 0.219007 b= 1.0215
Epoch: 1000 cost= 0.079687178 W= 0.220826 b= 1.00842
Optimization Finished!
Training cost= 0.0796872 W= 0.220826 b= 1.00842
</code></pre><p><img src="output_46_1.png" alt="png"></p>
<p><br>
<br></p>
<h3 id="logistic-regression"><a name="logistic-regression" class="plugin-anchor" href="#logistic-regression"><i class="fa fa-link" aria-hidden="true"></i></a>Logistic Regression</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># Import MINST data</span>
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
mnist = input_data.read_data_sets(<span class="hljs-string">&quot;/tmp/data/&quot;</span>, one_hot=<span class="hljs-keyword">True</span>)
</code></pre>
<pre><code>Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># Parameters</span>
learning_rate = <span class="hljs-number">0.01</span>
training_epochs = <span class="hljs-number">25</span>
batch_size = <span class="hljs-number">100</span>
display_step = <span class="hljs-number">5</span>

<span class="hljs-comment"># tf Graph Input</span>
x = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, <span class="hljs-number">784</span>]) <span class="hljs-comment"># mnist data image of shape 28*28=784</span>
y = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, <span class="hljs-number">10</span>]) <span class="hljs-comment"># 0-9 digits recognition =&gt; 10 classes</span>

<span class="hljs-comment"># Set model weights</span>
W = tf.Variable(tf.zeros([<span class="hljs-number">784</span>, <span class="hljs-number">10</span>]))
b = tf.Variable(tf.zeros([<span class="hljs-number">10</span>]))

<span class="hljs-comment"># Construct model</span>
pred = tf.nn.softmax(tf.matmul(x, W) + b) <span class="hljs-comment"># Softmax</span>

<span class="hljs-comment"># Minimize error using cross entropy</span>
cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=<span class="hljs-number">1</span>))  <span class="hljs-comment"># tf.reduce_mean &#x7C7B;&#x4F3C; np.mean()</span>
<span class="hljs-comment"># Gradient Descent</span>
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

<span class="hljs-comment"># Initializing the variables</span>
init = tf.initialize_all_variables()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Launch the graph</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    sess.run(init)

    <span class="hljs-comment"># Training cycle</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(training_epochs):
        avg_cost = <span class="hljs-number">0.</span>
        total_batch = int(mnist.train.num_examples/batch_size)
        <span class="hljs-comment"># Loop over all batches</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            <span class="hljs-comment"># Fit training using batch data</span>
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,
                                                          y: batch_ys})
            <span class="hljs-comment"># Compute average loss</span>
            avg_cost += c / total_batch
        <span class="hljs-comment"># Display logs per epoch step</span>
        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % display_step == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Epoch:&quot;</span>, <span class="hljs-string">&apos;%04d&apos;</span> % (epoch+<span class="hljs-number">1</span>), <span class="hljs-string">&quot;cost=&quot;</span>, <span class="hljs-string">&quot;{:.9f}&quot;</span>.format(avg_cost)

    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Optimization Finished!&quot;</span>

    <span class="hljs-comment"># Test model</span>
    correct_prediction = tf.equal(tf.argmax(pred, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))
    <span class="hljs-comment"># Calculate accuracy for 3000 examples</span>
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Accuracy:&quot;</span>, accuracy.eval({x: mnist.test.images[:<span class="hljs-number">3000</span>], y: mnist.test.labels[:<span class="hljs-number">3000</span>]})
</code></pre>
<pre><code>Epoch: 0005 cost= 0.465507779
Epoch: 0010 cost= 0.392393045
Epoch: 0015 cost= 0.362739271
Epoch: 0020 cost= 0.345433382
Epoch: 0025 cost= 0.333723887
Optimization Finished!
Accuracy: 0.888333
</code></pre><p><br>
<br></p>
<h2 id="neural-networks"><a name="neural-networks" class="plugin-anchor" href="#neural-networks"><i class="fa fa-link" aria-hidden="true"></i></a>Neural Networks</h2>
<p>[<a href="#sections">back to top</a>]</p>
<h3 id="multilayer-perceptron"><a name="multilayer-perceptron" class="plugin-anchor" href="#multilayer-perceptron"><i class="fa fa-link" aria-hidden="true"></i></a>Multilayer Perceptron</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-comment"># Import MINST data</span>
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
mnist = input_data.read_data_sets(<span class="hljs-string">&quot;/tmp/data/&quot;</span>, one_hot=<span class="hljs-keyword">True</span>)

<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
<pre><code>Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># Parameters</span>
learning_rate = <span class="hljs-number">0.001</span>
training_epochs = <span class="hljs-number">15</span>
batch_size = <span class="hljs-number">100</span>
display_step = <span class="hljs-number">5</span>

<span class="hljs-comment"># Network Parameters</span>
n_hidden_1 = <span class="hljs-number">256</span> <span class="hljs-comment"># 1st layer number of features</span>
n_hidden_2 = <span class="hljs-number">256</span> <span class="hljs-comment"># 2nd layer number of features</span>
n_input = <span class="hljs-number">784</span> <span class="hljs-comment"># MNIST data input (img shape: 28*28)</span>
n_classes = <span class="hljs-number">10</span> <span class="hljs-comment"># MNIST total classes (0-9 digits)</span>

<span class="hljs-comment"># tf Graph input</span>
x = tf.placeholder(<span class="hljs-string">&quot;float&quot;</span>, [<span class="hljs-keyword">None</span>, n_input])
y = tf.placeholder(<span class="hljs-string">&quot;float&quot;</span>, [<span class="hljs-keyword">None</span>, n_classes])
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Create model</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">multilayer_perceptron</span><span class="hljs-params">(x, weights, biases)</span>:</span>
    <span class="hljs-comment"># Hidden layer with RELU activation</span>
    layer_1 = tf.add(tf.matmul(x, weights[<span class="hljs-string">&apos;h1&apos;</span>]), biases[<span class="hljs-string">&apos;b1&apos;</span>])
    layer_1 = tf.nn.relu(layer_1)
    <span class="hljs-comment"># Hidden layer with RELU activation</span>
    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="hljs-string">&apos;h2&apos;</span>]), biases[<span class="hljs-string">&apos;b2&apos;</span>])
    layer_2 = tf.nn.relu(layer_2)
    <span class="hljs-comment"># Output layer with linear activation</span>
    out_layer = tf.matmul(layer_2, weights[<span class="hljs-string">&apos;out&apos;</span>]) + biases[<span class="hljs-string">&apos;out&apos;</span>]
    <span class="hljs-keyword">return</span> out_layer
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Store layers weight &amp; bias</span>
weights = {
    <span class="hljs-string">&apos;h1&apos;</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    <span class="hljs-string">&apos;h2&apos;</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    <span class="hljs-string">&apos;out&apos;</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
}
biases = {
    <span class="hljs-string">&apos;b1&apos;</span>: tf.Variable(tf.random_normal([n_hidden_1])),
    <span class="hljs-string">&apos;b2&apos;</span>: tf.Variable(tf.random_normal([n_hidden_2])),
    <span class="hljs-string">&apos;out&apos;</span>: tf.Variable(tf.random_normal([n_classes]))
}

<span class="hljs-comment"># Construct model</span>
pred = multilayer_perceptron(x, weights, biases)

<span class="hljs-comment"># Define loss and optimizer</span>
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

<span class="hljs-comment"># Initializing the variables</span>
init = tf.initialize_all_variables()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Launch the graph</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    sess.run(init)

    <span class="hljs-comment"># Training cycle</span>
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(training_epochs):
        avg_cost = <span class="hljs-number">0.</span>
        total_batch = int(mnist.train.num_examples/batch_size)
        <span class="hljs-comment"># Loop over all batches</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(total_batch):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            <span class="hljs-comment"># Run optimization op (backprop) and cost op (to get loss value)</span>
            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,
                                                          y: batch_y})
            <span class="hljs-comment"># Compute average loss</span>
            avg_cost += c / total_batch
        <span class="hljs-comment"># Display logs per epoch step</span>
        <span class="hljs-keyword">if</span> epoch % display_step == <span class="hljs-number">0</span>:
            <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Epoch:&quot;</span>, <span class="hljs-string">&apos;%04d&apos;</span> % (epoch+<span class="hljs-number">1</span>), <span class="hljs-string">&quot;cost=&quot;</span>, \
                <span class="hljs-string">&quot;{:.9f}&quot;</span>.format(avg_cost)
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Optimization Finished!&quot;</span>

    <span class="hljs-comment"># Test model</span>
    correct_prediction = tf.equal(tf.argmax(pred, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))
    <span class="hljs-comment"># Calculate accuracy</span>
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="hljs-string">&quot;float&quot;</span>))
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Accuracy:&quot;</span>, accuracy.eval({x: mnist.test.images, y: mnist.test.labels})
</code></pre>
<pre><code>Epoch: 0001 cost= 166.660608705
Epoch: 0006 cost= 9.265776215
Epoch: 0011 cost= 2.211729868
Optimization Finished!
Accuracy: 0.9434
</code></pre><p><br>
<br></p>
<h3 id="convolutional-neural-network"><a name="convolutional-neural-network" class="plugin-anchor" href="#convolutional-neural-network"><i class="fa fa-link" aria-hidden="true"></i></a>Convolutional Neural Network</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># Import MNIST data</span>
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
mnist = input_data.read_data_sets(<span class="hljs-string">&quot;/tmp/data/&quot;</span>, one_hot=<span class="hljs-keyword">True</span>)
</code></pre>
<pre><code>Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># Parameters</span>
learning_rate = <span class="hljs-number">0.001</span>
training_iters = <span class="hljs-number">200000</span>
batch_size = <span class="hljs-number">128</span>
display_step = <span class="hljs-number">200</span>

<span class="hljs-comment"># Network Parameters</span>
n_input = <span class="hljs-number">784</span> <span class="hljs-comment"># MNIST data input (img shape: 28*28)</span>
n_classes = <span class="hljs-number">10</span> <span class="hljs-comment"># MNIST total classes (0-9 digits)</span>
dropout = <span class="hljs-number">0.75</span> <span class="hljs-comment"># Dropout, probability to keep units</span>

<span class="hljs-comment"># tf Graph input</span>
x = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, n_input])
y = tf.placeholder(tf.float32, [<span class="hljs-keyword">None</span>, n_classes])
keep_prob = tf.placeholder(tf.float32) <span class="hljs-comment">#dropout (keep probability)</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Create some wrappers for simplicity</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv2d</span><span class="hljs-params">(x, W, b, strides=<span class="hljs-number">1</span>)</span>:</span>
    <span class="hljs-comment"># Conv2D wrapper, with bias and relu activation</span>
    x = tf.nn.conv2d(x, W, strides=[<span class="hljs-number">1</span>, strides, strides, <span class="hljs-number">1</span>], padding=<span class="hljs-string">&apos;SAME&apos;</span>)
    x = tf.nn.bias_add(x, b)
    <span class="hljs-keyword">return</span> tf.nn.relu(x)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">maxpool2d</span><span class="hljs-params">(x, k=<span class="hljs-number">2</span>)</span>:</span>
    <span class="hljs-comment"># MaxPool2D wrapper</span>
    <span class="hljs-keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="hljs-number">1</span>, k, k, <span class="hljs-number">1</span>], strides=[<span class="hljs-number">1</span>, k, k, <span class="hljs-number">1</span>],
                          padding=<span class="hljs-string">&apos;SAME&apos;</span>)


<span class="hljs-comment"># Create model</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv_net</span><span class="hljs-params">(x, weights, biases, dropout)</span>:</span>
    <span class="hljs-comment"># Reshape input picture</span>
    x = tf.reshape(x, shape=[<span class="hljs-number">-1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>])

    <span class="hljs-comment"># Convolution Layer</span>
    conv1 = conv2d(x, weights[<span class="hljs-string">&apos;wc1&apos;</span>], biases[<span class="hljs-string">&apos;bc1&apos;</span>])
    <span class="hljs-comment"># Max Pooling (down-sampling)</span>
    conv1 = maxpool2d(conv1, k=<span class="hljs-number">2</span>)

    <span class="hljs-comment"># Convolution Layer</span>
    conv2 = conv2d(conv1, weights[<span class="hljs-string">&apos;wc2&apos;</span>], biases[<span class="hljs-string">&apos;bc2&apos;</span>])
    <span class="hljs-comment"># Max Pooling (down-sampling)</span>
    conv2 = maxpool2d(conv2, k=<span class="hljs-number">2</span>)

    <span class="hljs-comment"># Fully connected layer</span>
    <span class="hljs-comment"># Reshape conv2 output to fit fully connected layer input</span>
    fc1 = tf.reshape(conv2, [<span class="hljs-number">-1</span>, weights[<span class="hljs-string">&apos;wd1&apos;</span>].get_shape().as_list()[<span class="hljs-number">0</span>]])
    fc1 = tf.add(tf.matmul(fc1, weights[<span class="hljs-string">&apos;wd1&apos;</span>]), biases[<span class="hljs-string">&apos;bd1&apos;</span>])
    fc1 = tf.nn.relu(fc1)
    <span class="hljs-comment"># Apply Dropout</span>
    fc1 = tf.nn.dropout(fc1, dropout)

    <span class="hljs-comment"># Output, class prediction</span>
    out = tf.add(tf.matmul(fc1, weights[<span class="hljs-string">&apos;out&apos;</span>]), biases[<span class="hljs-string">&apos;out&apos;</span>])
    <span class="hljs-keyword">return</span> out
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Store layers weight &amp; bias</span>
weights = {
    <span class="hljs-comment"># 5x5 conv, 1 input, 32 outputs</span>
    <span class="hljs-string">&apos;wc1&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>])),
    <span class="hljs-comment"># 5x5 conv, 32 inputs, 64 outputs</span>
    <span class="hljs-string">&apos;wc2&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>])),
    <span class="hljs-comment"># fully connected, 7*7*64 inputs, 1024 outputs</span>
    <span class="hljs-string">&apos;wd1&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">7</span>*<span class="hljs-number">7</span>*<span class="hljs-number">64</span>, <span class="hljs-number">1024</span>])),
    <span class="hljs-comment"># 1024 inputs, 10 outputs (class prediction)</span>
    <span class="hljs-string">&apos;out&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">1024</span>, n_classes]))
}

biases = {
    <span class="hljs-string">&apos;bc1&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">32</span>])),
    <span class="hljs-string">&apos;bc2&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">64</span>])),
    <span class="hljs-string">&apos;bd1&apos;</span>: tf.Variable(tf.random_normal([<span class="hljs-number">1024</span>])),
    <span class="hljs-string">&apos;out&apos;</span>: tf.Variable(tf.random_normal([n_classes]))
}

<span class="hljs-comment"># Construct model</span>
pred = conv_net(x, weights, biases, keep_prob)

<span class="hljs-comment"># Define loss and optimizer</span>
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

<span class="hljs-comment"># Evaluate model</span>
correct_pred = tf.equal(tf.argmax(pred, <span class="hljs-number">1</span>), tf.argmax(y, <span class="hljs-number">1</span>))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

<span class="hljs-comment"># Initializing the variables</span>
init = tf.initialize_all_variables()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Launch the graph</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    sess.run(init)
    step = <span class="hljs-number">1</span>
    <span class="hljs-comment"># Keep training until reach max iterations</span>
    <span class="hljs-keyword">while</span> step * batch_size &lt; training_iters:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        <span class="hljs-comment"># Run optimization op (backprop)</span>
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,
                                       keep_prob: dropout})
        <span class="hljs-keyword">if</span> step % display_step == <span class="hljs-number">0</span>:
            <span class="hljs-comment"># Calculate batch loss and accuracy</span>
            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,
                                                              y: batch_y,
                                                              keep_prob: <span class="hljs-number">1.</span>})
            <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Iter &quot;</span> + str(step*batch_size) + <span class="hljs-string">&quot;, Minibatch Loss= &quot;</span> + \
                  <span class="hljs-string">&quot;{:.6f}&quot;</span>.format(loss) + <span class="hljs-string">&quot;, Training Accuracy= &quot;</span> + \
                  <span class="hljs-string">&quot;{:.5f}&quot;</span>.format(acc)
        step += <span class="hljs-number">1</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Optimization Finished!&quot;</span>

    <span class="hljs-comment"># Calculate accuracy for 256 mnist test images</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Testing Accuracy:&quot;</span>, \
        sess.run(accuracy, feed_dict={x: mnist.test.images[:<span class="hljs-number">256</span>],
                                      y: mnist.test.labels[:<span class="hljs-number">256</span>],
                                      keep_prob: <span class="hljs-number">1.</span>})
</code></pre>
<pre><code>Iter 25600, Minibatch Loss= 1453.969238, Training Accuracy= 0.87500
Iter 51200, Minibatch Loss= 0.000000, Training Accuracy= 1.00000
Iter 76800, Minibatch Loss= 836.579651, Training Accuracy= 0.91406
Iter 102400, Minibatch Loss= 265.563293, Training Accuracy= 0.96875
Iter 128000, Minibatch Loss= 120.997910, Training Accuracy= 0.99219
Iter 153600, Minibatch Loss= 29.434311, Training Accuracy= 0.97656
Iter 179200, Minibatch Loss= 248.191101, Training Accuracy= 0.98438
Optimization Finished!
Testing Accuracy: 0.984375
</code></pre><p><br>
<br></p>
<h3 id="recurrent-neural-network-lstm"><a name="recurrent-neural-network-lstm" class="plugin-anchor" href="#recurrent-neural-network-lstm"><i class="fa fa-link" aria-hidden="true"></i></a>Recurrent Neural Network LSTM</h3>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.python.ops <span class="hljs-keyword">import</span> rnn, rnn_cell
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-comment"># Import MINST data</span>
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
mnist = input_data.read_data_sets(<span class="hljs-string">&quot;/tmp/data/&quot;</span>, one_hot=<span class="hljs-keyword">True</span>)
</code></pre>
<pre><code>Extracting /tmp/data/train-images-idx3-ubyte.gz
Extracting /tmp/data/train-labels-idx1-ubyte.gz
Extracting /tmp/data/t10k-images-idx3-ubyte.gz
Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># Parameters</span>
learning_rate = <span class="hljs-number">0.001</span>
training_iters = <span class="hljs-number">100000</span>
batch_size = <span class="hljs-number">128</span>
display_step = <span class="hljs-number">100</span>

<span class="hljs-comment"># Network Parameters</span>
n_input = <span class="hljs-number">28</span> <span class="hljs-comment"># MNIST data input (img shape: 28*28)</span>
n_steps = <span class="hljs-number">28</span> <span class="hljs-comment"># timesteps</span>
n_hidden = <span class="hljs-number">128</span> <span class="hljs-comment"># hidden layer num of features</span>
n_classes = <span class="hljs-number">10</span> <span class="hljs-comment"># MNIST total classes (0-9 digits)</span>

<span class="hljs-comment"># tf Graph input</span>
x = tf.placeholder(<span class="hljs-string">&quot;float&quot;</span>, [<span class="hljs-keyword">None</span>, n_steps, n_input])
y = tf.placeholder(<span class="hljs-string">&quot;float&quot;</span>, [<span class="hljs-keyword">None</span>, n_classes])

<span class="hljs-comment"># Define weights</span>
weights = {
    <span class="hljs-string">&apos;out&apos;</span>: tf.Variable(tf.random_normal([n_hidden, n_classes]))
}
biases = {
    <span class="hljs-string">&apos;out&apos;</span>: tf.Variable(tf.random_normal([n_classes]))
}
</code></pre>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">RNN</span><span class="hljs-params">(x, weights, biases)</span>:</span>

    <span class="hljs-comment"># Prepare data shape to match `rnn` function requirements</span>
    <span class="hljs-comment"># Current data input shape: (batch_size, n_steps, n_input)</span>
    <span class="hljs-comment"># Required shape: &apos;n_steps&apos; tensors list of shape (batch_size, n_input)</span>

    <span class="hljs-comment"># Permuting batch_size and n_steps</span>
    x = tf.transpose(x, [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>])
    <span class="hljs-comment"># Reshaping to (n_steps*batch_size, n_input)</span>
    x = tf.reshape(x, [<span class="hljs-number">-1</span>, n_input])
    <span class="hljs-comment"># Split to get a list of &apos;n_steps&apos; tensors of shape (batch_size, n_input)</span>
    x = tf.split(<span class="hljs-number">0</span>, n_steps, x)

    <span class="hljs-comment"># Define a lstm cell with tensorflow</span>
    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=<span class="hljs-number">1.0</span>, state_is_tuple=<span class="hljs-keyword">True</span>)

    <span class="hljs-comment"># Get lstm cell output</span>
    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)

    <span class="hljs-comment"># Linear activation, using rnn inner loop last output</span>
    <span class="hljs-keyword">return</span> tf.matmul(outputs[<span class="hljs-number">-1</span>], weights[<span class="hljs-string">&apos;out&apos;</span>]) + biases[<span class="hljs-string">&apos;out&apos;</span>]

pred = RNN(x, weights, biases)

<span class="hljs-comment"># Define loss and optimizer</span>
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

<span class="hljs-comment"># Evaluate model</span>
correct_pred = tf.equal(tf.argmax(pred,<span class="hljs-number">1</span>), tf.argmax(y,<span class="hljs-number">1</span>))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

<span class="hljs-comment"># Initializing the variables</span>
init = tf.initialize_all_variables()
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Launch the graph</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    sess.run(init)
    step = <span class="hljs-number">1</span>
    <span class="hljs-comment"># Keep training until reach max iterations</span>
    <span class="hljs-keyword">while</span> step * batch_size &lt; training_iters:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        <span class="hljs-comment"># Reshape data to get 28 seq of 28 elements</span>
        batch_x = batch_x.reshape((batch_size, n_steps, n_input))
        <span class="hljs-comment"># Run optimization op (backprop)</span>
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
        <span class="hljs-keyword">if</span> step % display_step == <span class="hljs-number">0</span>:
            <span class="hljs-comment"># Calculate batch accuracy</span>
            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})
            <span class="hljs-comment"># Calculate batch loss</span>
            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})
            <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Iter &quot;</span> + str(step*batch_size) + <span class="hljs-string">&quot;, Minibatch Loss= &quot;</span> + \
                  <span class="hljs-string">&quot;{:.6f}&quot;</span>.format(loss) + <span class="hljs-string">&quot;, Training Accuracy= &quot;</span> + \
                  <span class="hljs-string">&quot;{:.5f}&quot;</span>.format(acc)
        step += <span class="hljs-number">1</span>
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Optimization Finished!&quot;</span>

    <span class="hljs-comment"># Calculate accuracy for 128 mnist test images</span>
    test_len = <span class="hljs-number">128</span>
    test_data = mnist.test.images[:test_len].reshape((<span class="hljs-number">-1</span>, n_steps, n_input))
    test_label = mnist.test.labels[:test_len]
    <span class="hljs-keyword">print</span> <span class="hljs-string">&quot;Testing Accuracy:&quot;</span>, \
        sess.run(accuracy, feed_dict={x: test_data, y: test_label})
</code></pre>
<pre><code>Iter 12800, Minibatch Loss= 0.716396, Training Accuracy= 0.75000
Iter 25600, Minibatch Loss= 0.367348, Training Accuracy= 0.87500
Iter 38400, Minibatch Loss= 0.164333, Training Accuracy= 0.92969
Iter 51200, Minibatch Loss= 0.143476, Training Accuracy= 0.92969
Iter 64000, Minibatch Loss= 0.193304, Training Accuracy= 0.96094
Iter 76800, Minibatch Loss= 0.202645, Training Accuracy= 0.90625
Iter 89600, Minibatch Loss= 0.056868, Training Accuracy= 0.98438
Optimization Finished!
Testing Accuracy: 0.992188
</code></pre><h2 id="&#x4F7F;&#x7528;&#x5377;&#x79EF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x505A;&#x6587;&#x672C;&#x5206;&#x7C7B;"><a name="&#x4F7F;&#x7528;&#x5377;&#x79EF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x505A;&#x6587;&#x672C;&#x5206;&#x7C7B;" class="plugin-anchor" href="#&#x4F7F;&#x7528;&#x5377;&#x79EF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x505A;&#x6587;&#x672C;&#x5206;&#x7C7B;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x4F7F;&#x7528;&#x5377;&#x79EF;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x505A;&#x6587;&#x672C;&#x5206;&#x7C7B;</h2>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> os <span class="hljs-keyword">import</span> path
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> codecs
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> cPickle <span class="hljs-keyword">import</span> dump,load
<span class="hljs-comment">#dump(df, open(&apos;data/tmdf.pickle&apos;, &apos;wb&apos;))</span>
df = load(open(<span class="hljs-string">&apos;data/tmdf.pickle&apos;</span>,<span class="hljs-string">&apos;rb&apos;</span>))
</code></pre>
<pre><code class="lang-python">df.head()
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>txt</th>
      <th>seg_word</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>&#x672C;&#x62A5;&#x8BB0;&#x8005;&#x9648;&#x96EA;&#x9891;&#x5B9E;&#x4E60;&#x8BB0;&#x8005;&#x5510;&#x7FD4;&#x53D1;&#x81EA;&#x4E0A;&#x6D77;\r\n&#x3000;&#x3000;&#x4E00;&#x5BB6;&#x521A;&#x521A;&#x6210;&#x7ACB;&#x4E24;&#x5E74;&#x7684;&#x7F51;&#x7EDC;&#x652F;&#x4ED8;&#x516C;&#x53F8;&#xFF0C;&#x5B83;&#x7684;&#x76EE;&#x6807;&#x662F;...</td>
      <td>&#x672C;&#x62A5;&#x8BB0;&#x8005; &#x9648;&#x96EA;&#x9891; &#x5B9E;&#x4E60; &#x8BB0;&#x8005; &#x5510;&#x7FD4; &#x53D1;&#x81EA; &#x4E0A;&#x6D77; \r\n &#x3000; &#x3000; &#x4E00;&#x5BB6; &#x521A;&#x521A; &#x6210;&#x7ACB; ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>&#x8BC1;&#x5238;&#x901A;&#xFF1A;&#x767E;&#x8054;&#x80A1;&#x4EFD;&#x672A;&#x6765;5&#x5E74;&#x6709;&#x80FD;&#x529B;&#x4FDD;&#x6301;&#x9AD8;&#x901F;&#x589E;&#x957F;\r\n\r\n    &#x6DF1;&#x5EA6;&#x62A5;&#x544A; &#x6743;&#x5A01;&#x5185;&#x53C2;...</td>
      <td>&#x8BC1;&#x5238; &#x901A; &#xFF1A; &#x767E;&#x8054; &#x80A1;&#x4EFD; &#x672A;&#x6765; 5 &#x5E74; &#x6709; &#x80FD;&#x529B; &#x4FDD;&#x6301;&#x9AD8;&#x901F; &#x589E;&#x957F; \r\n ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>5&#x6708;09&#x65E5;&#x6D88;&#x606F;&#x5FEB;&#x8BC4;\r\n\r\n    &#x6DF1;&#x5EA6;&#x62A5;&#x544A; &#x6743;&#x5A01;&#x5185;&#x53C2; &#x6765;&#x81EA;&#x201C;&#x8BC1;&#x5238;&#x901A;&#x201D;www....</td>
      <td>5 &#x6708; 09 &#x65E5; &#x6D88;&#x606F; &#x5FEB;&#x8BC4; \r\n \r\n         &#x6DF1;&#x5EA6; &#x62A5;&#x544A;...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>5&#x6708;09&#x65E5;&#x6D88;&#x606F;&#x5FEB;&#x8BC4;\r\n\r\n    &#x6DF1;&#x5EA6;&#x62A5;&#x544A; &#x6743;&#x5A01;&#x5185;&#x53C2; &#x6765;&#x81EA;&#x201C;&#x8BC1;&#x5238;&#x901A;&#x201D;www....</td>
      <td>5 &#x6708; 09 &#x65E5; &#x6D88;&#x606F; &#x5FEB;&#x8BC4; \r\n \r\n         &#x6DF1;&#x5EA6; &#x62A5;&#x544A;...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>5&#x6708;09&#x65E5;&#x6D88;&#x606F;&#x5FEB;&#x8BC4;\r\n\r\n    &#x6DF1;&#x5EA6;&#x62A5;&#x544A; &#x6743;&#x5A01;&#x5185;&#x53C2; &#x6765;&#x81EA;&#x201C;&#x8BC1;&#x5238;&#x901A;&#x201D;www....</td>
      <td>5 &#x6708; 09 &#x65E5; &#x6D88;&#x606F; &#x5FEB;&#x8BC4; \r\n \r\n         &#x6DF1;&#x5EA6; &#x62A5;&#x544A;...</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"><span class="hljs-comment">#  &#x6587;&#x672C;&#x6574;&#x7406;&#x5B8C;&#x6BD5;&#xFF0C;&#x540E;&#x9762;&#x5EFA;&#x6A21;&#x9700;&#x8981;&#x5C06;&#x8BCD;&#x6C47;&#x8F6C;&#x6210;&#x6570;&#x5B57;&#x7F16;&#x53F7;&#xFF0C;&#x53EF;&#x4EE5;&#x4EBA;&#x5DE5;&#x8F6C;&#xFF0C;&#x4E5F;&#x53EF;&#x4EE5;&#x8BA9;keras&#x8F6C;</span>
textraw = df.seg_word.values.tolist()
textraw = [line.encode(<span class="hljs-string">&apos;utf-8&apos;</span>) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> textraw] <span class="hljs-comment"># &#x9700;&#x8981;&#x5B58;&#x4E3A;str&#x624D;&#x80FD;&#x88AB;keras&#x4F7F;&#x7528;</span>
</code></pre>
<pre><code class="lang-python">maxfeatures = <span class="hljs-number">50000</span> <span class="hljs-comment"># &#x53EA;&#x9009;&#x62E9;&#x6700;&#x91CD;&#x8981;&#x7684;&#x8BCD;</span>
<span class="hljs-keyword">from</span> keras.preprocessing.text <span class="hljs-keyword">import</span> Tokenizer
token = Tokenizer(nb_words=maxfeatures)
token.fit_on_texts(textraw) <span class="hljs-comment">#&#x5982;&#x679C;&#x6587;&#x672C;&#x8F83;&#x5927;&#x53EF;&#x4EE5;&#x4F7F;&#x7528;&#x6587;&#x672C;&#x6D41;</span>
text_seq = token.texts_to_sequences(textraw)
</code></pre>
<pre><code class="lang-python">np.median([len(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> text_seq]) <span class="hljs-comment">#  &#x6BCF;&#x6761;&#x65B0;&#x95FB;&#x5E73;&#x5747;400&#x4E2A;&#x8BCD;&#x6C47;</span>
</code></pre>
<pre><code>498.0
</code></pre><pre><code class="lang-python">y = df.label.values <span class="hljs-comment"># &#x5B9A;&#x4E49;&#x597D;&#x6807;&#x7B7E;</span>
nb_classes = len(np.unique(y))
print(nb_classes)
</code></pre>
<pre><code>9
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> absolute_import
<span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> RMSprop
<span class="hljs-keyword">from</span> keras.preprocessing <span class="hljs-keyword">import</span> sequence
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers.core <span class="hljs-keyword">import</span> Dense, Dropout, Activation, Flatten
<span class="hljs-keyword">from</span> keras.layers.embeddings <span class="hljs-keyword">import</span> Embedding
<span class="hljs-keyword">from</span> keras.layers.convolutional <span class="hljs-keyword">import</span> Convolution1D, MaxPooling1D
<span class="hljs-keyword">from</span> keras.layers.recurrent  <span class="hljs-keyword">import</span> SimpleRNN, GRU, LSTM
<span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> EarlyStopping
</code></pre>
<pre><code class="lang-python">maxlen = <span class="hljs-number">600</span> <span class="hljs-comment"># &#x5B9A;&#x4E49;&#x6587;&#x672C;&#x6700;&#x5927;&#x957F;&#x5EA6;</span>
batch_size = <span class="hljs-number">32</span> <span class="hljs-comment"># &#x6279;&#x6B21;</span>
word_dim = <span class="hljs-number">100</span> <span class="hljs-comment"># &#x8BCD;&#x5411;&#x91CF;&#x7EF4;&#x5EA6;</span>
nb_filter = <span class="hljs-number">200</span>  <span class="hljs-comment"># &#x5377;&#x79EF;&#x6838;&#x4E2A;&#x6570;</span>
filter_length = <span class="hljs-number">10</span> <span class="hljs-comment"># &#x5377;&#x79EF;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</span>
hidden_dims = <span class="hljs-number">50</span>  <span class="hljs-comment"># &#x9690;&#x85CF;&#x5C42;&#x795E;&#x7ECF;&#x5143;&#x4E2A;&#x6570;</span>
nb_epoch = <span class="hljs-number">10</span>      <span class="hljs-comment"># &#x8BAD;&#x7EC3;&#x8FED;&#x4EE3;&#x6B21;&#x6570;</span>
pool_length = <span class="hljs-number">50</span>   <span class="hljs-comment"># &#x6C60;&#x5316;&#x7A97;&#x53E3;&#x5927;&#x5C0F;</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split
train_X, test_X, train_y, test_y = train_test_split(text_seq, y , train_size=<span class="hljs-number">0.8</span>, random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x8F6C;&#x4E3A;&#x7B49;&#x957F;&#x77E9;&#x9635;&#xFF0C;&#x957F;&#x5EA6;&#x4E3A;maxlen</span>
print(<span class="hljs-string">&quot;Pad sequences (samples x time)&quot;</span>)
X_train = sequence.pad_sequences(train_X, maxlen=maxlen,padding=<span class="hljs-string">&apos;post&apos;</span>, truncating=<span class="hljs-string">&apos;post&apos;</span>)
X_test = sequence.pad_sequences(test_X, maxlen=maxlen,padding=<span class="hljs-string">&apos;post&apos;</span>, truncating=<span class="hljs-string">&apos;post&apos;</span>)
print(<span class="hljs-string">&apos;X_train shape:&apos;</span>, X_train.shape)
print(<span class="hljs-string">&apos;X_test shape:&apos;</span>, X_test.shape)
</code></pre>
<pre><code>Pad sequences (samples x time)
(&apos;X_train shape:&apos;, (14328, 600))
(&apos;X_test shape:&apos;, (3582, 600))
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x5C06;y&#x7684;&#x683C;&#x5F0F;&#x5C55;&#x5F00;&#x6210;one-hot</span>
<span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils
Y_train = np_utils.to_categorical(train_y, nb_classes)
Y_test = np_utils.to_categorical(test_y, nb_classes)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># for version bug</span>
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
tf.python.control_flow_ops = tf
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># CNN &#x6A21;&#x578B;</span>
print(<span class="hljs-string">&apos;Build model...&apos;</span>)
model = Sequential()

<span class="hljs-comment"># &#x8BCD;&#x5411;&#x91CF;&#x5D4C;&#x5165;&#x5C42;&#xFF0C;&#x8F93;&#x5165;&#xFF1A;&#x8BCD;&#x5178;&#x5927;&#x5C0F;&#xFF0C;&#x8BCD;&#x5411;&#x91CF;&#x5927;&#x5C0F;&#xFF0C;&#x6587;&#x672C;&#x957F;&#x5EA6;</span>
model.add(Embedding(maxfeatures, word_dim,input_length=maxlen,dropout=<span class="hljs-number">0.25</span>))
model.add(Convolution1D(nb_filter=nb_filter,
                        filter_length=filter_length,
                        border_mode=<span class="hljs-string">&quot;valid&quot;</span>,
                        activation=<span class="hljs-string">&quot;relu&quot;</span>))
<span class="hljs-comment"># &#x6C60;&#x5316;&#x5C42;</span>
model.add(MaxPooling1D(pool_length=pool_length))
model.add(Flatten())
<span class="hljs-comment"># &#x5168;&#x8FDE;&#x63A5;&#x5C42;</span>
model.add(Dense(hidden_dims))
model.add(Dropout(<span class="hljs-number">0.25</span>))
model.add(Activation(<span class="hljs-string">&apos;relu&apos;</span>))
model.add(Dense(nb_classes))
model.add(Activation(<span class="hljs-string">&apos;softmax&apos;</span>))
model.compile(loss=<span class="hljs-string">&apos;categorical_crossentropy&apos;</span>, optimizer=<span class="hljs-string">&apos;rmsprop&apos;</span>,metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])
</code></pre>
<pre><code>Build model...
</code></pre><pre><code class="lang-python">earlystop = EarlyStopping(monitor=<span class="hljs-string">&apos;val_loss&apos;</span>, patience=<span class="hljs-number">1</span>, verbose=<span class="hljs-number">1</span>)
result = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,
            validation_split=<span class="hljs-number">0.1</span>, callbacks=[earlystop])
</code></pre>
<pre><code>/Users/xiaokai/anaconda/envs/tensorflow/lib/python2.7/site-packages/keras/models.py:603: UserWarning: The &quot;show_accuracy&quot; argument is deprecated, instead you should pass the &quot;accuracy&quot; metric to the model at compile time:
`model.compile(optimizer, loss, metrics=[&quot;accuracy&quot;])`
  warnings.warn(&apos;The &quot;show_accuracy&quot; argument is deprecated, &apos;
/Users/xiaokai/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  &quot;Converting sparse IndexedSlices to a dense Tensor of unknown shape. &quot;


Train on 12895 samples, validate on 1433 samples
Epoch 1/10
12895/12895 [==============================] - 704s - loss: 1.4306 - val_loss: 0.5532
Epoch 2/10
12895/12895 [==============================] - 7724s - loss: 0.4912 - val_loss: 0.4273
Epoch 3/10
12895/12895 [==============================] - 765s - loss: 0.3511 - val_loss: 0.4003
Epoch 4/10
12895/12895 [==============================] - 807s - loss: 0.2571 - val_loss: 0.4114
Epoch 5/10
12864/12895 [============================&gt;.] - ETA: 5s - loss: 0.1971 Epoch 00004: early stopping
12895/12895 [==============================] - 2285s - loss: 0.1968 - val_loss: 0.4415
</code></pre><pre><code class="lang-python">score = earlystop.model.evaluate(X_test, Y_test, batch_size=batch_size)
print(<span class="hljs-string">&apos;Test score:&apos;</span>, score)
classes = earlystop.model.predict_classes(X_test, batch_size=batch_size)
acc = np_utils.accuracy(classes, test_y) <span class="hljs-comment"># &#x8981;&#x7528;&#x6CA1;&#x6709;&#x8F6C;&#x6362;&#x524D;&#x7684;y</span>
print(<span class="hljs-string">&apos;Test accuracy:&apos;</span>, acc)
</code></pre>
<pre><code>3582/3582 [==============================] - 73s
(&apos;Test score:&apos;, 0.4292584941548252)
3582/3582 [==============================] - 73s
(&apos;Test accuracy:&apos;, 0.89056393076493578)
</code></pre><pre><code class="lang-python">

</code></pre>
<script type="text/javascript">var className='atoc';</script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../w8-neural-network/8w.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page: 神经网络">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"深度学习","level":"1.11","depth":1,"previous":{"title":"神经网络","level":"1.10","depth":1,"path":"w8-neural-network/8w.md","ref":"w8-neural-network/8w.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","anchors","github","splitter","sharing","atoc","comment"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/lyltj2010/DataMining"},"atoc":{"addClass":true,"className":"atoc"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":true,"instapaper":false,"twitter":true,"vk":false,"weibo":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"comment":{"highlightCommented":true}},"theme":"default","author":"wizardforcel","pdf":{"pageNumbers":true,"fontSize":16,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"数据挖掘开源书","language":"zh","links":{"sidebar":{"数据挖掘开源书":"https://www.gitbook.com/book/wizardforcel/data-mining-book"},"gitbook":true},"gitbook":"*","description":"数据挖掘开源书"},"file":{"path":"w9-deep-learning/9w.md","mtime":"2016-12-24T21:25:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-04-29T11:04:29.751Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-atoc/atoc.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-comment/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

