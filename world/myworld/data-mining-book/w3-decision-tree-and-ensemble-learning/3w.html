
<!DOCTYPE HTML>
<html lang="zh" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>决策树和集成学习 · 数据挖掘开源书</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.2">
        <meta name="author" content="wizardforcel">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-anchors/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-atoc/atoc.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-comment/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="../w4-feature-engineering/4w.html" />
    
    
    <link rel="prev" href="../w2-preceptron-and-logistic-regression/2w.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="輸入並搜尋" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    
    
        
        <li>
            <a href="https://www.gitbook.com/book/wizardforcel/data-mining-book" target="_blank" class="custom-link">数据挖掘开源书</a>
        </li>
    
    

    
    <li class="divider"></li>
    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    简介
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../w0-introduction/0w.html">
            
                <a href="../w0-introduction/0w.html">
            
                    
                    数据挖掘导论和信贷模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../w1-regression/1w.html">
            
                <a href="../w1-regression/1w.html">
            
                    
                    回归模型和房价预测
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../w2-preceptron-and-logistic-regression/2w.html">
            
                <a href="../w2-preceptron-and-logistic-regression/2w.html">
            
                    
                    感知机和逻辑回归
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.5" data-path="3w.html">
            
                <a href="3w.html">
            
                    
                    决策树和集成学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../w4-feature-engineering/4w.html">
            
                <a href="../w4-feature-engineering/4w.html">
            
                    
                    特征工程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../w5-tuning-parameter/5w.html">
            
                <a href="../w5-tuning-parameter/5w.html">
            
                    
                    参数调优
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../w6-unsupervised-learning/6w.html">
            
                <a href="../w6-unsupervised-learning/6w.html">
            
                    
                    无监督学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../w7-text-mining/7w.html">
            
                <a href="../w7-text-mining/7w.html">
            
                    
                    文本挖掘
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../w8-neural-network/8w.html">
            
                <a href="../w8-neural-network/8w.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../w9-deep-learning/9w.html">
            
                <a href="../w9-deep-learning/9w.html">
            
                    
                    深度学习
            
                </a>
            

            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本書使用 GitBook 釋出
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >决策树和集成学习</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="sections"><a name="sections" class="plugin-anchor" href="#sections"><i class="fa fa-link" aria-hidden="true"></i></a>Sections</h2>
<ul>
<li><a href="#decision-trees-learning">Decision trees learning</a><ul>
<li><a href="#building-a-decision-tree">Building a decision tree</a></li>
<li><a href="#visualize-a-decision-tree">Visualize a decision tree</a></li>
<li><a href="#different-impurity-criteria">Different impurity criteria</a></li>
<li><a href="#implement-a-binary-decision-tree-in-python">Implement a binary decision tree in python</a></li>
</ul>
</li>
<li><p><a href="#combining-weak-to-strong-learners-via-random-forests">Combining weak to strong learners via random forests</a></p>
</li>
<li><p><a href="#learning-with-ensembles">Learning with ensembles</a></p>
<ul>
<li><a href="#majority-vote-classifier">Majority vote classifier</a><ul>
<li><a href="#votingclassifier-in-sklearn">VotingClassifier in Sklearn</a></li>
<li><a href="#combining-different-algorithms-for-classification-with-majority-vote">Combining different algorithms for classification with majority vote</a></li>
<li><a href="#evaluating-the-ensemble-classifier">Evaluating the ensemble classifier</a></li>
</ul>
</li>
<li><a href="#bagging----building-an-ensemble-of-classifiers-from-bootstrap-samples">Bagging -- Building an ensemble of classifiers from bootstrap samples</a></li>
<li><a href="#leveraging-of-weak-learners-via-adaptive-boosting">Leveraging weak learners via adaptive boosting</a></li>
</ul>
</li>
<li><a href="#algorithm-implementation">Algorithm implementation</a></li>
</ul>
<p><br>
<br></p>
<h1 id="decision-trees-learning"><a name="decision-trees-learning" class="plugin-anchor" href="#decision-trees-learning"><i class="fa fa-link" aria-hidden="true"></i></a>Decision trees learning</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>Here we&apos;ll explore a class of algorithms based on decision trees.
Decision trees at their root are extremely intuitive.  They
encode a series of &quot;if&quot; and &quot;else&quot; choices, similar to how a person might make a decision.
However, which questions to ask, and how to proceed for each answer is entirely learned from the data.</p>
<p>For example, if you wanted to create a guide to identifying an animal found in nature, you
might ask the following series of questions:</p>
<ul>
<li>Is the animal bigger or smaller than a meter long?<ul>
<li><em>bigger</em>: does the animal have horns?<ul>
<li><em>yes</em>: are the horns longer than ten centimeters?</li>
<li><em>no</em>: is the animal wearing a collar</li>
</ul>
</li>
<li><em>smaller</em>: does the animal have two or four legs?<ul>
<li><em>two</em>: does the animal have wings?</li>
<li><em>four</em>: does the animal have a bushy tail?</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>and so on.  This binary splitting of questions is the essence of a decision tree.</p>
<p>One of the main benefit of tree-based models is that they require little preprocessing of the data.
They can work with variables of different types (continuous and discrete) and are invariant to scaling of the features.</p>
<p>Another benefit is that tree-based models are what is called &quot;nonparametric&quot;, which means they don&apos;t have a fix set of parameters to learn. Instead, a tree model can become more and more flexible, if given more data.
In other words, the number of free parameters grows with the number of samples and is not fixed, as for example in linear models.</p>
<p><br>
<br></p>
<h2 id="building-a-decision-tree"><a name="building-a-decision-tree" class="plugin-anchor" href="#building-a-decision-tree"><i class="fa fa-link" aria-hidden="true"></i></a>Building a decision tree</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler

iris = datasets.load_iris()
X = iris.data[:, [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(
         X, y, test_size=<span class="hljs-number">0.3</span>, random_state=<span class="hljs-number">0</span>)

sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train) <span class="hljs-comment"># standardize by mean &amp; std</span>
X_test_std = sc.transform(X_test)
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_decision_regions</span><span class="hljs-params">(X, y, classifier, test_idx=None, resolution=<span class="hljs-number">0.02</span>)</span>:</span>

    <span class="hljs-comment"># setup marker generator and color map</span>
    markers = (<span class="hljs-string">&apos;s&apos;</span>, <span class="hljs-string">&apos;x&apos;</span>, <span class="hljs-string">&apos;o&apos;</span>, <span class="hljs-string">&apos;^&apos;</span>, <span class="hljs-string">&apos;v&apos;</span>)
    colors = (<span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;lightgreen&apos;</span>, <span class="hljs-string">&apos;gray&apos;</span>, <span class="hljs-string">&apos;cyan&apos;</span>)
    cmap = ListedColormap(colors[:len(np.unique(y))])

    <span class="hljs-comment"># plot the decision surface</span>
    x1_min, x1_max = X[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
    x2_min, x2_max = X[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, X[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                         np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=<span class="hljs-number">0.4</span>, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    <span class="hljs-comment"># plot all samples</span>
    <span class="hljs-keyword">for</span> idx, cl <span class="hljs-keyword">in</span> enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, <span class="hljs-number">0</span>], y=X[y == cl, <span class="hljs-number">1</span>],
                    alpha=<span class="hljs-number">0.8</span>, c=cmap(idx),
                    marker=markers[idx], label=cl)

    <span class="hljs-comment"># highlight test samples</span>
    <span class="hljs-keyword">if</span> test_idx:
        X_test, y_test = X[test_idx, :], y[test_idx]   
        plt.scatter(X_test[:, <span class="hljs-number">0</span>], X_test[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&apos;&apos;</span>, 
                alpha=<span class="hljs-number">1.0</span>, linewidth=<span class="hljs-number">1</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, 
                s=<span class="hljs-number">55</span>, label=<span class="hljs-string">&apos;test set&apos;</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier
<span class="hljs-comment"># max depth 3 using entropy for impurofy</span>
tree = DecisionTreeClassifier(criterion=<span class="hljs-string">&apos;entropy&apos;</span>, max_depth=<span class="hljs-number">3</span>, random_state=<span class="hljs-number">0</span>)
tree.fit(X_train, y_train)

X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))
plot_decision_regions(X_combined, y_combined, 
                      classifier=tree, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))

plt.xlabel(<span class="hljs-string">&apos;petal length [cm]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [cm]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/decision_tree_decision.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_11_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="visualize-a-decision-tree"><a name="visualize-a-decision-tree" class="plugin-anchor" href="#visualize-a-decision-tree"><i class="fa fa-link" aria-hidden="true"></i></a>Visualize a decision tree</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> export_graphviz
<span class="hljs-comment"># export tree as .dot file, install  GraphViz to transfer the format</span>
export_graphviz(tree, 
                out_file=<span class="hljs-string">&apos;tree.dot&apos;</span>, 
                feature_names=[<span class="hljs-string">&apos;petal length&apos;</span>, <span class="hljs-string">&apos;petal width&apos;</span>])
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># pip install pydotplus</span>
<span class="hljs-keyword">import</span> pydotplus
<span class="hljs-keyword">from</span> sklearn.externals.six <span class="hljs-keyword">import</span> StringIO
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> Image  
dot_data = StringIO()  
export_graphviz(tree, out_file=dot_data,  
                feature_names=[<span class="hljs-string">&apos;petal length&apos;</span>, <span class="hljs-string">&apos;petal width&apos;</span>],  
                class_names=iris.target_names,  
                filled=<span class="hljs-keyword">True</span>, rounded=<span class="hljs-keyword">True</span>,  
                special_characters=<span class="hljs-keyword">True</span>)  
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())
</code></pre>
<p><img src="output_16_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="different-impurity-criteria"><a name="different-impurity-criteria" class="plugin-anchor" href="#different-impurity-criteria"><i class="fa fa-link" aria-hidden="true"></i></a>Different impurity criteria</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gini</span><span class="hljs-params">(p)</span>:</span>
    <span class="hljs-keyword">return</span> (p)*(<span class="hljs-number">1</span> - (p)) + (<span class="hljs-number">1</span>-p)*(<span class="hljs-number">1</span> - (<span class="hljs-number">1</span>-p))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">entropy</span><span class="hljs-params">(p)</span>:</span>
    <span class="hljs-keyword">return</span> - p*np.log2(p) - (<span class="hljs-number">1</span> - p)*np.log2((<span class="hljs-number">1</span> - p))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">error</span><span class="hljs-params">(p)</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> - np.max([p, <span class="hljs-number">1</span> - p])

x = np.arange(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">0.01</span>)

ent = [entropy(p) <span class="hljs-keyword">if</span> p != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> x]
sc_ent = [e*<span class="hljs-number">0.5</span> <span class="hljs-keyword">if</span> e <span class="hljs-keyword">else</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> ent]
err = [error(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x]


fig = plt.figure()
ax = plt.subplot(<span class="hljs-number">111</span>)
<span class="hljs-keyword">for</span> i, lab, ls, c, <span class="hljs-keyword">in</span> zip([ent, sc_ent, gini(x), err], 
                  [<span class="hljs-string">&apos;Entropy&apos;</span>, <span class="hljs-string">&apos;Entropy (scaled)&apos;</span>, 
                   <span class="hljs-string">&apos;Gini Impurity&apos;</span>, <span class="hljs-string">&apos;Misclassification Error&apos;</span>],
                  [<span class="hljs-string">&apos;-&apos;</span>, <span class="hljs-string">&apos;-&apos;</span>, <span class="hljs-string">&apos;--&apos;</span>, <span class="hljs-string">&apos;-.&apos;</span>],
                  [<span class="hljs-string">&apos;black&apos;</span>, <span class="hljs-string">&apos;lightgray&apos;</span>, <span class="hljs-string">&apos;red&apos;</span>, <span class="hljs-string">&apos;green&apos;</span>, <span class="hljs-string">&apos;cyan&apos;</span>]):
    line = ax.plot(x, i, label=lab, linestyle=ls, lw=<span class="hljs-number">2</span>, color=c)


<span class="hljs-comment"># &#x753B;&#x56FE;</span>
ax.legend(loc=<span class="hljs-string">&apos;upper center&apos;</span>, bbox_to_anchor=(<span class="hljs-number">0.5</span>, <span class="hljs-number">1.15</span>),
          ncol=<span class="hljs-number">3</span>, fancybox=<span class="hljs-keyword">True</span>, shadow=<span class="hljs-keyword">False</span>)

ax.axhline(y=<span class="hljs-number">0.5</span>, linewidth=<span class="hljs-number">1</span>, color=<span class="hljs-string">&apos;k&apos;</span>, linestyle=<span class="hljs-string">&apos;--&apos;</span>)
ax.axhline(y=<span class="hljs-number">1.0</span>, linewidth=<span class="hljs-number">1</span>, color=<span class="hljs-string">&apos;k&apos;</span>, linestyle=<span class="hljs-string">&apos;--&apos;</span>)
plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">1.1</span>])
plt.xlabel(<span class="hljs-string">&apos;p(i=1)&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Impurity Index&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/impurity.png&apos;, dpi=300, bbox_inches=&apos;tight&apos;)</span>
</code></pre>
<p><img src="output_20_0.png" alt="png"></p>
<p><br>
<br></p>
<h2 id="implement-a-binary-decision-tree-in-python"><a name="implement-a-binary-decision-tree-in-python" class="plugin-anchor" href="#implement-a-binary-decision-tree-in-python"><i class="fa fa-link" aria-hidden="true"></i></a>Implement a binary decision tree in python</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-comment"># &#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x7C7B;&#xFF0C;&#x6765;&#x8868;&#x5F81;&#x4E8C;&#x5206;&#x6811;&#x7684;&#x7ED3;&#x6784;</span>
<span class="hljs-comment"># Tree &#x91CC;&#x7684;&#x5C5E;&#x6027;&#x9664;&#x4E86;&#x5305;&#x62EC;&#x5DE6;&#x53F3;&#x8282;&#x70B9;&#x7684; Tree &#x4E4B;&#x5916;&#xFF0C;&#x8FD8;&#x6709;&#x6B64;&#x8282;&#x70B9;&#x4E2D;&#x5305;&#x62EC;&#x6570;&#x636E;&#x7684;&#x6807;&#x7B7E;&#x53CA;&#x5176;&#x71B5;&#x503C;&#xFF0C;&#x7136;&#x540E;&#x8FD8;&#x6709;&#x8981;&#x5207;&#x5206; feature &#x7684; idex</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Tree</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; Binary Tree&quot;&quot;&quot;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, labels, split_idx=None,
                 children_left=None, children_right=None)</span>:</span>
        self.children_left = children_left
        self.children_right = children_right
        self.labels = labels
        self.split_idx = split_idx
        self.entropy = calc_entropy(self.labels)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(self)</span>:</span>
        most_freq = np.bincount(self.labels).argmax()  <span class="hljs-comment"># find most frequent element</span>
        <span class="hljs-keyword">return</span> most_freq

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span><span class="hljs-params">(self, level=<span class="hljs-number">0</span>)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot; make it easy to visualize a tree&quot;&quot;&quot;</span>
        prefix = <span class="hljs-string">&quot;\t&quot;</span> * level
        string = prefix + <span class="hljs-string">&quot;entropy = {}, labels = {}, [0s, 1s] = {}\n&quot;</span>.format(
            self.entropy, self.labels, np.bincount(self.labels, minlength=<span class="hljs-number">2</span>))
        <span class="hljs-keyword">if</span> self.split_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
            string += prefix + <span class="hljs-string">&quot;split on Column {}\n&quot;</span>.format(self.split_idx)
            string += prefix + <span class="hljs-string">&quot;True:\n&quot;</span>
            string += self.children_left.__repr__(level+<span class="hljs-number">1</span>)
            string += prefix + <span class="hljs-string">&quot;False:\n&quot;</span>
            string += self.children_right.__repr__(level+<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> string


<span class="hljs-comment"># &#x8BA1;&#x7B97;&#x4E00;&#x7EC4;&#x6570;&#x636E;&#x91CC;&#x7684;&#x71B5;&#x503C;</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_entropy</span><span class="hljs-params">(labels)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; calculate entropy from an array of labels&quot;&quot;&quot;</span>
    size = float(len(labels))
    cnt = Counter(labels)
    entropy = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> set(labels):
        prob = cnt[label] / size
        entropy += <span class="hljs-number">-1</span> * prob * np.log2(prob)
    <span class="hljs-keyword">return</span> entropy


<span class="hljs-comment"># &#x4E0D;&#x540C;&#x7684;&#x51B3;&#x7B56;&#x6811;&#x7B97;&#x6CD5; (&#x5982; ID3, C4.5, CART &#x7B49;) &#x4F1A;&#x7528;&#x4E0D;&#x540C;&#x7684;&#x6807;&#x51C6;&#x6765;&#x9009;&#x62E9;&#x8981;&#x5207;&#x5206;&#x7684; feature</span>
<span class="hljs-comment"># &#x8FD9;&#x91CC;&#x4F7F;&#x7528;&#x7684;&#x662F; Information Gain&#xFF0C;&#x5373; feature &#x5207;&#x5206;&#x524D;&#x540E;&#x7684;&#x71B5;&#x503C;&#x53D8;&#x5316;</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_best_feature_to_split</span><span class="hljs-params">(features, labels)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; choose the best split feature which maximize information gain &quot;&quot;&quot;</span>
    num_features = features.shape[<span class="hljs-number">1</span>]
    base_entropy = calc_entropy(labels)

    best_info_gain = <span class="hljs-number">0</span> 
    best_feature = <span class="hljs-keyword">None</span>

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_features):
        new_entropy = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> value <span class="hljs-keyword">in</span> [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]:
            new_labels = labels[features[:, i] == value]
            weight = float(len(new_labels)) / len(labels)
            new_entropy += weight * calc_entropy(new_labels)
        info_gain = base_entropy - new_entropy
        <span class="hljs-keyword">if</span> info_gain &gt; best_info_gain:
            best_info_gain = info_gain
            best_feature = i
    <span class="hljs-keyword">return</span> best_feature


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_decision_tree</span><span class="hljs-params">(features, labels, 
                         current_depth=<span class="hljs-number">0</span>, max_depth=<span class="hljs-number">10</span>)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; recursively create tree &quot;&quot;&quot;</span>
    tree = Tree(labels)

    <span class="hljs-comment"># define stop condition</span>
    <span class="hljs-comment"># stop when all data in this node are from the same class</span>
    <span class="hljs-keyword">if</span> len(set(labels)) == <span class="hljs-number">1</span>:
        <span class="hljs-keyword">return</span> tree
    <span class="hljs-comment"># stop when max_depth are reached</span>
    <span class="hljs-keyword">if</span> current_depth &gt;= max_depth:
        <span class="hljs-keyword">return</span> tree

    <span class="hljs-comment"># split on the best feature found</span>
    best_feature = choose_best_feature_to_split(features, labels)
    <span class="hljs-keyword">if</span> best_feature <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
        <span class="hljs-keyword">return</span> tree

    <span class="hljs-comment"># recursively build subtrees</span>
    msk = (features[:, best_feature] == <span class="hljs-number">1</span>)
    tree.split_idx = best_feature
    tree.children_left = create_decision_tree(
        features[msk], labels[msk], current_depth+<span class="hljs-number">1</span>)
    tree.children_right = create_decision_tree(
        features[~msk], labels[~msk], current_depth+<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> tree
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># &#x6A21;&#x62DF;&#x4E00;&#x7EC4;&#x6570;&#x636E;&#x6765;&#x6D4B;&#x8BD5;</span>
data = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
                 [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                 [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],
                 [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
labels = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>])


tree = create_decision_tree(data, labels)
</code></pre>
<pre><code class="lang-python">tree
</code></pre>
<pre><code>entropy = 0.811278124459, labels = [1 0 0 0], [0s, 1s] = [3 1]
split on Column 0
True:
    entropy = 1.0, labels = [1 0], [0s, 1s] = [1 1]
    split on Column 1
    True:
        entropy = 0.0, labels = [0], [0s, 1s] = [1 0]
    False:
        entropy = 0.0, labels = [1], [0s, 1s] = [0 1]
False:
    entropy = 0.0, labels = [0 0], [0s, 1s] = [2 0]
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># &#x904D;&#x5386;&#x4E8C;&#x5206;&#x6811;&#xFF0C;&#x6765;&#x5F97;&#x5230;&#x5206;&#x7C7B;&#x9884;&#x6D4B;</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_classify</span><span class="hljs-params">(tree, data_row)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot; prediction for new data&quot;&quot;&quot;</span>
    <span class="hljs-keyword">if</span> tree.split_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>: <span class="hljs-comment"># if it&apos;s a leaf</span>
        <span class="hljs-keyword">return</span> tree.predict()

    split_idx = tree.split_idx
    <span class="hljs-keyword">if</span> data_row[split_idx]:  <span class="hljs-comment"># if True for split condition</span>
        <span class="hljs-keyword">return</span> _classify(tree.children_left, data_row)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> _classify(tree.children_right, data_row)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">classify</span><span class="hljs-params">(tree, data)</span>:</span>
    data = np.array(data)
    num_row = data.shape[<span class="hljs-number">0</span>]
    results = np.empty(shape=num_row)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_row):
        results[i] = _classify(tree, data[i, :])
    <span class="hljs-keyword">return</span> results
</code></pre>
<pre><code class="lang-python">new_data = [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
            [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]

classify(tree, new_data)
</code></pre>
<pre><code>array([ 0.,  1.])
</code></pre><p><br>
<br></p>
<h1 id="combining-weak-to-strong-learners-via-random-forests"><a name="combining-weak-to-strong-learners-via-random-forests" class="plugin-anchor" href="#combining-weak-to-strong-learners-via-random-forests"><i class="fa fa-link" aria-hidden="true"></i></a>Combining weak to strong learners via random forests</h1>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x53EF;&#x4EE5;&#x770B;&#x505A;&#x662F; ensemble of decision trees, &#x5C06;&#x5F31;&#x7684;&#x6A21;&#x578B;&#x7ED3;&#x5408;&#x5728;&#x4E00;&#x8D77;&#x53D8;&#x6210;&#x5F3A;&#x6A21;&#x578B;. &#x66F4;&#x6613;&#x6269;&#x5C55;, &#x4E14;&#x8F83;&#x5C11;&#x4F1A; overfitting.</p>
<ol>
<li>draw a random <strong>bootstrap</strong> sample of size n (with replacement)</li>
<li>grow decision tree from bootstrap sample, at each node:<ul>
<li>randomly select d features without replacement</li>
<li>split node using feature that provides best split</li>
</ul>
</li>
<li>repeat 1 &amp; 2 k times.</li>
<li>aggregate the prediction by each tree to assign the class label by <strong>majority vote</strong></li>
</ol>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-comment"># from 10 decision trees, n_jobs &#x503C;&#x4F7F;&#x7528; cpu &#x4E2A;&#x6570;</span>
forest = RandomForestClassifier(criterion=<span class="hljs-string">&apos;entropy&apos;</span>,
                                n_estimators=<span class="hljs-number">10</span>, 
                                random_state=<span class="hljs-number">1</span>,
                                n_jobs=<span class="hljs-number">2</span>)
forest.fit(X_train, y_train)

plot_decision_regions(X_combined, y_combined, 
                      classifier=forest, test_idx=range(<span class="hljs-number">105</span>,<span class="hljs-number">150</span>))

plt.xlabel(<span class="hljs-string">&apos;petal length [cm]&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;petal width [cm]&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/random_forest.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_33_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="learning-with-ensembles"><a name="learning-with-ensembles" class="plugin-anchor" href="#learning-with-ensembles"><i class="fa fa-link" aria-hidden="true"></i></a>Learning with ensembles</h1>
<p>&#x5C06;&#x4E00;&#x7CFB;&#x5217;&#x5206;&#x7C7B;&#x5668;&#x96C6;&#x5408;&#x8D77;&#x6765;, &#x53D6;&#x591A;&#x6570;&#x4E3A;&#x5206;&#x7C7B;&#x7ED3;&#x679C;</p>
<p>Build powerful models from weak learners that learn from their mistakes</p>
<p>ensemble method &#x5C31;&#x662F;&#x8BB2;&#x591A;&#x4E2A;&#x4E0D;&#x540C;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x96C6;&#x5408;&#x7EC4;&#x5408;&#x4E3A;&#x4E00;&#x4E2A;&#x5927;&#x7684;&#x5206;&#x7C7B;&#x5668;. &#x9009;&#x62E9;&#x6700;&#x7EC8;&#x7ED3;&#x679C;&#x662F;&#x4EE5; majority voting</p>
<p>&#x5373;&#x4F7F;&#x6BCF;&#x4E2A;&#x5355;&#x72EC;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x9519;&#x8BEF;&#x7387;&#x8F83;&#x9AD8;, &#x4F46;&#x5C06;&#x591A;&#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x7EC4;&#x5408;&#x4E4B;&#x540E;, &#x9519;&#x8BEF;&#x7387;&#x5C31;&#x4F1A;&#x5927;&#x5927;&#x964D;&#x4F4E;</p>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x5047;&#x8BBE;&#x6211;&#x4EEC;&#x7EC4;&#x5408;&#x4E86; n &#x4E2A;&#x5206;&#x7C7B;&#x5668;&#xFF0C;&#x5B83;&#x4EEC;&#x7684;&#x9519;&#x8BEF;&#x7387;&#x90FD;&#x4E3A; <script type="math/tex; "> \varepsilon </script>, &#x5404;&#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x4E4B;&#x95F4;&#x72EC;&#x7ACB;&#x3002;<br>&#x5219;&#x8FD9; n &#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x91CC;, &#x591A;&#x4E8E; k &#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x5206;&#x7C7B;&#x9519;&#x8BEF;&#x7684;&#x6982;&#x7387;&#x4E3A;
<script type="math/tex; "> P(y \geq k) = \sum^n_k \binom{n}{k} \varepsilon^k (1-\varepsilon)^{n-k}</script></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> scipy.misc <span class="hljs-keyword">import</span> comb
<span class="hljs-keyword">import</span> math

<span class="hljs-comment"># emsemble error rate</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ensemble_error</span><span class="hljs-params">(n_classifier, error)</span>:</span>
    k_start = int(math.ceil(n_classifier / <span class="hljs-number">2.0</span>))
    probs = [comb(n_classifier, k) * error**k * 
               (<span class="hljs-number">1</span>-error)**(n_classifier - k) 
             <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(k_start, n_classifier + <span class="hljs-number">1</span>)]
    <span class="hljs-keyword">return</span> sum(probs)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># 11&#x4E2A;&#x5206;&#x7C7B;&#x5668;, &#x6BCF;&#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x7684; error rate &#x662F;0.25&#x7684;&#x8BDD;, &#x901A;&#x8FC7; combinator &#x4E4B;&#x540E;&#x7684; error rate</span>
ensemble_error(n_classifier=<span class="hljs-number">11</span>, error=<span class="hljs-number">0.25</span>)
</code></pre>
<pre><code>0.034327507019042969
</code></pre><pre><code class="lang-python"><span class="hljs-comment"># ensemble error &#x548C; base error &#x7684;&#x5173;&#x7CFB;</span>
error_range = np.arange(<span class="hljs-number">0.0</span>, <span class="hljs-number">1.01</span>, <span class="hljs-number">0.01</span>)
ens_errors = [ensemble_error(n_classifier=<span class="hljs-number">11</span>, error=error) 
              <span class="hljs-keyword">for</span> error <span class="hljs-keyword">in</span> error_range]

plt.plot(error_range, ens_errors, 
         label=<span class="hljs-string">&apos;Ensemble error&apos;</span>, linewidth=<span class="hljs-number">2</span>)

plt.plot(error_range, error_range, 
         linestyle=<span class="hljs-string">&apos;--&apos;</span>, label=<span class="hljs-string">&apos;Base error&apos;</span>,linewidth=<span class="hljs-number">2</span>)

plt.xlabel(<span class="hljs-string">&apos;Base error&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;Base/Ensemble error&apos;</span>)
plt.legend(loc=<span class="hljs-string">&apos;upper left&apos;</span>)
plt.grid()
plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/ensemble_err.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_40_0.png" alt="png"></p>
<p><script type="math/tex; ">\varepsilon</script> &lt; 0.5 &#x65F6;, emsemble error &#x90FD;&#x8981;&#x4F4E;&#x4E8E; base error, <script type="math/tex; ">\varepsilon</script> &gt; 0.5 &#x65F6;, emsemble error &#x5C31;&#x4F1A;&#x5927;&#x4E8E; base error</p>
<p><br>
<br></p>
<h1 id="majority-vote-classifier"><a name="majority-vote-classifier" class="plugin-anchor" href="#majority-vote-classifier"><i class="fa fa-link" aria-hidden="true"></i></a>Majority vote classifier</h1>
<p>combine different classication algorithms associated with individual weights for confidence</p>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x5F53;&#x591A;&#x4E2A;&#x5206;&#x7C7B;&#x5668; C &#x62E5;&#x6709;&#x76F8;&#x540C;&#x6743;&#x91CD;&#x65F6;&#xFF0C;ensemble &#x7ED9;&#x51FA;&#x7684;&#x9884;&#x6D4B; <script type="math/tex; ">\hat y</script> &#x4E3A;&#x4F17;&#x6570;&#xFF1A;
<script type="math/tex; "> \hat y = mode\{C_1(x), C_2(x), \dotso, C_m(x)\} </script></p>
<p>&#x82E5;&#x5206;&#x7C7B;&#x5668; <script type="math/tex; ">C_j</script> &#x5BF9;&#x5E94;&#x4E0D;&#x540C;&#x7684;&#x6743;&#x91CD; <script type="math/tex; ">w_j</script>, &#x5219;
<script type="math/tex; "> \hat y = \arg\max_i \sum^m_{j=1}w_j P_{ij} </script>
&#x5176;&#x4E2D; <script type="math/tex; ">P_{ij}</script> &#x662F; <script type="math/tex; ">C_j</script> &#x9884;&#x6D4B;&#x7ED3;&#x679C;&#x4E3A; i &#x7684;&#x6982;&#x7387;</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
np.argmax(np.bincount([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
                      weights=[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>]))

<span class="hljs-comment"># np.argmax: returns the indices of the maximum values along an axis.</span>
<span class="hljs-comment"># np.bincount: Count number of occurrences of each value in array of non-negative ints</span>
</code></pre>
<pre><code>1
</code></pre><pre><code class="lang-python">np.bincount([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
                      weights=[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>])
</code></pre>
<pre><code>array([ 0.4,  0.6])
</code></pre><pre><code class="lang-python">ex = np.array([[<span class="hljs-number">0.9</span>, <span class="hljs-number">0.1</span>],  <span class="hljs-comment"># C1 &#x7684;&#x9884;&#x6D4B;&#x7ED3;&#x679C;</span>
               [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>],  <span class="hljs-comment"># C2 &#x7684;&#x9884;&#x6D4B;&#x7ED3;&#x679C;</span>
               [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]]) <span class="hljs-comment"># C3 &#x7684;&#x9884;&#x6D4B;&#x7ED3;&#x679C;</span>

p = np.average(ex, 
               axis=<span class="hljs-number">0</span>, 
               weights=[<span class="hljs-number">0.2</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.6</span>])  <span class="hljs-comment"># C1, C2, C3 &#x7684;&#x6743;&#x91CD;</span>
p
</code></pre>
<pre><code>array([ 0.58,  0.42])
</code></pre><p><script type="math/tex; ">p(i_0| x) = 0.58</script>
<script type="math/tex; ">p(i_1 | x) = 0.42</script>
<script type="math/tex; ">\hat y = \arg\max_i [p(i_0 | x), p(i_1 | x)] = 0</script></p>
<pre><code class="lang-python">np.argmax(p)
</code></pre>
<pre><code>0
</code></pre><p><br>
<br></p>
<h2 id="votingclassifier-in-sklearn"><a name="votingclassifier-in-sklearn" class="plugin-anchor" href="#votingclassifier-in-sklearn"><i class="fa fa-link" aria-hidden="true"></i></a>VotingClassifier in Sklearn</h2>
<p>&#x4F7F;&#x7528; Sklearn &#x4E2D;&#x81EA;&#x5E26;&#x7684; <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html" target="_blank">VotingClassifier</a></p>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-keyword">from</span> sklearn.naive_bayes <span class="hljs-keyword">import</span> GaussianNB
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifier

<span class="hljs-comment"># &#x4F7F;&#x7528; 3 &#x4E2A;&#x5206;&#x7C7B;&#x5668;</span>
clf1 = LogisticRegression(random_state=<span class="hljs-number">1</span>)
clf2 = RandomForestClassifier(random_state=<span class="hljs-number">1</span>)
clf3 = GaussianNB()

<span class="hljs-comment"># &#x751F;&#x6210;&#x6570;&#x636E;</span>
X = np.array([[<span class="hljs-number">-1</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>], [<span class="hljs-number">-3</span>, <span class="hljs-number">-2</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>]])
y = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])

<span class="hljs-comment"># voting=&apos;hard&apos;, uses predicted class labels for majority rule voting.</span>
eclf1 = VotingClassifier(estimators=[
        (<span class="hljs-string">&apos;lr&apos;</span>, clf1), (<span class="hljs-string">&apos;rf&apos;</span>, clf2), (<span class="hljs-string">&apos;gnb&apos;</span>, clf3)], voting=<span class="hljs-string">&apos;hard&apos;</span>)
eclf1 = eclf1.fit(X, y)
print(eclf1.predict(X))

<span class="hljs-comment"># voting=&apos;soft&apos;, predicts the class label based on the argmax of the sums of the predicted probalities </span>
eclf2 = VotingClassifier(estimators=[
        (<span class="hljs-string">&apos;lr&apos;</span>, clf1), (<span class="hljs-string">&apos;rf&apos;</span>, clf2), (<span class="hljs-string">&apos;gnb&apos;</span>, clf3)],
        voting=<span class="hljs-string">&apos;soft&apos;</span>)
eclf2 = eclf2.fit(X, y)
print(eclf2.predict(X))

<span class="hljs-comment"># add weight</span>
eclf3 = VotingClassifier(estimators=[
       (<span class="hljs-string">&apos;lr&apos;</span>, clf1), (<span class="hljs-string">&apos;rf&apos;</span>, clf2), (<span class="hljs-string">&apos;gnb&apos;</span>, clf3)],
       voting=<span class="hljs-string">&apos;soft&apos;</span>, weights=[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])
eclf3 = eclf3.fit(X, y)
print(eclf3.predict(X))
</code></pre>
<pre><code>[1 1 1 2 2 2]
[1 1 1 2 2 2]
[1 1 1 2 2 2]
</code></pre><p><br>
<br></p>
<h2 id="combining-different-algorithms-for-classification-with-majority-vote"><a name="combining-different-algorithms-for-classification-with-majority-vote" class="plugin-anchor" href="#combining-different-algorithms-for-classification-with-majority-vote"><i class="fa fa-link" aria-hidden="true"></i></a>Combining different algorithms for classification with majority vote</h2>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder

<span class="hljs-comment"># load iris data</span>
iris = datasets.load_iris()
X, y = iris.data[<span class="hljs-number">50</span>:, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]], iris.target[<span class="hljs-number">50</span>:]

st = StandardScaler()
X = st.fit_transform(X)

le = LabelEncoder()
y = le.fit_transform(y)

<span class="hljs-comment"># 50% train, 50% test</span>
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=<span class="hljs-number">0.5</span>, random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression
<span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier
<span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier
<span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> cross_val_score

clf1 = LogisticRegression(C=<span class="hljs-number">0.01</span>, random_state=<span class="hljs-number">42</span>)
clf2 = KNeighborsClassifier(n_neighbors=<span class="hljs-number">1</span>)
clf3 = DecisionTreeClassifier(max_depth=<span class="hljs-number">1</span>, random_state=<span class="hljs-number">42</span>)

clf_labels = [<span class="hljs-string">&apos;Logistic Regression&apos;</span>, <span class="hljs-string">&apos;KNN&apos;</span>, <span class="hljs-string">&apos;Decision Tree&apos;</span>]
all_clf = [clf1, clf2, clf3]

print(<span class="hljs-string">&apos;10-fold cross validation:\n&apos;</span>)
<span class="hljs-keyword">for</span> clf, label <span class="hljs-keyword">in</span> zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train,
                             cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&apos;roc_auc&apos;</span>)
    print(<span class="hljs-string">&quot;ROC AUC: %0.2f (+/- %0.2f) [%s]&quot;</span>
             % (scores.mean(), scores.std(), label))
</code></pre>
<pre><code>10-fold cross validation:

ROC AUC: 0.93 (+/- 0.15) [Logistic Regression]
ROC AUC: 0.93 (+/- 0.10) [KNN]
ROC AUC: 0.92 (+/- 0.15) [Decision Tree]
</code></pre><pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> VotingClassifier

mv_clf = VotingClassifier(
    estimators=[(<span class="hljs-string">&apos;c1&apos;</span>, clf1), (<span class="hljs-string">&apos;c2&apos;</span>, clf2), (<span class="hljs-string">&apos;c3&apos;</span>, clf3)], voting=<span class="hljs-string">&apos;soft&apos;</span>)

clf_labels += [<span class="hljs-string">&apos;Majority Voting&apos;</span>]
all_clf += [mv_clf]

print(<span class="hljs-string">&apos;10-fold cross validation:\n&apos;</span>)
<span class="hljs-keyword">for</span> clf, label <span class="hljs-keyword">in</span> zip(all_clf, clf_labels):
    scores = cross_val_score(estimator=clf, X=X_train, y=y_train,
                             cv=<span class="hljs-number">10</span>, scoring=<span class="hljs-string">&apos;roc_auc&apos;</span>)
    print(<span class="hljs-string">&quot;ROC AUC: %0.2f (+/- %0.2f) [%s]&quot;</span>
             % (scores.mean(), scores.std(), label))
</code></pre>
<pre><code>10-fold cross validation:

ROC AUC: 0.93 (+/- 0.15) [Logistic Regression]
ROC AUC: 0.93 (+/- 0.10) [KNN]
ROC AUC: 0.92 (+/- 0.15) [Decision Tree]
ROC AUC: 0.97 (+/- 0.10) [Majority Voting]
</code></pre><p>&#x6700;&#x540E;&#x4E00;&#x4E2A;&#x662F; majority voting, &#x660E;&#x663E;&#x6BD4;&#x5355;&#x72EC;&#x7684;&#x5206;&#x7C7B;&#x5668;&#x7ED3;&#x679C;&#x597D;</p>
<p><br>
<br></p>
<h2 id="evaluating-the-ensemble-classifier"><a name="evaluating-the-ensemble-classifier" class="plugin-anchor" href="#evaluating-the-ensemble-classifier"><i class="fa fa-link" aria-hidden="true"></i></a>Evaluating the ensemble classifier</h2>
<p>[<a href="#sections">back to top</a>]</p>
<p>&#x5728;&#x6D4B;&#x8BD5;&#x96C6;&#x4E0A;&#x8BC4;&#x4F30;&#x5404;&#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x7684; ROC AUC</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> roc_curve
<span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> auc

colors = [<span class="hljs-string">&apos;black&apos;</span>, <span class="hljs-string">&apos;orange&apos;</span>, <span class="hljs-string">&apos;blue&apos;</span>, <span class="hljs-string">&apos;green&apos;</span>]
linestyles = [<span class="hljs-string">&apos;:&apos;</span>, <span class="hljs-string">&apos;--&apos;</span>, <span class="hljs-string">&apos;-.&apos;</span>, <span class="hljs-string">&apos;-&apos;</span>]
<span class="hljs-keyword">for</span> clf, label, clr, ls \
        <span class="hljs-keyword">in</span> zip(all_clf, clf_labels, colors, linestyles):

    <span class="hljs-comment"># assuming the label of the positive class is 1</span>
    y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, <span class="hljs-number">1</span>]
    fpr, tpr, thresholds = roc_curve(y_true=y_test, 
                                     y_score=y_pred)
    roc_auc = auc(x=fpr, y=tpr)
    plt.plot(fpr, tpr, 
             color=clr, 
             linestyle=ls, 
             label=<span class="hljs-string">&apos;%s (auc = %0.2f)&apos;</span> % (label, roc_auc))

plt.legend(loc=<span class="hljs-string">&apos;lower right&apos;</span>)
plt.plot([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
         linestyle=<span class="hljs-string">&apos;--&apos;</span>, 
         color=<span class="hljs-string">&apos;gray&apos;</span>, 
         linewidth=<span class="hljs-number">2</span>)

plt.xlim([<span class="hljs-number">-0.1</span>, <span class="hljs-number">1.1</span>])
plt.ylim([<span class="hljs-number">-0.1</span>, <span class="hljs-number">1.1</span>])
plt.grid()
plt.xlabel(<span class="hljs-string">&apos;False Positive Rate&apos;</span>)
plt.ylabel(<span class="hljs-string">&apos;True Positive Rate&apos;</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/roc.png&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_66_0.png" alt="png"></p>
<p>ROC &#x53EF;&#x770B;&#x51FA;, ensemble classfifier &#x5728; test set &#x4E0A;&#x8868;&#x73B0;&#x4E0D;&#x9519;</p>
<p>&#x5BF9;&#x6BD4;&#x51B3;&#x7B56;&#x8FB9;&#x754C;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> product

x_min = X_train[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>
x_max = X_train[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
y_min = X_train[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>
y_max = X_train[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>

xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),
                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))

<span class="hljs-comment"># &#x6DFB;&#x52A0; subplots</span>
f, axarr = plt.subplots(nrows=<span class="hljs-number">2</span>, ncols=<span class="hljs-number">2</span>, 
                        sharex=<span class="hljs-string">&apos;col&apos;</span>, sharey=<span class="hljs-string">&apos;row&apos;</span>, 
                        figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">5</span>))

<span class="hljs-keyword">for</span> idx, clf, tt <span class="hljs-keyword">in</span> zip(product([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]),
                        all_clf, clf_labels):
    clf.fit(X_train, y_train)
    Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T)
    Z = Z.reshape(xx.shape)

    axarr[idx[<span class="hljs-number">0</span>], idx[<span class="hljs-number">1</span>]].contourf(xx, yy, Z, alpha=<span class="hljs-number">0.3</span>)
    axarr[idx[<span class="hljs-number">0</span>], idx[<span class="hljs-number">1</span>]].scatter(X_train[y_train==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                                  X_train[y_train==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
                                  c=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>, s=<span class="hljs-number">50</span>)
    axarr[idx[<span class="hljs-number">0</span>], idx[<span class="hljs-number">1</span>]].scatter(X_train[y_train==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], 
                                  X_train[y_train==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                                  c=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>, s=<span class="hljs-number">50</span>)
    axarr[idx[<span class="hljs-number">0</span>], idx[<span class="hljs-number">1</span>]].set_title(tt)

plt.text(<span class="hljs-number">-3.5</span>, <span class="hljs-number">-4.5</span>, 
         s=<span class="hljs-string">&apos;Sepal width [standardized]&apos;</span>, 
         ha=<span class="hljs-string">&apos;center&apos;</span>, va=<span class="hljs-string">&apos;center&apos;</span>, fontsize=<span class="hljs-number">12</span>)
plt.text(<span class="hljs-number">-10.5</span>, <span class="hljs-number">4.5</span>, 
         s=<span class="hljs-string">&apos;Petal length [standardized]&apos;</span>, 
         ha=<span class="hljs-string">&apos;center&apos;</span>, va=<span class="hljs-string">&apos;center&apos;</span>, 
         fontsize=<span class="hljs-number">12</span>, rotation=<span class="hljs-number">90</span>)

plt.tight_layout()
<span class="hljs-comment"># plt.savefig(&apos;./figures/voting_panel&apos;, bbox_inches=&apos;tight&apos;, dpi=300)</span>
</code></pre>
<p><img src="output_69_0.png" alt="png"></p>
<p><br>
<br></p>
<h1 id="bagging----building-an-ensemble-of-classifiers-from-bootstrap-samples"><a name="bagging----building-an-ensemble-of-classifiers-from-bootstrap-samples" class="plugin-anchor" href="#bagging----building-an-ensemble-of-classifiers-from-bootstrap-samples"><i class="fa fa-link" aria-hidden="true"></i></a>Bagging -- Building an ensemble of classifiers from bootstrap samples</h1>
<ul>
<li>draw <code>bootstrap</code> samples (random samples with replacement) from initial training set</li>
<li>random forests are a special case of bagging where we also use random feature subsets to fit the individual decision trees</li>
</ul>
<p>[<a href="#sections">back to top</a>]</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-comment"># wine dataset</span>
df_wine = pd.read_csv(<span class="hljs-string">&apos;ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine/wine.data&apos;</span>, 
                      header=<span class="hljs-keyword">None</span>)

df_wine.columns = [<span class="hljs-string">&apos;Class label&apos;</span>, <span class="hljs-string">&apos;Alcohol&apos;</span>, <span class="hljs-string">&apos;Malic acid&apos;</span>, <span class="hljs-string">&apos;Ash&apos;</span>, 
<span class="hljs-string">&apos;Alcalinity of ash&apos;</span>, <span class="hljs-string">&apos;Magnesium&apos;</span>, <span class="hljs-string">&apos;Total phenols&apos;</span>, 
<span class="hljs-string">&apos;Flavanoids&apos;</span>, <span class="hljs-string">&apos;Nonflavanoid phenols&apos;</span>, <span class="hljs-string">&apos;Proanthocyanins&apos;</span>, 
<span class="hljs-string">&apos;Color intensity&apos;</span>, <span class="hljs-string">&apos;Hue&apos;</span>, <span class="hljs-string">&apos;OD280/OD315 of diluted wines&apos;</span>, <span class="hljs-string">&apos;Proline&apos;</span>]

<span class="hljs-comment"># only consider Wine classes 2 and 3</span>
df_wine = df_wine[df_wine[<span class="hljs-string">&apos;Class label&apos;</span>] != <span class="hljs-number">1</span>]

y = df_wine[<span class="hljs-string">&apos;Class label&apos;</span>].values
X = df_wine[[<span class="hljs-string">&apos;Alcohol&apos;</span>, <span class="hljs-string">&apos;Hue&apos;</span>]].values
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> LabelEncoder
<span class="hljs-keyword">from</span> sklearn.cross_validation <span class="hljs-keyword">import</span> train_test_split

<span class="hljs-comment"># &#x8F6C;&#x6362; label</span>
le = LabelEncoder()
y = le.fit_transform(y)

<span class="hljs-comment"># 60% train, 40% test</span>
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=<span class="hljs-number">0.40</span>, random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># sklearn &#x63D0;&#x4F9B;&#x7684; BaggingClassifier&#xFF0C; &#x5176;&#x5B9E;&#x529F;&#x80FD;&#x5DF2;&#x7ECF;&#x8D85;&#x8FC7; Bagging &#x4E86;</span>
<span class="hljs-comment"># &#x5B83;&#x65E2;&#x80FD;&#x5BF9; samples &#x91C7;&#x6837;&#xFF0C;&#x4E5F;&#x80FD;&#x5BF9; features &#x91C7;&#x6837;</span>
<span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> BaggingClassifier
<span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier

tree = DecisionTreeClassifier(criterion=<span class="hljs-string">&apos;entropy&apos;</span>)

<span class="hljs-comment"># &#x7528; Decision Tree &#x4F5C; base </span>
bag = BaggingClassifier(base_estimator=tree,
                        n_estimators=<span class="hljs-number">500</span>, 
                        max_samples=<span class="hljs-number">1.0</span>,  <span class="hljs-comment"># &#x5B50;&#x91C7;&#x6837; samples &#x7684;&#x6BD4;&#x4F8B;</span>
                        max_features=<span class="hljs-number">1.0</span>,  <span class="hljs-comment"># &#x5B50;&#x91C7;&#x6837; features &#x7684;&#x6BD4;&#x4F8B;</span>
                        bootstrap=<span class="hljs-keyword">True</span>,   <span class="hljs-comment"># &#x91C7;&#x6837; samples &#x65F6;&#x662F;&#x5426;&#x4F7F;&#x7528; bootstrap</span>
                        bootstrap_features=<span class="hljs-keyword">False</span>,  <span class="hljs-comment"># &#x91C7;&#x6837; features &#x65F6;&#x662F;&#x5426;&#x4F7F;&#x7528; bootstrap</span>
                        random_state=<span class="hljs-number">1</span>)
</code></pre>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score

tree = tree.fit(X_train, y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)

tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)
print(<span class="hljs-string">&apos;Decision tree train/test accuracies %.3f/%.3f&apos;</span>
      % (tree_train, tree_test))

bag = bag.fit(X_train, y_train)
y_train_pred = bag.predict(X_train)
y_test_pred = bag.predict(X_test)

bag_train = accuracy_score(y_train, y_train_pred) 
bag_test = accuracy_score(y_test, y_test_pred) 
print(<span class="hljs-string">&apos;Bagging train/test accuracies %.3f/%.3f&apos;</span>
      % (bag_train, bag_test))
</code></pre>
<pre><code>Decision tree train/test accuracies 1.000/0.854
Bagging train/test accuracies 1.000/0.896
</code></pre><p>&#x4F7F;&#x7528; Bagging &#x4E4B;&#x540E;&#xFF0C;&#x6D4B;&#x8BD5;&#x96C6;&#x7684;&#x51C6;&#x786E;&#x7387;&#x6709;&#x63D0;&#x5347;</p>
<pre><code class="lang-python"><span class="hljs-comment"># &#x753B;&#x51B3;&#x7B56;&#x8FB9;&#x754C;</span>
%matplotlib inline
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

x_min = X_train[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>
x_max = X_train[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
y_min = X_train[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>
y_max = X_train[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>

xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),
                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))

f, axarr = plt.subplots(nrows=<span class="hljs-number">1</span>, ncols=<span class="hljs-number">2</span>, 
                        sharex=<span class="hljs-string">&apos;col&apos;</span>, 
                        sharey=<span class="hljs-string">&apos;row&apos;</span>, 
                        figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">3</span>))


<span class="hljs-keyword">for</span> idx, clf, tt <span class="hljs-keyword">in</span> zip([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],
                        [tree, bag],
                        [<span class="hljs-string">&apos;Decision Tree&apos;</span>, <span class="hljs-string">&apos;Bagging&apos;</span>]):
    clf.fit(X_train, y_train)

    Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T)
    Z = Z.reshape(xx.shape)

    axarr[idx].contourf(xx, yy, Z, alpha=<span class="hljs-number">0.3</span>)
    axarr[idx].scatter(X_train[y_train==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                       X_train[y_train==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
                       c=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>)

    axarr[idx].scatter(X_train[y_train==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], 
                       X_train[y_train==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                       c=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>)

    axarr[idx].set_title(tt)

axarr[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;Alcohol&apos;</span>, fontsize=<span class="hljs-number">12</span>)
plt.text(<span class="hljs-number">10.2</span>, <span class="hljs-number">-1.2</span>, s=<span class="hljs-string">&apos;Hue&apos;</span>, 
         ha=<span class="hljs-string">&apos;center&apos;</span>, va=<span class="hljs-string">&apos;center&apos;</span>, fontsize=<span class="hljs-number">12</span>)

plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/bagging_region.png&apos;, </span>
<span class="hljs-comment">#            dpi=300, </span>
<span class="hljs-comment">#            bbox_inches=&apos;tight&apos;)</span>
</code></pre>
<p><img src="output_78_0.png" alt="png"></p>
<p>Bagging &#x51CF;&#x5C11;&#x4E86; overfitting&#xFF0C; &#x4F7F;&#x51B3;&#x7B56;&#x8FB9;&#x754C;&#x66F4;&#x5E73;&#x6ED1;</p>
<p><br>
<br></p>
<h1 id="leveraging-of-weak-learners-via-adaptive-boosting"><a name="leveraging-of-weak-learners-via-adaptive-boosting" class="plugin-anchor" href="#leveraging-of-weak-learners-via-adaptive-boosting"><i class="fa fa-link" aria-hidden="true"></i></a>Leveraging of weak learners via adaptive boosting</h1>
<p>let the weak learners subsequently learn from misclassified training samples to improve the performance of the ensemble</p>
<p>[<a href="#sections">back to top</a>]</p>
<p>Adaptive boosting (AdaBoost)</p>
<ol>
<li>Set weight vector w to uniform weights where <script type="math/tex; ">\sum_i w_i = 1</script></li>
<li>For j in m boosting rounds, do the following:</li>
<li>Train a weighted weak learner: <script type="math/tex; ">C_j = train(X,y,w)</script>.</li>
<li>Predict class labels: <script type="math/tex; ">\hat{y} = predict(C_j, X)</script> .</li>
<li>Compute weighted error rate: <script type="math/tex; ">\epsilon = w \cdot (\hat{y} \neq y)</script>.</li>
<li>Compute coefficient: <script type="math/tex; ">\alpha_j = \frac 1 2 ln\frac{1-\epsilon}{\epsilon}</script> .</li>
<li>Update weights: <script type="math/tex; ">w:= w \times exp(-\alpha_j \times \hat{y} \times y)</script> .</li>
<li>Normalize weights to sum to 1: <script type="math/tex; ">w:= \frac{w}{\sum_i w_i}</script> .</li>
<li>Compute final prediction: <script type="math/tex; ">\hat{y} = (\sum_{j=1}^{m}(\alpha_j \times predict(C_j, X))>0)</script>.</li>
</ol>
<p><script type="math/tex; ">\cdot</script> denotes dot product between two vectors<br><script type="math/tex; ">\times</script> denotes element-wise multiplication of two vectors</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> AdaBoostClassifier

tree = DecisionTreeClassifier(criterion=<span class="hljs-string">&apos;entropy&apos;</span>, 
                              max_depth=<span class="hljs-number">1</span>)

ada = AdaBoostClassifier(base_estimator=tree,
                         n_estimators=<span class="hljs-number">500</span>, 
                         learning_rate=<span class="hljs-number">0.1</span>,
                         random_state=<span class="hljs-number">0</span>)
</code></pre>
<pre><code class="lang-python">tree = tree.fit(X_train, y_train)
y_train_pred = tree.predict(X_train)
y_test_pred = tree.predict(X_test)

tree_train = accuracy_score(y_train, y_train_pred)
tree_test = accuracy_score(y_test, y_test_pred)
print(<span class="hljs-string">&apos;Decision tree train/test accuracies %.3f/%.3f&apos;</span>
      % (tree_train, tree_test))

ada = ada.fit(X_train, y_train)
y_train_pred = ada.predict(X_train)
y_test_pred = ada.predict(X_test)

ada_train = accuracy_score(y_train, y_train_pred) 
ada_test = accuracy_score(y_test, y_test_pred) 
print(<span class="hljs-string">&apos;AdaBoost train/test accuracies %.3f/%.3f&apos;</span>
      % (ada_train, ada_test))
</code></pre>
<pre><code>Decision tree train/test accuracies 0.845/0.854
AdaBoost train/test accuracies 1.000/0.875
</code></pre><p>Adaboost &#x53EF;&#x4EE5;&#x51CF;&#x5C11; Bias&#xFF0C;&#x4F46;&#x53EF;&#x80FD;&#x5F15;&#x5165;&#x66F4;&#x591A;&#x7684; Variance</p>
<pre><code class="lang-python">x_min, x_max = X_train[:, <span class="hljs-number">0</span>].min() - <span class="hljs-number">1</span>, X_train[:, <span class="hljs-number">0</span>].max() + <span class="hljs-number">1</span>
y_min, y_max = X_train[:, <span class="hljs-number">1</span>].min() - <span class="hljs-number">1</span>, X_train[:, <span class="hljs-number">1</span>].max() + <span class="hljs-number">1</span>
xx, yy = np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number">0.1</span>),
                     np.arange(y_min, y_max, <span class="hljs-number">0.1</span>))

f, axarr = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, sharex=<span class="hljs-string">&apos;col&apos;</span>, sharey=<span class="hljs-string">&apos;row&apos;</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">3</span>))


<span class="hljs-keyword">for</span> idx, clf, tt <span class="hljs-keyword">in</span> zip([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>],
                        [tree, ada],
                        [<span class="hljs-string">&apos;Decision Tree&apos;</span>, <span class="hljs-string">&apos;AdaBoost&apos;</span>]):
    clf.fit(X_train, y_train)

    Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T)
    Z = Z.reshape(xx.shape)

    axarr[idx].contourf(xx, yy, Z, alpha=<span class="hljs-number">0.3</span>)
    axarr[idx].scatter(X_train[y_train==<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                       X_train[y_train==<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], 
                       c=<span class="hljs-string">&apos;blue&apos;</span>, marker=<span class="hljs-string">&apos;^&apos;</span>)
    axarr[idx].scatter(X_train[y_train==<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], 
                       X_train[y_train==<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                       c=<span class="hljs-string">&apos;red&apos;</span>, marker=<span class="hljs-string">&apos;o&apos;</span>)
    axarr[idx].set_title(tt)

axarr[<span class="hljs-number">0</span>].set_ylabel(<span class="hljs-string">&apos;Alcohol&apos;</span>, fontsize=<span class="hljs-number">12</span>)
plt.text(<span class="hljs-number">10.2</span>, <span class="hljs-number">-1.2</span>, s=<span class="hljs-string">&apos;Hue&apos;</span>, 
         ha=<span class="hljs-string">&apos;center&apos;</span>, va=<span class="hljs-string">&apos;center&apos;</span>, fontsize=<span class="hljs-number">12</span>)

plt.tight_layout()
<span class="hljs-comment">#plt.savefig(&apos;./figures/adaboost_region.png&apos;, </span>
<span class="hljs-comment">#           dpi=300, </span>
<span class="hljs-comment">#           bbox_inches=&apos;tight&apos;)</span>
</code></pre>
<p><img src="output_87_0.png" alt="png"></p>
<p>Adaboost &#x7684;&#x51B3;&#x7B56;&#x8FB9;&#x754C;&#x6BD4; tree &#x590D;&#x6742;, &#x4E0E; BaggingClassifier &#x76F8;&#x4F3C;.</p>
<p>Ensemble method &#x9700;&#x8981;&#x66F4;&#x591A;&#x7684;&#x8BA1;&#x7B97;&#x8D44;&#x6E90;, &#x8FD9;&#x4E2A;&#x5728;&#x5B9E;&#x9645;&#x8FD0;&#x7528;&#x4E2D;&#x4E5F;&#x662F;&#x8981;&#x8003;&#x8651;&#x7684;.</p>
<p><br>
<br></p>
<h1 id="algorithm-implementation"><a name="algorithm-implementation" class="plugin-anchor" href="#algorithm-implementation"><i class="fa fa-link" aria-hidden="true"></i></a>Algorithm implementation</h1>
<p>[<a href="#sections">back to top</a>]</p>
<h3 id="&#x5E7F;&#x4E49;&#x63D0;&#x5347;&#x6811;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;"><a name="&#x5E7F;&#x4E49;&#x63D0;&#x5347;&#x6811;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;" class="plugin-anchor" href="#&#x5E7F;&#x4E49;&#x63D0;&#x5347;&#x6811;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x5E7F;&#x4E49;&#x63D0;&#x5347;&#x6811;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;</h3>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plot
%matplotlib inline
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree
<span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor
<span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor
<span class="hljs-keyword">import</span> random
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># Build a simple data set with y = x + random</span>
n_points = <span class="hljs-number">1000</span>

<span class="hljs-comment"># x values for plotting</span>
x_plot = [(float(i) / float(n_points) - <span class="hljs-number">0.5</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n_points + <span class="hljs-number">1</span>)]

<span class="hljs-comment"># x needs to be list of lists.</span>
x = [[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> x_plot]

<span class="hljs-comment"># y (labels) has random noise added to x-value</span>
<span class="hljs-comment"># set seed</span>
numpy.random.seed(<span class="hljs-number">1</span>)
y = [s + numpy.random.normal(scale=<span class="hljs-number">0.1</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> x_plot]

<span class="hljs-comment"># take fixed test set 30% of sample</span>
n_sample = int(n_points * <span class="hljs-number">0.30</span>)
idx_test = random.sample(range(n_points), n_sample)
idx_test.sort()
idx_train = [idx <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> range(n_points) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (idx <span class="hljs-keyword">in</span> idx_test)]

<span class="hljs-comment"># Define test and training attribute and label sets</span>
x_train = [x[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_train]
x_test = [x[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_test]
y_train = [y[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_train]
y_test = [y[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_test]

<span class="hljs-comment"># train a series of models on random subsets of the training data</span>
<span class="hljs-comment"># collect the models in a list and check error of composite as list grows</span>

<span class="hljs-comment"># maximum number of models to generate</span>
num_trees_max = <span class="hljs-number">30</span>

<span class="hljs-comment"># tree depth - typically at the high end</span>
tree_depth = <span class="hljs-number">5</span>

<span class="hljs-comment"># initialize a list to hold models</span>
mode_list = []
pred_list = []
eps = <span class="hljs-number">0.3</span>

<span class="hljs-comment"># initialize residuals to be the labels y</span>
residuals = list(y_train)

<span class="hljs-keyword">for</span> i_trees <span class="hljs-keyword">in</span> range(num_trees_max):
    mode_list.append(DecisionTreeRegressor(max_depth=tree_depth))
    mode_list[<span class="hljs-number">-1</span>].fit(x_train, residuals)

    <span class="hljs-comment"># make prediction with latest model and add to list of predictions</span>
    latest_in_sample_prediction = mode_list[<span class="hljs-number">-1</span>].predict(x_train)

    <span class="hljs-comment"># use new predictions to update residuals</span>
    residuals = [residuals[i] - eps * latest_in_sample_prediction[i]
                 <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(residuals))]

    latest_out_sample_prediction = mode_list[<span class="hljs-number">-1</span>].predict(x_test)
    pred_list.append(list(latest_out_sample_prediction))

<span class="hljs-comment"># build cumulative prediction from first &quot;n&quot; models</span>
mse = []
all_predictions = []
<span class="hljs-keyword">for</span> i_models <span class="hljs-keyword">in</span> range(len(mode_list)):

    <span class="hljs-comment"># add the first &quot;i_models&quot; of the predictions and multiply by eps</span>
    prediction = []
    <span class="hljs-keyword">for</span> i_pred <span class="hljs-keyword">in</span> range(len(x_test)):
        prediction.append(
            sum([pred_list[i][i_pred] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(i_models + <span class="hljs-number">1</span>)]) * eps)

    all_predictions.append(prediction)
    errors = [(y_test[i] - prediction[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(y_test))]
    mse.append(sum([e * e <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> errors]) / len(y_test))

n_models = [i + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(mode_list))]
</code></pre>
<pre><code class="lang-python">plot.plot(n_models, mse)
plot.axis(<span class="hljs-string">&apos;tight&apos;</span>)
plot.xlabel(<span class="hljs-string">&apos;Number of Models in Ensemble&apos;</span>)
plot.ylabel(<span class="hljs-string">&apos;Mean Squared Error&apos;</span>)
plot.ylim((<span class="hljs-number">0.0</span>, max(mse)))
plot.show()

plot_list = [<span class="hljs-number">0</span>, <span class="hljs-number">14</span>, <span class="hljs-number">29</span>]
line_type = [<span class="hljs-string">&apos;:&apos;</span>, <span class="hljs-string">&apos;-.&apos;</span>, <span class="hljs-string">&apos;--&apos;</span>]
plot.figure()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(plot_list)):
    i_plot = plot_list[i]
    text_legend = <span class="hljs-string">&apos;Prediction with &apos;</span> + str(i_plot) + <span class="hljs-string">&apos; Trees&apos;</span>
    plot.plot(x_test, all_predictions[i_plot], label=text_legend,
              linestyle=line_type[i])
plot.plot(x_test, y_test, label=<span class="hljs-string">&apos;True y Value&apos;</span>, alpha=<span class="hljs-number">0.25</span>)
plot.legend(bbox_to_anchor=(<span class="hljs-number">1</span>, <span class="hljs-number">0.3</span>))
plot.axis(<span class="hljs-string">&apos;tight&apos;</span>)
plot.xlabel(<span class="hljs-string">&apos;x value&apos;</span>)
plot.ylabel(<span class="hljs-string">&apos;Predictions&apos;</span>);
</code></pre>
<p><img src="output_95_0.png" alt="png"></p>
<p><img src="output_95_1.png" alt="png"></p>
<p><br>
<br></p>
<h3 id="&#x968F;&#x673A;&#x68EE;&#x6797;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;"><a name="&#x968F;&#x673A;&#x68EE;&#x6797;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;" class="plugin-anchor" href="#&#x968F;&#x673A;&#x68EE;&#x6797;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x968F;&#x673A;&#x68EE;&#x6797;&#x7B97;&#x6CD5;&#x8BE6;&#x89E3;</h3>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> urllib2
<span class="hljs-keyword">import</span> numpy
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> tree
<span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeRegressor
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> sqrt
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plot

<span class="hljs-comment"># read data into iterable</span>
target_url = <span class="hljs-string">&quot;ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine-quality/winequality-red.csv&quot;</span>
data = urllib2.urlopen(target_url)

x_list = []
labels = []
names = []
first_line = <span class="hljs-keyword">True</span>
<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> data:
    <span class="hljs-keyword">if</span> first_line:
        names = line.strip().split(<span class="hljs-string">&quot;;&quot;</span>)
        first_line = <span class="hljs-keyword">False</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-comment"># split on semi-colon</span>
        row = line.strip().split(<span class="hljs-string">&quot;;&quot;</span>)
        <span class="hljs-comment"># put labels in separate array</span>
        labels.append(float(row[<span class="hljs-number">-1</span>]))
        <span class="hljs-comment"># remove label from row</span>
        row.pop()
        <span class="hljs-comment"># convert row to floats</span>
        float_row = [float(num) <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> row]
        x_list.append(float_row)

nrows = len(x_list)
ncols = len(x_list[<span class="hljs-number">0</span>])

<span class="hljs-comment"># take fixed test set 30% of sample</span>
random.seed(<span class="hljs-number">1</span>)  <span class="hljs-comment"># set seed so results are the same each run</span>
n_sample = int(nrows * <span class="hljs-number">0.30</span>)
idx_test = random.sample(range(nrows), n_sample)
idx_test.sort()
idx_train = [idx <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> range(nrows) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (idx <span class="hljs-keyword">in</span> idx_test)]

<span class="hljs-comment"># Define test and training attribute and label sets</span>
x_train = [x_list[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_train]
x_test = [x_list[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_test]
y_train = [labels[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_train]
y_test = [labels[r] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> idx_test]

<span class="hljs-comment"># train a series of models on random subsets of the training data</span>
<span class="hljs-comment"># collect the models in a list and check error of composite as list grows</span>

<span class="hljs-comment"># maximum number of models to generate</span>
num_trees_max = <span class="hljs-number">30</span>

<span class="hljs-comment"># tree depth - typically at the high end</span>
tree_depth = <span class="hljs-number">12</span>

<span class="hljs-comment"># pick how many attributes will be used in each model.</span>
<span class="hljs-comment"># authors recommend 1/3 for regression problem</span>
n_attr = <span class="hljs-number">4</span>

<span class="hljs-comment"># initialize a list to hold models</span>
mode_list = []
index_list = []
pred_list = []
n_train_rows = len(y_train)

<span class="hljs-keyword">for</span> i_trees <span class="hljs-keyword">in</span> range(num_trees_max):

    mode_list.append(DecisionTreeRegressor(max_depth=tree_depth))

    <span class="hljs-comment"># take random sample of attributes</span>
    idx_attr = random.sample(range(ncols), n_attr)
    idx_attr.sort()
    index_list.append(idx_attr)

    <span class="hljs-comment"># take a random sample of training rows</span>
    idx_rows = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(int(<span class="hljs-number">0.5</span> * n_train_rows)):
        idx_rows.append(random.choice(range(len(x_train))))
    idx_rows.sort()

    <span class="hljs-comment"># build training set</span>
    x_rf_train = []
    y_rf_train = []

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(idx_rows)):
        temp = [x_train[idx_rows[i]][j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> idx_attr]
        x_rf_train.append(temp)
        y_rf_train.append(y_train[idx_rows[i]])

    mode_list[<span class="hljs-number">-1</span>].fit(x_rf_train, y_rf_train)

    <span class="hljs-comment"># restrict xTest to attributes selected for training</span>
    x_rf_test = []
    <span class="hljs-keyword">for</span> xx <span class="hljs-keyword">in</span> x_test:
        temp = [xx[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> idx_attr]
        x_rf_test.append(temp)

    latest_out_sample_prediction = mode_list[<span class="hljs-number">-1</span>].predict(x_rf_test)
    pred_list.append(list(latest_out_sample_prediction))

<span class="hljs-comment"># build cumulative prediction from first &quot;n&quot; models</span>
mse = []
all_predictions = []
<span class="hljs-keyword">for</span> i_models <span class="hljs-keyword">in</span> range(len(mode_list)):

    <span class="hljs-comment"># add the first &quot;iModels&quot; of the predictions and multiply by eps</span>
    prediction = []
    <span class="hljs-keyword">for</span> i_pred <span class="hljs-keyword">in</span> range(len(x_test)):
        prediction.append(
            sum([pred_list[i][i_pred] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(i_models + <span class="hljs-number">1</span>)]) / (
            i_models + <span class="hljs-number">1</span>))

    all_predictions.append(prediction)
    errors = [(y_test[i] - prediction[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(y_test))]
    mse.append(sum([e * e <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> errors]) / len(y_test))

n_models = [i + <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(mode_list))]

plot.plot(n_models, mse)
plot.axis(<span class="hljs-string">&apos;tight&apos;</span>)
plot.xlabel(<span class="hljs-string">&apos;Number of Trees in Ensemble&apos;</span>)
plot.ylabel(<span class="hljs-string">&apos;Mean Squared Error&apos;</span>)
plot.ylim((<span class="hljs-number">0.0</span>, max(mse)))
plot.show()

print(<span class="hljs-string">&apos;Minimum MSE&apos;</span>)
print(min(mse))
</code></pre>
<p><img src="output_98_0.png" alt="png"></p>
<pre><code>Minimum MSE
0.389088116065
</code></pre><h2 id="&#x7EC3;&#x4E60;&#x9898;&#xFF1A;-&#x4F7F;&#x7528;&#x63D0;&#x5347;&#x6811;&#x548C;&#x968F;&#x673A;&#x68EE;&#x6797;&#x4E24;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x8FDB;&#x884C;&#x5EFA;&#x6A21;&#x6BD4;&#x8F83;"><a name="&#x7EC3;&#x4E60;&#x9898;&#xFF1A;-&#x4F7F;&#x7528;&#x63D0;&#x5347;&#x6811;&#x548C;&#x968F;&#x673A;&#x68EE;&#x6797;&#x4E24;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x8FDB;&#x884C;&#x5EFA;&#x6A21;&#x6BD4;&#x8F83;" class="plugin-anchor" href="#&#x7EC3;&#x4E60;&#x9898;&#xFF1A;-&#x4F7F;&#x7528;&#x63D0;&#x5347;&#x6811;&#x548C;&#x968F;&#x673A;&#x68EE;&#x6797;&#x4E24;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x8FDB;&#x884C;&#x5EFA;&#x6A21;&#x6BD4;&#x8F83;"><i class="fa fa-link" aria-hidden="true"></i></a>&#x7EC3;&#x4E60;&#x9898;&#xFF1A; &#x4F7F;&#x7528;&#x63D0;&#x5347;&#x6811;&#x548C;&#x968F;&#x673A;&#x68EE;&#x6797;&#x4E24;&#x79CD;&#x65B9;&#x6CD5;&#xFF0C;&#x5BF9;&#x4FE1;&#x8D37;&#x6570;&#x636E;&#x96C6;&#x8FDB;&#x884C;&#x5EFA;&#x6A21;&#x6BD4;&#x8F83;</h2>
<pre><code class="lang-python">

</code></pre>
<p><br>
<br></p>
<p><br>
<br></p>
<script type="text/javascript">var className='atoc';</script>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="../w2-preceptron-and-logistic-regression/2w.html" class="navigation navigation-prev " aria-label="Previous page: 感知机和逻辑回归">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="../w4-feature-engineering/4w.html" class="navigation navigation-next " aria-label="Next page: 特征工程">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"决策树和集成学习","level":"1.5","depth":1,"next":{"title":"特征工程","level":"1.6","depth":1,"path":"w4-feature-engineering/4w.md","ref":"w4-feature-engineering/4w.md","articles":[]},"previous":{"title":"感知机和逻辑回归","level":"1.4","depth":1,"path":"w2-preceptron-and-logistic-regression/2w.md","ref":"w2-preceptron-and-logistic-regression/2w.md","articles":[]},"dir":"ltr"},"config":{"plugins":["mathjax","anchors","github","splitter","sharing","atoc","comment"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"github":{"url":"https://github.com/lyltj2010/DataMining"},"atoc":{"addClass":true,"className":"atoc"},"splitter":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":true,"instapaper":false,"twitter":true,"vk":false,"weibo":true},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"anchors":{},"comment":{"highlightCommented":true}},"theme":"default","author":"wizardforcel","pdf":{"pageNumbers":true,"fontSize":16,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"数据挖掘开源书","language":"zh","links":{"sidebar":{"数据挖掘开源书":"https://www.gitbook.com/book/wizardforcel/data-mining-book"},"gitbook":true},"gitbook":"*","description":"数据挖掘开源书"},"file":{"path":"w3-decision-tree-and-ensemble-learning/3w.md","mtime":"2016-12-24T21:25:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.2","time":"2017-04-29T11:04:29.751Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-atoc/atoc.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-comment/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

